ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 4.4 LRU vs LFU vs Random : Choisir la bonne stratÃ©gie

## Introduction

Le choix de l'algorithme d'Ã©viction (LRU, LFU ou Random) est l'une des dÃ©cisions les plus importantes lors de la configuration de Redis. Ce choix impacte directement :

- **Le hit ratio** : Pourcentage de requÃªtes servies depuis le cache
- **La performance** : CPU et latence des opÃ©rations
- **L'utilisation mÃ©moire** : EfficacitÃ© de l'espace disponible
- **La prÃ©visibilitÃ©** : Comportement du systÃ¨me en production

Contrairement Ã  une base de donnÃ©es traditionnelle oÃ¹ tout est persistÃ©, Redis en tant que cache doit constamment **dÃ©cider quoi garder en mÃ©moire**. Cette section compare en profondeur les trois algorithmes principaux pour vous aider Ã  faire le bon choix.

## Les trois algorithmes : Vue d'ensemble

### LRU (Least Recently Used)

**Principe** : Ã‰victe les donnÃ©es les **moins rÃ©cemment utilisÃ©es**.

```
HypothÃ¨se fondamentale:
"Les donnÃ©es rÃ©cemment accÃ©dÃ©es seront probablement accÃ©dÃ©es Ã  nouveau bientÃ´t"

Chronologie des accÃ¨s:
Time â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
     A   B   C   A   D   B   E
     â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚
     â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€ Plus rÃ©cent
                             â†‘
                             Garde E, B, D, A
                             Ã‰victe C
```

### LFU (Least Frequently Used)

**Principe** : Ã‰victe les donnÃ©es les **moins frÃ©quemment utilisÃ©es**.

```
HypothÃ¨se fondamentale:
"Les donnÃ©es frÃ©quemment accÃ©dÃ©es continueront d'Ãªtre frÃ©quemment accÃ©dÃ©es"

Compteur d'accÃ¨s:
Key A: 100 accÃ¨s
Key B: 50 accÃ¨s
Key C: 10 accÃ¨s
Key D: 5 accÃ¨s
     â†‘
     Ã‰victe D (moins frÃ©quent)
```

### Random

**Principe** : Ã‰victe des donnÃ©es **alÃ©atoirement**.

```
HypothÃ¨se fondamentale:
"Toutes les donnÃ©es ont la mÃªme probabilitÃ© d'Ãªtre accÃ©dÃ©es"

Ou: "La vitesse d'Ã©viction prime sur la qualitÃ© du choix"

Keys: [A, B, C, D, E]
      â””â”€ SÃ©lection alÃ©atoire
         Ã‰victe au hasard
```

## MÃ©canismes internes dÃ©taillÃ©s

### LRU : ImplÃ©mentation Redis

#### Structure de donnÃ©es

```c
// Champ LRU dans RedisObject (24 bits)
typedef struct redisObject {
    unsigned type:4;
    unsigned encoding:4;
    unsigned lru:24;      // â† LRU timestamp
    int refcount;
    void *ptr;
} robj;

// LRU_BITS = 24 bits
// Range: 0 Ã  16,777,215 (2^24 - 1)
// RÃ©solution: 1 seconde (ajustable)
```

#### Calcul du timestamp LRU

```c
// Horloge LRU globale (secondes divisÃ©es par rÃ©solution)
#define LRU_CLOCK_RESOLUTION 1000  // 1 seconde
#define LRU_CLOCK_MAX ((1<<24)-1)  // 16,777,215

unsigned int getLRUClock(void) {
    return (mstime() / LRU_CLOCK_RESOLUTION) & LRU_CLOCK_MAX;
}

// Mise Ã  jour Ã  chaque accÃ¨s
void updateLRU(robj *o) {
    o->lru = getLRUClock();
}
```

#### Calcul de l'idle time

```c
// Calcul du temps d'inactivitÃ© (en secondes)
unsigned long long estimateObjectIdleTime(robj *o) {
    unsigned long long lruclock = LRU_CLOCK();

    if (lruclock >= o->lru) {
        // Cas normal
        return (lruclock - o->lru) * LRU_CLOCK_RESOLUTION;
    } else {
        // Wrap-around (aprÃ¨s ~194 jours)
        return (LRU_CLOCK_MAX - o->lru + lruclock) * LRU_CLOCK_RESOLUTION;
    }
}
```

#### Wraparound du compteur

```
LRU timeline (24 bits):
0 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 16,777,215 (wrap) â”€â”€â†’ 0

Exemple de wraparound:
Key A: lru = 16,777,210
Global: lru = 5 (aprÃ¨s wrap)

Idle time = (16,777,215 - 16,777,210 + 5) = 10 secondes âœ“
```

#### Approximation LRU avec Ã©chantillonnage

```c
// Redis n'implÃ©mente pas un LRU parfait (trop coÃ»teux)
// Utilise un Ã©chantillonnage probabiliste

robj *evictionPoolFindBest(void) {
    struct evictionPoolEntry pool[EVPOOL_SIZE];

    // Ã‰chantillonne maxmemory_samples clÃ©s
    for (int i = 0; i < maxmemory_samples; i++) {
        dictEntry *de = dictGetRandomKey(db->dict);
        robj *o = dictGetVal(de);

        // Calcule idle time
        unsigned long long idle = estimateObjectIdleTime(o);

        // InsÃ¨re dans le pool si idle > min(pool)
        evictionPoolInsert(pool, de, idle);
    }

    // Retourne entrÃ©e avec plus grand idle time
    return pool[EVPOOL_SIZE - 1];
}
```

**QualitÃ© de l'approximation** :

```
LRU parfait (liste doublement chaÃ®nÃ©e):
- ComplexitÃ©: O(1) pour accÃ¨s, O(1) pour Ã©viction
- Overhead: 16 bytes par clÃ© (2 pointeurs)
- MÃ©moire: Inacceptable pour millions de clÃ©s

LRU approximÃ© (Ã©chantillonnage):
- ComplexitÃ©: O(N) oÃ¹ N = maxmemory_samples
- Overhead: 3 bytes par clÃ© (champ lru)
- PrÃ©cision: 85-95% selon samples
```

### LFU : ImplÃ©mentation Redis

#### Structure de donnÃ©es

Le champ LRU (24 bits) est rÃ©utilisÃ© pour stocker les donnÃ©es LFU :

```c
// En mode LFU, le champ lru est rÃ©interprÃ©tÃ©:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 16 bits            â”‚ 8 bits         â”‚
â”‚ Last decrement timeâ”‚ Counter (0-255)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

#define LFU_INIT_VAL 5

// Extraction des composants
#define LFUDecrAndReturn(o) /* voir ci-dessous */
#define LFULogIncr(counter) /* voir ci-dessous */
```

#### Compteur logarithmique

Le compteur n'est pas incrÃ©mentÃ© de faÃ§on linÃ©aire mais **logarithmique** :

```c
/* IncrÃ©mente le compteur de faÃ§on logarithmique */
uint8_t LFULogIncr(uint8_t counter) {
    if (counter == 255) return 255;  // Saturation

    double r = (double)rand() / RAND_MAX;
    double baseval = counter - LFU_INIT_VAL;
    if (baseval < 0) baseval = 0;

    double p = 1.0 / (baseval * server.lfu_log_factor + 1);

    if (r < p) counter++;
    return counter;
}
```

**Comportement selon lfu-log-factor** :

```
lfu-log-factor = 10 (dÃ©faut):

Hits     Counter    ProbabilitÃ© d'incrÃ©ment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1        5          100%
10       ~7         50%
100      ~10        10%
1000     ~15        1%
1M       ~25        0.001%

lfu-log-factor = 100:

Hits     Counter    ProbabilitÃ© d'incrÃ©ment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1        5          100%
10       ~6         91%
100      ~8         50%
1000     ~10        10%
1M       ~15        0.1%
```

**Visualisation** :

```python
import math
import matplotlib.pyplot as plt

def lfu_counter(hits, log_factor=10, init_val=5):
    counter = init_val
    for _ in range(hits):
        baseval = max(0, counter - init_val)
        p = 1.0 / (baseval * log_factor + 1)
        if random.random() < p:
            counter = min(255, counter + 1)
    return counter

# Courbe counter vs hits pour diffÃ©rents log_factor
log_factors = [1, 10, 100]
hits_range = range(1, 10000)

for lf in log_factors:
    counters = [lfu_counter(h, lf) for h in hits_range]
    plt.plot(hits_range, counters, label=f'log_factor={lf}')

plt.xlabel('Hits')
plt.ylabel('Counter')
plt.legend()
# Courbe logarithmique
```

#### Decay du compteur

Le compteur **dÃ©croÃ®t** dans le temps pour oublier les anciens accÃ¨s :

```c
/* DÃ©crÃ©mente le compteur selon le temps Ã©coulÃ© */
unsigned long LFUDecrAndReturn(robj *o) {
    unsigned long ldt = o->lru >> 8;  // Last decrement time
    unsigned long counter = o->lru & 255;

    unsigned long num_periods = server.lfu_decay_time ?
        LFUTimeElapsed(ldt) / server.lfu_decay_time : 0;

    if (num_periods)
        counter = (num_periods > counter) ? 0 : counter - num_periods;

    return counter;
}
```

**Comportement du decay** :

```
lfu-decay-time = 1 (1 minute):

Time elapsed    Counter change
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
0-59s          Pas de dÃ©cay
60s            -1
120s           -2
...            ...

Si counter = 10:
- AprÃ¨s 10 min sans accÃ¨s â†’ counter = 0
- ClÃ© devient candidate Ã  l'Ã©viction

lfu-decay-time = 10 (10 minutes):
- Decay plus lent
- Favorise les anciennes donnÃ©es populaires
```

#### StratÃ©gie complÃ¨te LFU

```
1. Nouvel objet crÃ©Ã©
   â””â”€> counter = 5 (LFU_INIT_VAL)

2. Ã€ chaque accÃ¨s
   â”œâ”€> DÃ©crÃ©mente selon temps Ã©coulÃ©
   â””â”€> IncrÃ©mente probabilistiquement

3. Ã€ l'Ã©viction
   â”œâ”€> Ã‰chantillonne N clÃ©s
   â”œâ”€> Applique decay sur chaque clÃ©
   â”œâ”€> SÃ©lectionne clÃ© avec plus petit counter
   â””â”€> Ã‰victe
```

### Random : ImplÃ©mentation Redis

L'algorithme Random est le plus simple :

```c
// SÃ©lection alÃ©atoire
robj *evictRandomKey(redisDb *db) {
    dictEntry *de = dictGetRandomKey(db->dict);

    if (de == NULL) return NULL;

    return dictGetVal(de);
}

// Pas de calcul de score
// Pas de comparaison
// Juste une sÃ©lection alÃ©atoire
```

**Avantages de la simplicitÃ©** :

```
Overhead CPU:
- LRU:    ~1.15x baseline
- LFU:    ~1.25x baseline
- Random: ~1.02x baseline  â† Minimal

Latence d'Ã©viction:
- LRU:    0.5-2 ms
- LFU:    1-5 ms
- Random: 0.1-0.5 ms  â† Plus rapide
```

## Comparaison algorithmique

### ComplexitÃ© temporelle

| OpÃ©ration | LRU | LFU | Random |
|-----------|-----|-----|--------|
| **AccÃ¨s (GET)** | O(1) + update | O(1) + update + decay | O(1) |
| **Ã‰viction** | O(N) samples | O(N) samples + decay | O(1) |
| **Update overhead** | ~1ns | ~10ns | 0 |

### ComplexitÃ© spatiale

| Algorithme | Overhead par clÃ© | Structure additionnelle |
|------------|------------------|-------------------------|
| **LRU** | 3 bytes (24 bits) | Pool 16 entrÃ©es |
| **LFU** | 3 bytes (24 bits) | Pool 16 entrÃ©es |
| **Random** | 0 bytes | Aucune |

### PrÃ©cision de l'Ã©viction

**Test standardisÃ©** : Workload 80/20 (80% des accÃ¨s sur 20% des clÃ©s)

```
Algorithme    PrÃ©cision    Hit Ratio    Description
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
LRU parfait   100%         ~90%         ThÃ©orique
LRU (samp=5)  ~85%         ~82%         Redis dÃ©faut
LRU (samp=10) ~95%         ~87%         Redis optimisÃ©
LFU (log=10)  ~90%         ~87%         Redis dÃ©faut
LFU (log=100) ~85%         ~85%         Moins discriminant
Random        ~50%         ~60%         Baseline
```

## Benchmarks pratiques

### Setup du benchmark

```python
import redis
import random
import time

class CacheBenchmark:
    def __init__(self, policy, maxmemory='10mb'):
        self.r = redis.Redis(decode_responses=True)
        self.r.flushdb()
        self.r.config_set('maxmemory', maxmemory)
        self.r.config_set('maxmemory-policy', f'allkeys-{policy}')

    def zipfian_distribution(self, n, alpha=1.5):
        """GÃ©nÃ¨re distribution Zipf (80/20)"""
        weights = [1 / (i ** alpha) for i in range(1, n + 1)]
        total = sum(weights)
        return [w / total for w in weights]

    def run_workload(self, num_keys=1000, num_ops=100000):
        # PrÃ©-remplissage
        for i in range(num_keys):
            self.r.set(f'key:{i}', f'value{i}')

        # Distribution Zipfian
        probs = self.zipfian_distribution(num_keys)

        hits = 0
        misses = 0
        start = time.time()

        for _ in range(num_ops):
            # SÃ©lectionne clÃ© selon distribution
            key_id = random.choices(range(num_keys), probs)[0]

            result = self.r.get(f'key:{key_id}')
            if result:
                hits += 1
            else:
                misses += 1
                # Recharge
                self.r.set(f'key:{key_id}', f'value{key_id}')

        elapsed = time.time() - start

        return {
            'hits': hits,
            'misses': misses,
            'hit_ratio': hits / (hits + misses),
            'ops_per_sec': num_ops / elapsed,
            'avg_latency_ms': (elapsed / num_ops) * 1000
        }
```

### RÃ©sultats benchmark : Hit Ratio

**Workload 80/20** (80% accÃ¨s sur 20% clÃ©s) :

```bash
# Test
python benchmark.py --workload zipf --alpha 1.5

Results:
Policy         Hit Ratio    OPS/sec    Avg Latency
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
allkeys-lru    82.4%        45,200     0.022 ms
allkeys-lfu    87.1%        41,800     0.024 ms
allkeys-random 59.7%        48,500     0.021 ms
```

**Workload uniforme** (accÃ¨s Ã©quiprobables) :

```
Policy         Hit Ratio    OPS/sec    Avg Latency
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
allkeys-lru    51.2%        46,100     0.022 ms
allkeys-lfu    50.8%        42,500     0.024 ms
allkeys-random 50.1%        48,900     0.020 ms
```

**Workload "scan"** (lecture sÃ©quentielle pÃ©riodique) :

```
ScÃ©nario: 1 scan complet toutes les 100 requÃªtes

Policy         Hit Ratio    Impact scan
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
allkeys-lru    45.2%        âŒ TrÃ¨s affectÃ©
allkeys-lfu    78.3%        âœ… RÃ©siste bien
allkeys-random 59.1%        âš ï¸ Non affectÃ©
```

### RÃ©sultats benchmark : Performance CPU

**Measurement** : CPU cycles par Ã©viction

```
Test: Ã‰viction de 10,000 clÃ©s

Algorithm    CPU Cycles    Time (ms)    Normalized
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
random       1,250,000     0.5          1.00x
lru (s=3)    1,875,000     0.75         1.50x
lru (s=5)    2,500,000     1.0          2.00x
lru (s=10)   4,375,000     1.75         3.50x
lfu (s=5)    3,125,000     1.25         2.50x
```

### RÃ©sultats benchmark : Latency distribution

**P50, P95, P99 latencies** (Ã©viction active) :

```
Policy         P50      P95      P99      Max
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
allkeys-random 0.1ms    0.3ms    0.5ms    2ms
allkeys-lru    0.5ms    1.5ms    3ms      8ms
allkeys-lfu    1.0ms    3.0ms    6ms      15ms
```

## Patterns d'accÃ¨s et choix de stratÃ©gie

### 1. AccÃ¨s temporels (Temporal Locality)

**CaractÃ©ristiques** :
- Les donnÃ©es rÃ©cemment accÃ©dÃ©es sont rÃ©accÃ©dÃ©es bientÃ´t
- Pattern typique : navigation utilisateur, sessions

**Exemple** :

```
User session flow:
Time â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
     Login â†’ Profile â†’ Settings â†’ Logout
     â”‚       â”‚         â”‚           â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     Toutes accÃ©dÃ©es dans une fenÃªtre courte
```

**Recommandation** : **LRU** âœ…

```conf
maxmemory-policy allkeys-lru
maxmemory-samples 5
```

**Justification** :
- LRU capture parfaitement la temporal locality
- DonnÃ©es rÃ©centes restent en cache
- Ã‰conomique en CPU

### 2. AccÃ¨s frÃ©quentiels (Frequency Locality)

**CaractÃ©ristiques** :
- Certaines donnÃ©es sont accÃ©dÃ©es beaucoup plus souvent
- Pattern stable dans le temps
- Pattern typique : top produits, top articles

**Exemple** :

```
E-commerce product views (1 semaine):

Product A: 50,000 views  â† Hot
Product B: 10,000 views
Product C: 5,000 views
Product D: 1,000 views
...
Product Z: 10 views      â† Cold
```

**Recommandation** : **LFU** âœ…

```conf
maxmemory-policy allkeys-lfu
lfu-log-factor 10
lfu-decay-time 5  # Decay plus lent pour stabilitÃ©
maxmemory-samples 10  # Meilleure prÃ©cision
```

**Justification** :
- LFU identifie et garde les donnÃ©es populaires
- RÃ©siste aux scans temporaires
- Optimal pour hot data stable

### 3. AccÃ¨s en rafales (Burst Access)

**CaractÃ©ristiques** :
- Pics d'accÃ¨s soudains sur certaines donnÃ©es
- PopularitÃ© change rapidement
- Pattern typique : trending topics, flash sales

**Exemple** :

```
News article views:
Hour 1:  Article X: 1000 views    â† Breaking news
Hour 2:  Article X: 50000 views   â† Trending
Hour 3:  Article X: 5000 views    â† DÃ©cline
Hour 24: Article X: 100 views     â† Cold
```

**Recommandation** : **LRU** âœ… ou **LFU avec decay rapide** âš ï¸

```conf
# Option 1: LRU (prÃ©fÃ©rÃ©)
maxmemory-policy allkeys-lru

# Option 2: LFU avec decay rapide
maxmemory-policy allkeys-lfu
lfu-decay-time 1  # 1 minute (trÃ¨s rapide)
lfu-log-factor 5  # Croissance rapide du compteur
```

**Justification** :
- LRU s'adapte rapidement aux changements
- LFU avec decay rapide oublie vite les anciens patterns

### 4. Scans pÃ©riodiques

**CaractÃ©ristiques** :
- Lecture complÃ¨te du dataset pÃ©riodiquement
- Pollue le cache LRU
- Pattern typique : batch jobs, analytics, backups

**Exemple** :

```
Normal access + periodic scan:

Time: 0-55 min   â†’ AccÃ¨s utilisateurs normaux
Time: 55-60 min  â†’ SCAN complet (backup/analytics)
                   â””â”€> Tous les keys deviennent "rÃ©cents"
                   â””â”€> Ã‰vacue le cache utile !

AprÃ¨s scan avec LRU:
Cache = [donnÃ©es scan inutiles] âŒ
      â‰  [donnÃ©es utilisateurs] âœ…
```

**Recommandation** : **LFU** âœ…

```conf
maxmemory-policy allkeys-lfu
lfu-log-factor 10
lfu-decay-time 5
```

**Justification** :
- LFU ignore les accÃ¨s uniques du scan
- Garde les vraies donnÃ©es frÃ©quentes
- LRU serait complÃ¨tement invalidÃ©

### 5. AccÃ¨s uniformes

**CaractÃ©ristiques** :
- Toutes les donnÃ©es ont la mÃªme probabilitÃ© d'accÃ¨s
- Pas de pattern particulier
- Pattern typique : rare en pratique

**Recommandation** : **Random** âœ…

```conf
maxmemory-policy allkeys-random
```

**Justification** :
- LRU et LFU n'apportent aucun bÃ©nÃ©fice
- Random est plus rapide
- SimplicitÃ© maximale

### 6. Working Set stable

**CaractÃ©ristiques** :
- Ensemble de donnÃ©es actives stable
- Taille << mÃ©moire totale
- Pattern typique : rÃ©fÃ©rentiels, configurations

**Exemple** :

```
Application configuration:
- 1000 configs utilisÃ©es activement
- 10000 configs totales
- Cache: 2000 configs

Working set (1000) tient facilement en cache
â†’ Peu d'Ã©victions
â†’ Algorithme importe peu
```

**Recommandation** : **LRU** ou **Random** âœ…

```conf
# Simple et efficace
maxmemory-policy allkeys-lru
maxmemory-samples 3  # Peut rÃ©duire
```

**Justification** :
- Peu d'Ã©victions = impact algorithme minimal
- Optimiser pour la performance

## Cas d'usage dÃ©taillÃ©s

### Cas 1 : Session Store

**Contexte** :
- Sessions utilisateur HTTP
- AccÃ¨s sÃ©quentiels (mÃªme user)
- TTL automatique

**Pattern d'accÃ¨s** :

```
User logs in â†’ Creates session
  â”œâ”€> Multiple requests in quick succession
  â”œâ”€> Idle period
  â””â”€> More requests or timeout

Temporal locality: Forte âœ…
Frequency locality: Faible
```

**Configuration recommandÃ©e** :

```conf
maxmemory 4gb
maxmemory-policy allkeys-lru  # â† Temporal locality
maxmemory-samples 5
```

**Alternative avec TTL** :

```conf
maxmemory-policy volatile-lru  # ProtÃ¨ge autres donnÃ©es
# Sessions avec TTL:
# SETEX session:abc123 1800 {data}
```

### Cas 2 : Cache API externe

**Contexte** :
- Mise en cache de rÃ©ponses API
- Certains endpoints trÃ¨s populaires
- CoÃ»t de rechargement Ã©levÃ©

**Pattern d'accÃ¨s** :

```
API endpoints popularity:
/api/trending    â†’ 10,000 req/min  â† Hot
/api/search      â†’ 5,000 req/min
/api/product/:id â†’ Variable
/api/obscure     â†’ 10 req/min      â† Cold

Frequency locality: Forte âœ…
Temporal locality: Moyenne
```

**Configuration recommandÃ©e** :

```conf
maxmemory 16gb
maxmemory-policy allkeys-lfu  # â† Frequency locality
lfu-log-factor 10
lfu-decay-time 10  # 10 min pour stabilitÃ©
maxmemory-samples 10
```

**Justification** :
- Endpoints populaires restent en cache
- Endpoints rares Ã©victÃ©s rapidement
- Maximise le hit ratio sur donnÃ©es coÃ»teuses

### Cas 3 : E-commerce product catalog

**Contexte** :
- Catalogue produits
- Top produits trÃ¨s consultÃ©s
- Scans pÃ©riodiques (inventory sync)

**Pattern d'accÃ¨s** :

```
Product views (Zipfian distribution):
Top 100 products  â†’ 80% des vues
Mid 1000 products â†’ 15% des vues
Tail 10000        â†’ 5% des vues

+ Scan complet toutes les heures (inventory)

Frequency locality: TrÃ¨s forte âœ…
Scan pollution: PrÃ©sente âŒ
```

**Configuration recommandÃ©e** :

```conf
maxmemory 32gb
maxmemory-policy allkeys-lfu  # â† RÃ©siste au scan
lfu-log-factor 10
lfu-decay-time 15  # Decay lent
maxmemory-samples 15  # Haute prÃ©cision
```

**Impact du scan** :

```
With LRU:
- Scan invalide tout le cache âŒ
- Hit ratio drops 90% â†’ 20% pendant sync

With LFU:
- Scan n'affecte pas les compteurs significativement âœ…
- Hit ratio stable ~85%
```

### Cas 4 : Real-time analytics

**Contexte** :
- Compteurs temps rÃ©el
- Dashboards
- Toutes les mÃ©triques aussi importantes

**Pattern d'accÃ¨s** :

```
Metrics:
counter:sales:today
counter:users:online
counter:errors:rate
...

Tous accÃ©dÃ©s rÃ©guliÃ¨rement et uniformÃ©ment

Locality: Aucune
```

**Configuration recommandÃ©e** :

```conf
maxmemory 8gb
maxmemory-policy allkeys-random  # â† Pas de pattern
# Ou noeviction si toutes les mÃ©triques critiques
```

**Justification** :
- Aucun algorithme intelligent n'apporte de bÃ©nÃ©fice
- Random = performance maximale
- Ou noeviction si perte inacceptable

### Cas 5 : CDN / Image cache

**Contexte** :
- Cache d'images/fichiers statiques
- Quelques images trÃ¨s populaires (logo, avatar default)
- Longue traÃ®ne d'images rares

**Pattern d'accÃ¨s** :

```
Image requests:
logo.png          â†’ 1M req/day  â† Hot
avatar_default    â†’ 500k req/day
user_avatar_123   â†’ 10 req/day   â† Cold
...

Frequency locality: TrÃ¨s forte âœ…
Temporal locality: Faible
```

**Configuration recommandÃ©e** :

```conf
maxmemory 128gb  # Large cache
maxmemory-policy allkeys-lfu
lfu-log-factor 100  # Croissance trÃ¨s lente
lfu-decay-time 1440  # 24h (trÃ¨s lent)
maxmemory-samples 20  # PrÃ©cision maximale
```

**Justification** :
- Images populaires doivent TOUJOURS Ãªtre en cache
- LFU avec dÃ©cay lent = stabilitÃ© maximale
- Tail images Ã©victÃ©es aggressivement

### Cas 6 : Rate Limiting

**Contexte** :
- Compteurs de rate limiting par IP/user
- AccÃ¨s en rafales
- TTL court

**Pattern d'accÃ¨s** :

```
Rate limit counters:
user:123:rate â†’ Multiple accesses in seconds
              â†’ Then TTL expires (60s)

Temporal locality: TrÃ¨s forte (bursts) âœ…
Frequency locality: Faible
```

**Configuration recommandÃ©e** :

```conf
maxmemory 2gb
maxmemory-policy volatile-lru  # Avec TTL mandatory
maxmemory-samples 5
```

**Alternative** :

```python
# All rate limit keys have TTL
redis.incr(f"rate:{user_id}")
redis.expire(f"rate:{user_id}", 60)

# volatile-lru Ã©vacue les anciens compteurs
```

## Tuning et optimisation

### Tuning LRU

#### maxmemory-samples

**Impact sur la qualitÃ©** :

```python
# Test empirique
def test_lru_quality(samples):
    # Workload 80/20
    results = benchmark(
        policy='allkeys-lru',
        samples=samples,
        workload='zipf'
    )
    return results['hit_ratio']

samples=3:  78.2% hit ratio
samples=5:  82.4% hit ratio  â† DÃ©faut
samples=10: 87.1% hit ratio
samples=20: 89.5% hit ratio
```

**Recommandations** :

```conf
# Development/testing
maxmemory-samples 3

# Production standard
maxmemory-samples 5  # DÃ©faut

# Production critique (hit ratio >> CPU)
maxmemory-samples 10

# High-frequency trading, CDN edge
maxmemory-samples 20
```

### Tuning LFU

#### lfu-log-factor

ContrÃ´le la vitesse de croissance du compteur :

```conf
# Croissance rapide (favorise donnÃ©es rÃ©centes)
lfu-log-factor 1
# 100 hits â†’ counter ~50

# Croissance moyenne (dÃ©faut)
lfu-log-factor 10
# 100 hits â†’ counter ~10

# Croissance lente (favorise donnÃ©es anciennes)
lfu-log-factor 100
# 100 hits â†’ counter ~3
```

**Cas d'usage** :

```
Pattern volatil (trending topics):
â””â”€> lfu-log-factor 5  # RÃ©agit vite

Pattern stable (rÃ©fÃ©rentiels):
â””â”€> lfu-log-factor 50  # StabilitÃ©

Pattern mixte:
â””â”€> lfu-log-factor 10  # DÃ©faut
```

#### lfu-decay-time

ContrÃ´le la vitesse d'oubli :

```conf
# Oubli rapide (adaptatif)
lfu-decay-time 1  # 1 minute
# Old hot key devient cold en ~10 minutes

# Oubli moyen (dÃ©faut)
lfu-decay-time 5  # 5 minutes
# Old hot key devient cold en ~50 minutes

# Oubli lent (stable)
lfu-decay-time 60  # 1 heure
# Old hot key devient cold en ~10 heures
```

**Cas d'usage** :

```
Charge fluctuante (flash sales):
â””â”€> lfu-decay-time 1  # Adaptation rapide

Charge stable (API gateway):
â””â”€> lfu-decay-time 10  # Balance

Working set quasi-permanent (CDN):
â””â”€> lfu-decay-time 60  # StabilitÃ© maximale
```

#### Combinaisons recommandÃ©es

**Pattern: Highly volatile** (social media trends)

```conf
maxmemory-policy allkeys-lfu
lfu-log-factor 5      # Croissance rapide
lfu-decay-time 1      # Oubli rapide
maxmemory-samples 5
```

**Pattern: Stable hot data** (CDN edge cache)

```conf
maxmemory-policy allkeys-lfu
lfu-log-factor 50     # Croissance lente
lfu-decay-time 60     # Oubli trÃ¨s lent
maxmemory-samples 20  # PrÃ©cision max
```

**Pattern: Balanced** (API caching)

```conf
maxmemory-policy allkeys-lfu
lfu-log-factor 10     # DÃ©faut
lfu-decay-time 5      # DÃ©faut
maxmemory-samples 10
```

### Optimisations communes

#### Lazy freeing

Toujours activer pour toutes les politiques :

```conf
lazyfree-lazy-eviction yes
lazyfree-lazy-expire yes
lazyfree-lazy-server-del yes
```

**Impact** :

```
Without lazyfree:
- Ã‰viction grande clÃ© â†’ 100ms blocage âŒ

With lazyfree:
- Ã‰viction grande clÃ© â†’ <1ms (async) âœ…
```

#### Active defrag

Avec Ã©viction active, la fragmentation peut s'accumuler :

```conf
activedefrag yes
active-defrag-cycle-min 5
active-defrag-cycle-max 75
active-defrag-threshold-lower 10
```

## Migration entre stratÃ©gies

### Ã€ chaud (sans downtime)

```bash
# Ã‰tat actuel
redis-cli CONFIG GET maxmemory-policy
# "allkeys-lru"

# Changement Ã  chaud
redis-cli CONFIG SET maxmemory-policy allkeys-lfu

# Persister
redis-cli CONFIG REWRITE
```

**Impact** :

```
Changement LRU â†’ LFU:
- Counters initialisÃ©s Ã  LFU_INIT_VAL (5)
- Anciennes donnÃ©es avantagÃ©es temporairement
- Normalisation aprÃ¨s quelques minutes

Changement LFU â†’ LRU:
- Timestamp initialisÃ© Ã  now()
- Toutes les clÃ©s paraissent "rÃ©centes"
- Normalisation aprÃ¨s quelques accÃ¨s
```

### Migration progressive (A/B testing)

```python
# Setup A/B test
def route_to_redis(user_id):
    if hash(user_id) % 2 == 0:
        return redis_lru  # Instance LRU
    else:
        return redis_lfu  # Instance LFU

# Mesure comparative
def measure_hit_ratio(redis_instance, duration=3600):
    stats_before = redis_instance.info('stats')
    time.sleep(duration)
    stats_after = redis_instance.info('stats')

    hits_delta = stats_after['keyspace_hits'] - stats_before['keyspace_hits']
    misses_delta = stats_after['keyspace_misses'] - stats_before['keyspace_misses']

    return hits_delta / (hits_delta + misses_delta)

# RÃ©sultats
print(f"LRU hit ratio: {measure_hit_ratio(redis_lru)}")
print(f"LFU hit ratio: {measure_hit_ratio(redis_lfu)}")
```

### Checklist de migration

```
Avant migration:
â˜ Benchmark hit ratio actuel
â˜ Mesure CPU baseline
â˜ Mesure P99 latency
â˜ Identifier workload pattern

Pendant migration:
â˜ CONFIG SET en test
â˜ Observer mÃ©triques 1 heure
â˜ Comparer avec baseline
â˜ A/B test si critique

AprÃ¨s migration:
â˜ CONFIG REWRITE si satisfait
â˜ Monitor 24h
â˜ Ajuster tuning si nÃ©cessaire
â˜ Documenter changement
```

## MÃ©triques de dÃ©cision

### Tableau rÃ©capitulatif

| CritÃ¨re | LRU | LFU | Random | Recommandation |
|---------|-----|-----|--------|----------------|
| **Hit ratio (80/20)** | 82% | 87% | 60% | LFU gagne |
| **Hit ratio (uniforme)** | 51% | 51% | 50% | Ã‰galitÃ© |
| **CPU overhead** | +15% | +25% | +2% | Random gagne |
| **Latency P99** | 3ms | 6ms | 0.5ms | Random gagne |
| **RÃ©sistance scans** | âŒ Faible | âœ… Forte | âš ï¸ N/A | LFU gagne |
| **AdaptabilitÃ©** | âœ… Rapide | âš ï¸ Lent | âš ï¸ N/A | LRU gagne |
| **SimplicitÃ©** | âœ… Simple | âš ï¸ Complexe | âœ… TrÃ¨s simple | Random gagne |
| **Overhead mÃ©moire** | 3 bytes | 3 bytes | 0 bytes | Random gagne |
| **Tuning requis** | Minimal | Moyen | Aucun | Random gagne |

### Arbre de dÃ©cision

```
Avez-vous un pattern d'accÃ¨s identifiable ?
â”œâ”€ NON â†’ Random
â””â”€ OUI
   â”‚
   â”œâ”€ Pattern temporel (rÃ©cence) ?
   â”‚  â””â”€ OUI â†’ LRU
   â”‚
   â”œâ”€ Pattern frÃ©quentiel (popularitÃ©) ?
   â”‚  â””â”€ OUI
   â”‚     â”‚
   â”‚     â”œâ”€ Stable dans le temps ?
   â”‚     â”‚  â””â”€ OUI â†’ LFU (decay lent)
   â”‚     â”‚
   â”‚     â””â”€ Volatile (trending) ?
   â”‚        â””â”€ OUI â†’ LRU ou LFU (decay rapide)
   â”‚
   â””â”€ Scans pÃ©riodiques ?
      â””â”€ OUI â†’ LFU
```

### Scoring system

Ã‰valuez votre cas d'usage :

```python
def choose_policy(answers):
    score_lru = 0
    score_lfu = 0
    score_random = 0

    # Q1: Temporal locality forte ?
    if answers['temporal_locality'] == 'strong':
        score_lru += 3

    # Q2: Frequency locality forte ?
    if answers['frequency_locality'] == 'strong':
        score_lfu += 3

    # Q3: Scans pÃ©riodiques ?
    if answers['periodic_scans']:
        score_lfu += 2
        score_lru -= 2

    # Q4: Pattern stable ?
    if answers['stable_pattern']:
        score_lfu += 2

    # Q5: VolatilitÃ© (trending) ?
    if answers['volatile_pattern']:
        score_lru += 2
        score_lfu += 1

    # Q6: Performance critique ?
    if answers['performance_critical']:
        score_random += 2

    # Q7: Hit ratio critique ?
    if answers['hit_ratio_critical']:
        score_lfu += 2

    # Q8: Toutes donnÃ©es Ã©quiprobables ?
    if answers['uniform_access']:
        score_random += 5

    policies = [
        ('LRU', score_lru),
        ('LFU', score_lfu),
        ('Random', score_random)
    ]

    return max(policies, key=lambda x: x[1])[0]
```

## Conclusion

### RÃ¨gles d'or

1. **Pour 80% des cas : LRU**
   - Simple, efficace, prÃ©dictible
   - Bon compromis performance/qualitÃ©

2. **Pour hot data stable : LFU**
   - Meilleur hit ratio
   - RÃ©siste aux scans
   - NÃ©cessite tuning

3. **Pour pattern uniforme : Random**
   - Performance maximale
   - SimplicitÃ© absolue
   - Aucun overhead

### Configuration par dÃ©faut recommandÃ©e

**Pour dÃ©marrer** :

```conf
# Configuration universelle sÃ»re
maxmemory 8gb
maxmemory-policy allkeys-lru
maxmemory-samples 5
lazyfree-lazy-eviction yes
```

**AprÃ¨s analyse** :

```bash
# 1. Mesurer pattern
redis-cli MONITOR | analyze_pattern.py

# 2. Benchmark
./benchmark.sh --policies lru,lfu --duration 3600

# 3. Choisir selon rÃ©sultats
redis-cli CONFIG SET maxmemory-policy allkeys-lfu

# 4. Tuner
redis-cli CONFIG SET lfu-log-factor 10
redis-cli CONFIG SET lfu-decay-time 5

# 5. Monitorer et ajuster
```

### Ne pas oublier

- **Mesurer avant d'optimiser** : Benchmarker est obligatoire
- **Le contexte est roi** : Pas de solution universelle
- **Tuning itÃ©ratif** : Ajuster progressivement
- **Monitoring continu** : Hit ratio, CPU, latency
- **Documenter** : Justifier les choix de configuration

Le choix entre LRU, LFU et Random n'est pas une question de "meilleur algorithme" mais de **meilleur ajustement Ã  votre workload**. Prenez le temps de caractÃ©riser votre pattern d'accÃ¨s et de benchmarker.

La section suivante explorera les namespaces et bonnes pratiques de nommage des clÃ©s.

â­ï¸ [Namespaces et bonnes pratiques de nommage (Key patterns)](/04-cycle-vie-donnee/05-namespaces-bonnes-pratiques-nommage.md)
