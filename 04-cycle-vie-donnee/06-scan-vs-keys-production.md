üîù Retour au [Sommaire](/SOMMAIRE.md)

# 4.6 SCAN vs KEYS : Ne jamais bloquer la production

## Introduction

**KEYS est l'une des commandes les plus dangereuses de Redis en production.** Elle peut transformer votre syst√®me haute performance en goulot d'√©tranglement complet en une seule requ√™te. Pourtant, elle reste tentante par sa simplicit√© apparente : `KEYS user:*` retourne instantan√©ment toutes les cl√©s correspondantes... ou du moins c'est ce qu'on croit.

Cette section explique pourquoi KEYS est dangereuse, comment SCAN r√©sout le probl√®me, et comment l'utiliser correctement dans tous les contextes de production.

## Le probl√®me de KEYS

### Comportement bloquant

```bash
# Commande simple en apparence
KEYS user:*
```

**Ce qui se passe r√©ellement** :

```
Client ‚Üí KEYS user:*
    ‚Üì
Redis re√ßoit commande
    ‚Üì
BLOQUE le serveur complet
    ‚Üì
Parcourt TOUT le keyspace (dict principal)
    ‚Üì
Pour chaque cl√©:
  ‚îú‚îÄ V√©rifie pattern
  ‚îú‚îÄ Ajoute √† r√©sultat si match
  ‚îî‚îÄ Continue
    ‚Üì
Toutes les cl√©s parcourues (10M cl√©s = ~1 seconde)
    ‚Üì
Retourne r√©sultat complet
    ‚Üì
D√âBLOQUE le serveur
```

### Impact en production

**Sc√©nario r√©el** :

```
Production Redis:
‚îú‚îÄ 10 millions de cl√©s
‚îú‚îÄ 50,000 requ√™tes/seconde
‚îî‚îÄ Quelqu'un ex√©cute: KEYS *

R√©sultat:
Time 0ms:    KEYS d√©marre
Time 10ms:   100,000 requ√™tes en attente
Time 50ms:   500,000 requ√™tes en attente (timeout commence)
Time 100ms:  1,000,000 requ√™tes en attente
Time 500ms:  Clients timeout en masse
Time 1000ms: KEYS termine, retourne 10M cl√©s
Time 1001ms: Avalanche de retries
Time 1005ms: Redis overload, crash possible
```

**M√©triques observ√©es** :

```bash
# Avant KEYS
redis-cli INFO stats | grep instantaneous_ops_per_sec
instantaneous_ops_per_sec:50000

# Pendant KEYS (1 seconde)
instantaneous_ops_per_sec:0  # ZERO !

# Apr√®s KEYS
instantaneous_ops_per_sec:150000  # Pic de retries
```

### Pourquoi KEYS est O(N)

**Code interne simplifi√©** :

```c
// redis/src/db.c (simplifi√©)
void keysCommand(client *c) {
    dictIterator *di;
    dictEntry *de;
    sds pattern = c->argv[1]->ptr;
    int allkeys = (pattern[0] == '*' && patternlen == 1);

    // It√©rateur sur TOUT le dictionnaire
    di = dictGetIterator(c->db->dict);

    // Parcourt TOUTES les cl√©s
    while((de = dictNext(di)) != NULL) {
        sds key = dictGetKey(de);
        robj *keyobj;

        // V√©rifie pattern
        if (allkeys || stringmatchlen(pattern, patternlen,
                                       key, sdslen(key), 0)) {
            keyobj = createStringObject(key, sdslen(key));

            // V√©rifie expiration
            if (!keyIsExpired(c->db, keyobj)) {
                addReplyBulk(c, keyobj);
                numkeys++;
            }
            decrRefCount(keyobj);
        }
    }
    dictReleaseIterator(di);
}
```

**Complexit√©** : `O(N)` o√π N = nombre total de cl√©s dans la DB

**Temps d'ex√©cution estim√©** :

```
1,000 cl√©s      ‚Üí ~0.1 ms   ‚úÖ Acceptable (dev)
10,000 cl√©s     ‚Üí ~1 ms     ‚ö†Ô∏è Limite
100,000 cl√©s    ‚Üí ~10 ms    ‚ùå Dangereux
1,000,000 cl√©s  ‚Üí ~100 ms   ‚ùå Inacceptable
10,000,000 cl√©s ‚Üí ~1000 ms  ‚ùå Catastrophique
```

### Architecture single-threaded

Redis est **single-threaded** pour les commandes :

```
Thread principal Redis:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Boucle √©v√©nements           ‚îÇ
‚îÇ                             ‚îÇ
‚îÇ while(1) {                  ‚îÇ
‚îÇ   cmd = getNextCommand()    ‚îÇ
‚îÇ   execute(cmd)              ‚îÇ ‚Üê BLOQUE ici pendant KEYS
‚îÇ   sendResponse()            ‚îÇ
‚îÇ }                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Pendant KEYS:
- Aucune autre commande n'est trait√©e
- GET, SET, INCR, etc. tous bloqu√©s
- Monitoring bloqu√©
- R√©plication bloqu√©e
```

**D√©monstration** :

```bash
# Terminal 1
redis-cli KEYS *
# ... attend 1 seconde

# Terminal 2 (pendant KEYS)
redis-cli PING
# ... timeout apr√®s 1 seconde
# (nil)
```

## La solution : SCAN

### Principe fondamental

SCAN est un **it√©rateur incr√©mental** qui parcourt le keyspace par petits morceaux :

```
KEYS:
Client ‚Üí KEYS * ‚Üí [ATTEND 1s] ‚Üí [10M cl√©s]

SCAN:
Client ‚Üí SCAN 0 ‚Üí [10ms] ‚Üí [100 cl√©s + cursor]
       ‚Üí SCAN cursor ‚Üí [10ms] ‚Üí [100 cl√©s + cursor]
       ‚Üí SCAN cursor ‚Üí [10ms] ‚Üí [100 cl√©s + cursor]
       ...
       (100,000 it√©rations, mais non-bloquant)
```

### Garanties de SCAN

SCAN offre les garanties suivantes :

1. **Non bloquant** : Chaque appel est O(1) moyen
2. **Complet** : Toutes les cl√©s pr√©sentes du d√©but √† la fin sont retourn√©es
3. **Sans √©tat c√¥t√© serveur** : Le cursor est opaque mais sans √©tat serveur
4. **Pas de duplicatas garantis** : Sauf si cl√©s modifi√©es pendant scan

**Garanties formelles** :

```
Cl√© pr√©sente de t0 √† tN ‚Üí Retourn√©e AU MOINS 1 fois
Cl√© ajout√©e pendant scan ‚Üí Peut √™tre retourn√©e ou non
Cl√© supprim√©e pendant scan ‚Üí Peut √™tre retourn√©e ou non
```

### Signature de SCAN

```bash
SCAN cursor [MATCH pattern] [COUNT count] [TYPE type]
```

**Param√®tres** :

- `cursor` : Position dans l'it√©ration (0 pour d√©marrer)
- `MATCH` : Pattern de filtrage (optionnel)
- `COUNT` : Hint du nombre d'√©l√©ments par it√©ration (d√©faut: 10)
- `TYPE` : Filtre par type de donn√©es (Redis 6.0+)

**Retour** :

```bash
SCAN 0
# 1) "17"        ‚Üê Nouveau cursor
# 2) 1) "key:1"  ‚Üê Cl√©s de cette it√©ration
#    2) "key:2"
#    3) "key:3"
```

## M√©canisme interne de SCAN

### Structure du dictionnaire Redis

Redis utilise une **hash table** avec rehashing incr√©mental :

```c
typedef struct dict {
    dictType *type;
    void *privdata;
    dictht ht[2];          // Deux hash tables pour rehashing
    long rehashidx;        // -1 si pas de rehashing en cours
    int iterators;
} dict;

typedef struct dictht {
    dictEntry **table;     // Array de buckets
    unsigned long size;    // Taille de la table (puissance de 2)
    unsigned long sizemask;// size - 1 (pour masquage rapide)
    unsigned long used;    // Nombre d'entr√©es
} dictht;
```

**Visualisation** :

```
Hash Table (size = 8):
Index   Bucket
  0  ‚Üí  [key:1] ‚Üí [key:9] ‚Üí NULL
  1  ‚Üí  [key:2] ‚Üí NULL
  2  ‚Üí  NULL
  3  ‚Üí  [key:3] ‚Üí [key:11] ‚Üí NULL
  4  ‚Üí  [key:4] ‚Üí NULL
  5  ‚Üí  NULL
  6  ‚Üí  [key:6] ‚Üí NULL
  7  ‚Üí  [key:7] ‚Üí [key:15] ‚Üí NULL
```

### Le cursor : Bits invers√©s

Le cursor de SCAN n'est **pas s√©quentiel** mais utilise les **bits invers√©s** :

```
Taille table = 8 (2^3)
Bits n√©cessaires = 3

Ordre de parcours (bits invers√©s):
Binary    Decimal   Bucket
000    ‚Üí  0      ‚Üí  0
100    ‚Üí  4      ‚Üí  4
010    ‚Üí  2      ‚Üí  2
110    ‚Üí  6      ‚Üí  6
001    ‚Üí  1      ‚Üí  1
101    ‚Üí  5      ‚Üí  5
011    ‚Üí  3      ‚Üí  3
111    ‚Üí  7      ‚Üí  7
```

**Pourquoi ?** Permet de g√©rer le rehashing sans re-scanner :

```
Rehashing de 8 ‚Üí 16 buckets:

Bucket 0 (000) ‚Üí se split en:
  - Bucket 0 (0000)
  - Bucket 8 (1000)

Si on a d√©j√† visit√© 0 avec cursor=000:
  - Prochain cursor = 100 (bucket 4)
  - Bucket 8 (1000) sera visit√© car 100 < 1000

Pas de duplication ni de miss !
```

### Algorithme de SCAN

**Code simplifi√©** :

```c
void scanGenericCommand(client *c, robj *o, unsigned long cursor) {
    dictIterator *di;
    dictEntry *de;
    list *keys = listCreate();
    long maxiterations = COUNT_DEFAULT;

    // It√®re sur les buckets
    do {
        // Calcule bucket actuel
        unsigned long bucket = cursor & dictht->sizemask;

        // Parcourt la cha√Æne du bucket
        de = dictht->table[bucket];
        while (de) {
            sds key = dictGetKey(de);

            // Filtre par pattern si n√©cessaire
            if (pattern == NULL ||
                stringmatchlen(pattern, patternlen, key, sdslen(key), 0)) {
                listAddNodeTail(keys, createStringObject(key, sdslen(key)));
            }

            de = de->next;
            maxiterations--;

            if (maxiterations == 0) break;
        }

        // Prochain cursor (bits invers√©s)
        cursor = (cursor | (cursor >> 1)) + 1;

    } while (cursor != 0 && maxiterations > 0);

    // Retourne nouveau cursor + cl√©s
    addReplyArrayLen(c, 2);
    addReplyBulkLongLong(c, cursor);
    addReplyArrayLen(c, listLength(keys));
    // ... retourne cl√©s
}
```

### COUNT : Un hint, pas une garantie

Le param√®tre `COUNT` est un **hint** :

```bash
SCAN 0 COUNT 100
# Peut retourner:
# - 0 cl√©s (bucket vide)
# - 50 cl√©s (moins que COUNT)
# - 100 cl√©s (COUNT exact)
# - 150 cl√©s (plus que COUNT si longue cha√Æne)
```

**Comportement** :

```
COUNT contr√¥le le nombre d'it√©rations, PAS le nombre de cl√©s:

COUNT 10:
- Parcourt ~10 buckets
- Retourne toutes les cl√©s de ces buckets
- Si bucket a 20 cl√©s ‚Üí retourne 20 cl√©s

COUNT 100:
- Parcourt ~100 buckets
- Plus de cl√©s retourn√©es en moyenne
- Plus de CPU par appel
- Moins d'appels totaux
```

**Recommandations** :

```bash
# D√©faut (petits volumes)
SCAN cursor COUNT 10

# Production (√©quilibr√©)
SCAN cursor COUNT 100

# Batch processing (performance)
SCAN cursor COUNT 1000

# Syst√®me tr√®s charg√© (latence min)
SCAN cursor COUNT 5
```

## Utilisation pratique de SCAN

### It√©ration compl√®te basique

```python
import redis

def scan_all_keys(redis_client):
    """It√®re sur toutes les cl√©s"""
    cursor = 0
    keys = []

    while True:
        # SCAN retourne (new_cursor, keys_list)
        cursor, batch = redis_client.scan(cursor, count=100)
        keys.extend(batch)

        # cursor == 0 signifie fin d'it√©ration
        if cursor == 0:
            break

    return keys

# Utilisation
r = redis.Redis()
all_keys = scan_all_keys(r)
print(f"Total keys: {len(all_keys)}")
```

### Avec pattern matching

```python
def scan_pattern(redis_client, pattern, count=100):
    """Scanne cl√©s avec pattern"""
    cursor = 0
    matched_keys = []

    while True:
        cursor, keys = redis_client.scan(cursor, match=pattern, count=count)
        matched_keys.extend(keys)

        if cursor == 0:
            break

    return matched_keys

# Utilisation
user_keys = scan_pattern(r, "user:*")
cache_keys = scan_pattern(r, "cache:api:*")
```

### Generator pattern (√©conomie m√©moire)

```python
def scan_keys_generator(redis_client, pattern=None, count=100):
    """Generator pour it√©ration m√©moire-efficace"""
    cursor = 0

    while True:
        cursor, keys = redis_client.scan(cursor, match=pattern, count=count)

        # Yield chaque cl√© individuellement
        for key in keys:
            yield key

        if cursor == 0:
            break

# Utilisation
for key in scan_keys_generator(r, "user:*:profile"):
    profile = r.get(key)
    process(profile)

# Ou avec limite
from itertools import islice
first_1000 = list(islice(scan_keys_generator(r, "cache:*"), 1000))
```

### Traitement par batch

```python
def scan_and_process_batch(redis_client, pattern, processor, batch_size=100):
    """Traite cl√©s par batch"""
    cursor = 0
    total_processed = 0

    while True:
        cursor, keys = redis_client.scan(cursor, match=pattern, count=batch_size)

        if keys:
            # Traite batch
            processor(keys)
            total_processed += len(keys)

        if cursor == 0:
            break

    return total_processed

# Utilisation
def delete_batch(keys):
    if keys:
        r.delete(*keys)

deleted = scan_and_process_batch(r, "temp:*", delete_batch)
print(f"Deleted {deleted} keys")
```

### Avec timeout (protection)

```python
import time

def scan_with_timeout(redis_client, pattern, timeout_seconds=10, count=100):
    """SCAN avec timeout pour √©viter boucles infinies"""
    cursor = 0
    keys = []
    start_time = time.time()

    while True:
        # V√©rifie timeout
        if time.time() - start_time > timeout_seconds:
            raise TimeoutError(f"SCAN exceeded {timeout_seconds}s timeout")

        cursor, batch = redis_client.scan(cursor, match=pattern, count=count)
        keys.extend(batch)

        if cursor == 0:
            break

    return keys
```

### Parall√©lisation (avec pr√©cautions)

```python
from concurrent.futures import ThreadPoolExecutor

def scan_parallel(redis_client, pattern, num_workers=4):
    """SCAN parall√®le (exp√©rimental)"""
    # Note: SCAN n'est pas vraiment parall√©lisable
    # Mais on peut traiter les r√©sultats en parall√®le

    def worker(keys_batch):
        results = []
        for key in keys_batch:
            value = redis_client.get(key)
            results.append((key, value))
        return results

    # SCAN s√©quentiel (pas d'autre choix)
    cursor = 0
    all_results = []

    with ThreadPoolExecutor(max_workers=num_workers) as executor:
        while True:
            cursor, keys = redis_client.scan(cursor, match=pattern, count=100)

            if keys:
                # Traitement parall√®le des cl√©s
                future = executor.submit(worker, keys)
                all_results.append(future)

            if cursor == 0:
                break

    # Collecte r√©sultats
    results = []
    for future in all_results:
        results.extend(future.result())

    return results
```

## SCAN avec MATCH vs filtrage client

### O√π se fait le filtrage ?

**MATCH c√¥t√© serveur** :

```bash
SCAN 0 MATCH user:* COUNT 100
```

**Processus** :

```
1. Redis scanne ~100 buckets
2. Pour chaque cl√©:
   ‚îú‚îÄ V√©rifie pattern "user:*"
   ‚îú‚îÄ Si match: Ajoute au r√©sultat
   ‚îî‚îÄ Si non-match: Ignore
3. Retourne uniquement les cl√©s match√©es
```

**Filtrage c√¥t√© client** :

```python
# R√©cup√®re toutes les cl√©s
cursor, keys = r.scan(cursor, count=100)

# Filtre c√¥t√© client
user_keys = [k for k in keys if k.startswith('user:')]
```

### Performance compar√©e

**Benchmark** :

```python
import time

# Setup: 100k cl√©s, 10k matchent "user:*"
for i in range(100000):
    if i < 10000:
        r.set(f"user:{i}", "data")
    else:
        r.set(f"other:{i}", "data")

# Test 1: MATCH c√¥t√© serveur
start = time.time()
keys = []
cursor = 0
while True:
    cursor, batch = r.scan(cursor, match="user:*", count=100)
    keys.extend(batch)
    if cursor == 0:
        break
elapsed_server = time.time() - start

# Test 2: Filtrage client
start = time.time()
keys = []
cursor = 0
while True:
    cursor, batch = r.scan(cursor, count=100)
    keys.extend([k for k in batch if k.startswith(b'user:')])
    if cursor == 0:
        break
elapsed_client = time.time() - start

print(f"Server MATCH: {elapsed_server:.2f}s")
print(f"Client filter: {elapsed_client:.2f}s")
```

**R√©sultats typiques** :

```
100k cl√©s totales, 10k matchent:

Server MATCH: 0.45s
Client filter: 0.52s

Diff√©rence: ~15% plus lent c√¥t√© client

Raison:
- Transfert r√©seau de 100k cl√©s vs 10k cl√©s
- Parsing 100k r√©ponses vs 10k r√©ponses
```

**Recommandation** : Toujours utiliser `MATCH` c√¥t√© serveur si possible.

### Limitations de MATCH

MATCH supporte les wildcards simples :

```bash
# Wildcards support√©s
SCAN 0 MATCH user:*          # ‚úÖ Pr√©fixe
SCAN 0 MATCH *:profile       # ‚úÖ Suffixe
SCAN 0 MATCH user:*:profile  # ‚úÖ Milieu
SCAN 0 MATCH user:?00:*      # ‚úÖ ? = 1 caract√®re
SCAN 0 MATCH user:[0-9]*     # ‚úÖ Range

# Limitations
SCAN 0 MATCH user:{123|456}:*  # ‚ùå Pas de regex
SCAN 0 MATCH ^user.*$          # ‚ùå Pas d'ancres
```

**Pour patterns complexes** : Filtrer c√¥t√© client

```python
import re

def scan_with_regex(redis_client, regex_pattern, count=100):
    """SCAN avec regex c√¥t√© client"""
    pattern = re.compile(regex_pattern)
    cursor = 0
    matched = []

    while True:
        cursor, keys = redis_client.scan(cursor, count=count)

        # Filtre regex c√¥t√© client
        for key in keys:
            if pattern.match(key.decode() if isinstance(key, bytes) else key):
                matched.append(key)

        if cursor == 0:
            break

    return matched

# Utilisation
# Trouve users avec ID √† 3 chiffres exactement
users = scan_with_regex(r, r'^user:\d{3}:profile$')
```

## TYPE filter (Redis 6.0+)

### Filtrage par type de donn√©es

```bash
# Scanne uniquement les strings
SCAN 0 TYPE string

# Scanne uniquement les hashes
SCAN 0 TYPE hash

# Scanne uniquement les lists
SCAN 0 TYPE list

# Combine avec MATCH
SCAN 0 MATCH user:* TYPE hash COUNT 100
```

**Types support√©s** :

```bash
string
list
set
zset
hash
stream
```

**Exemple pratique** :

```python
def scan_by_type(redis_client, data_type, pattern=None, count=100):
    """SCAN filtr√© par type"""
    cursor = 0
    keys = []

    while True:
        args = {'cursor': cursor, 'count': count, '_type': data_type}
        if pattern:
            args['match'] = pattern

        cursor, batch = redis_client.scan(**args)
        keys.extend(batch)

        if cursor == 0:
            break

    return keys

# Utilisation
hash_keys = scan_by_type(r, 'hash', 'user:*')
string_keys = scan_by_type(r, 'string', 'cache:*')
```

**Performance** :

```
Sans TYPE:
- Scanne toutes cl√©s
- Application v√©rifie type avec TYPE command
- 2 requ√™tes par cl√©

Avec TYPE:
- Filtre c√¥t√© serveur
- Retourne uniquement type demand√©
- 1 requ√™te par batch
```

## Variantes de SCAN

### SSCAN : Scan de Set

```bash
# SSCAN <key> <cursor> [MATCH pattern] [COUNT count]
SSCAN myset 0 MATCH member:* COUNT 100
```

**Exemple** :

```python
def sscan_all(redis_client, key, pattern=None, count=100):
    """It√®re sur tous membres d'un Set"""
    cursor = 0
    members = []

    while True:
        cursor, batch = redis_client.sscan(key, cursor, match=pattern, count=count)
        members.extend(batch)

        if cursor == 0:
            break

    return members

# Utilisation
# Set avec 10M membres
r.sadd('users:active', *range(10000000))

# SSCAN au lieu de SMEMBERS (bloquant)
active_users = sscan_all(r, 'users:active')
```

### HSCAN : Scan de Hash

```bash
# HSCAN <key> <cursor> [MATCH pattern] [COUNT count]
HSCAN myhash 0 MATCH field:* COUNT 100
```

**Retour** : Alternance field/value

```bash
HSCAN user:123 0
# 1) "0"
# 2) 1) "name"
#    2) "Alice"
#    3) "email"
#    4) "alice@example.com"
```

**Exemple** :

```python
def hscan_all(redis_client, key, pattern=None, count=100):
    """It√®re sur tous champs d'un Hash"""
    cursor = 0
    items = {}

    while True:
        cursor, data = redis_client.hscan(key, cursor, match=pattern, count=count)
        items.update(data)

        if cursor == 0:
            break

    return items

# Utilisation
# Hash avec 1M champs
for i in range(1000000):
    r.hset('bigdata', f'field:{i}', f'value{i}')

# HSCAN au lieu de HGETALL (bloquant)
data = hscan_all(r, 'bigdata', match='field:100*')
```

### ZSCAN : Scan de Sorted Set

```bash
# ZSCAN <key> <cursor> [MATCH pattern] [COUNT count]
ZSCAN myzset 0 MATCH member:* COUNT 100
```

**Retour** : Alternance member/score

```bash
ZSCAN leaderboard 0
# 1) "0"
# 2) 1) "player1"
#    2) "1000"
#    3) "player2"
#    4) "950"
```

**Exemple** :

```python
def zscan_all(redis_client, key, pattern=None, count=100):
    """It√®re sur tous membres d'un Sorted Set"""
    cursor = 0
    items = {}

    while True:
        cursor, data = redis_client.zscan(key, cursor, match=pattern, count=count)
        items.update(data)

        if cursor == 0:
            break

    return items

# Utilisation
zset_data = zscan_all(r, 'leaderboard')
# {b'player1': 1000.0, b'player2': 950.0, ...}
```

## Migration KEYS ‚Üí SCAN

### Code legacy avec KEYS

```python
# ‚ùå Code legacy dangereux
def delete_cache_old(redis_client):
    keys = redis_client.keys('cache:*')  # BLOQUE !
    if keys:
        redis_client.delete(*keys)
    return len(keys)
```

### Migration progressive

**√âtape 1** : Wrapper avec switch

```python
USE_SCAN = os.getenv('REDIS_USE_SCAN', 'false').lower() == 'true'

def get_keys_by_pattern(redis_client, pattern):
    if USE_SCAN:
        # Nouveau comportement
        cursor = 0
        keys = []
        while True:
            cursor, batch = redis_client.scan(cursor, match=pattern, count=100)
            keys.extend(batch)
            if cursor == 0:
                break
        return keys
    else:
        # Legacy
        return redis_client.keys(pattern)
```

**√âtape 2** : D√©ployer avec flag d√©sactiv√©

```bash
# Production
REDIS_USE_SCAN=false

# Monitor, pas d'erreur
```

**√âtape 3** : Activer progressivement

```bash
# Activer 10% trafic
if hash(user_id) % 10 == 0:
    USE_SCAN = True
else:
    USE_SCAN = False
```

**√âtape 4** : Full migration

```bash
# Production
REDIS_USE_SCAN=true

# Supprimer ancien code apr√®s stabilisation
```

### Pattern de migration complet

```python
class RedisKeyScanner:
    """Abstraction KEYS/SCAN avec migration progressive"""

    def __init__(self, redis_client, force_scan=None):
        self.redis = redis_client

        if force_scan is None:
            # Lecture config
            force_scan = os.getenv('REDIS_FORCE_SCAN', 'true').lower() == 'true'

        self.use_scan = force_scan

    def get_keys(self, pattern, count=100):
        """R√©cup√®re cl√©s par pattern"""
        if self.use_scan:
            return self._scan_keys(pattern, count)
        else:
            return self._keys_legacy(pattern)

    def _scan_keys(self, pattern, count):
        """Impl√©mentation SCAN"""
        cursor = 0
        keys = []

        while True:
            cursor, batch = self.redis.scan(cursor, match=pattern, count=count)
            keys.extend(batch)
            if cursor == 0:
                break

        return keys

    def _keys_legacy(self, pattern):
        """Impl√©mentation legacy KEYS"""
        import warnings
        warnings.warn(
            f"Using deprecated KEYS command for pattern: {pattern}",
            DeprecationWarning
        )
        return self.redis.keys(pattern)

    def delete_by_pattern(self, pattern, batch_size=1000):
        """Supprime cl√©s par pattern (safe)"""
        total_deleted = 0
        cursor = 0

        while True:
            cursor, keys = self.redis.scan(cursor, match=pattern, count=batch_size)

            if keys:
                # Delete batch
                self.redis.delete(*keys)
                total_deleted += len(keys)

            if cursor == 0:
                break

        return total_deleted

# Utilisation
scanner = RedisKeyScanner(r)
cache_keys = scanner.get_keys('cache:*')
deleted = scanner.delete_by_pattern('temp:*')
```

## Performance et benchmarking

### Benchmark KEYS vs SCAN

```python
import time
import redis

def benchmark_keys_vs_scan():
    r = redis.Redis()

    # Setup: Diff√©rentes tailles
    sizes = [1000, 10000, 100000, 1000000]

    for size in sizes:
        # Populate
        r.flushdb()
        for i in range(size):
            r.set(f'key:{i}', f'value{i}')

        # Benchmark KEYS
        start = time.time()
        keys_result = r.keys('key:*')
        keys_time = time.time() - start

        # Benchmark SCAN
        start = time.time()
        cursor = 0
        scan_result = []
        while True:
            cursor, batch = r.scan(cursor, match='key:*', count=100)
            scan_result.extend(batch)
            if cursor == 0:
                break
        scan_time = time.time() - start

        print(f"\nSize: {size:,} keys")
        print(f"KEYS:  {keys_time:.4f}s")
        print(f"SCAN:  {scan_time:.4f}s")
        print(f"Ratio: {scan_time/keys_time:.2f}x")
        print(f"Results match: {len(keys_result) == len(scan_result)}")

benchmark_keys_vs_scan()
```

**R√©sultats typiques** :

```
Size: 1,000 keys
KEYS:  0.0002s
SCAN:  0.0015s  (7.5x plus lent)
Results match: True

Size: 10,000 keys
KEYS:  0.0018s
SCAN:  0.0142s  (7.9x plus lent)
Results match: True

Size: 100,000 keys
KEYS:  0.0167s  ‚Üê Commence √† √™tre notable
SCAN:  0.1301s  (7.8x plus lent)
Results match: True

Size: 1,000,000 keys
KEYS:  0.1823s  ‚Üê BLOQUE 180ms !
SCAN:  1.4156s  (7.8x plus lent)
Results match: True
```

**Analyse** :

- SCAN est ~8x plus lent en temps total
- Mais SCAN ne bloque jamais Redis
- KEYS bloque pendant TOUT le temps
- SCAN permet d'intercaler autres requ√™tes

### Impact sur throughput

**Test de charge** :

```python
import threading
import time

def load_generator(redis_client, duration=10):
    """G√©n√®re charge constante"""
    start = time.time()
    ops = 0

    while time.time() - start < duration:
        redis_client.get('test')
        ops += 1

    return ops

def test_keys_impact():
    r = redis.Redis()

    # Populate 1M keys
    for i in range(1000000):
        r.set(f'key:{i}', f'value{i}')

    # Test baseline (sans KEYS)
    print("Baseline (no KEYS):")
    ops = load_generator(r, duration=10)
    print(f"  {ops:,} ops in 10s = {ops/10:,.0f} ops/sec")

    # Test avec KEYS concurrents
    print("\nWith KEYS every 100ms:")

    def keys_runner():
        for _ in range(100):  # 100x KEYS pendant 10s
            r.keys('key:*')
            time.sleep(0.1)

    keys_thread = threading.Thread(target=keys_runner)
    keys_thread.start()

    ops = load_generator(r, duration=10)
    keys_thread.join()

    print(f"  {ops:,} ops in 10s = {ops/10:,.0f} ops/sec")
    print(f"  Impact: {((ops_baseline - ops) / ops_baseline * 100):.1f}% drop")

test_keys_impact()
```

**R√©sultats attendus** :

```
Baseline (no KEYS):
  5,000,000 ops in 10s = 500,000 ops/sec

With KEYS every 100ms:
  2,000,000 ops in 10s = 200,000 ops/sec
  Impact: 60% drop
```

### Optimisation de COUNT

**Test de diff√©rentes valeurs COUNT** :

```python
def benchmark_count_values():
    r = redis.Redis()

    # 100k cl√©s
    r.flushdb()
    for i in range(100000):
        r.set(f'key:{i}', f'value{i}')

    counts = [10, 50, 100, 500, 1000, 5000]

    for count in counts:
        start = time.time()
        cursor = 0
        total_keys = 0
        iterations = 0

        while True:
            cursor, batch = r.scan(cursor, count=count)
            total_keys += len(batch)
            iterations += 1
            if cursor == 0:
                break

        elapsed = time.time() - start

        print(f"COUNT={count:4d}: {elapsed:.4f}s, "
              f"{iterations:5d} iterations, "
              f"{total_keys:6d} keys")

benchmark_count_values()
```

**R√©sultats typiques** :

```
COUNT=  10: 2.1456s, 10847 iterations, 100000 keys
COUNT=  50: 0.5234s,  2156 iterations, 100000 keys
COUNT= 100: 0.2891s,  1087 iterations, 100000 keys
COUNT= 500: 0.1234s,   217 iterations, 100000 keys
COUNT=1000: 0.0892s,   109 iterations, 100000 keys
COUNT=5000: 0.0567s,    22 iterations, 100000 keys
```

**Analyse** :

```
COUNT plus √©lev√©:
‚úÖ Moins d'it√©rations
‚úÖ Moins de round-trips r√©seau
‚úÖ Plus rapide au total

‚ùå Plus de travail par it√©ration
‚ùå Latence l√©g√®rement plus √©lev√©e par call
‚ùå Plus de m√©moire par batch

Recommandation production: COUNT=100-1000
```

## Cas d'usage en production

### 1. Nettoyage de cache expir√©

```python
def cleanup_expired_cache(redis_client, namespace='cache:',
                          batch_size=1000, max_time=60):
    """Nettoie cache en respectant timeout"""
    pattern = f"{namespace}*"
    start_time = time.time()
    total_deleted = 0
    cursor = 0

    while True:
        # Check timeout
        if time.time() - start_time > max_time:
            print(f"Timeout reached, deleted {total_deleted} keys")
            break

        # Scan batch
        cursor, keys = redis_client.scan(cursor, match=pattern, count=batch_size)

        if keys:
            # V√©rifie TTL et supprime si expir√© ou pas de TTL
            to_delete = []
            for key in keys:
                ttl = redis_client.ttl(key)
                if ttl == -1:  # Pas de TTL
                    to_delete.append(key)

            if to_delete:
                redis_client.delete(*to_delete)
                total_deleted += len(to_delete)

        if cursor == 0:
            break

    return total_deleted
```

### 2. Migration de donn√©es

```python
def migrate_keys_to_new_format(redis_client, old_pattern, transformer):
    """Migre cl√©s vers nouveau format"""
    cursor = 0
    migrated = 0
    errors = []

    while True:
        cursor, keys = redis_client.scan(cursor, match=old_pattern, count=100)

        for old_key in keys:
            try:
                # R√©cup√®re ancienne valeur
                value = redis_client.get(old_key)
                if value is None:
                    continue

                # Transforme cl√© et valeur
                new_key, new_value = transformer(old_key, value)

                # √âcrit nouveau format
                redis_client.set(new_key, new_value)

                # Optionnel: Supprime ancien
                # redis_client.delete(old_key)

                migrated += 1

            except Exception as e:
                errors.append((old_key, str(e)))

        if cursor == 0:
            break

    return migrated, errors

# Utilisation
def transform_user_key(old_key, old_value):
    # user:123 ‚Üí user:v2:123
    user_id = old_key.split(':')[1]
    new_key = f"user:v2:{user_id}"

    # Parse et transforme valeur
    data = json.loads(old_value)
    data['version'] = 2
    new_value = json.dumps(data)

    return new_key, new_value

migrated, errors = migrate_keys_to_new_format(r, "user:*", transform_user_key)
print(f"Migrated: {migrated}, Errors: {len(errors)}")
```

### 3. Monitoring et statistiques

```python
def analyze_keyspace(redis_client):
    """Analyse le keyspace"""
    stats = {
        'total_keys': 0,
        'by_type': {},
        'by_namespace': {},
        'with_ttl': 0,
        'without_ttl': 0,
        'total_memory': 0
    }

    cursor = 0
    sample_size = 10000  # √âchantillon pour performance
    sampled = 0

    while sampled < sample_size:
        cursor, keys = redis_client.scan(cursor, count=100)

        for key in keys:
            # Type
            key_type = redis_client.type(key)
            stats['by_type'][key_type] = stats['by_type'].get(key_type, 0) + 1

            # Namespace (premier segment)
            namespace = key.split(b':')[0] if b':' in key else b'root'
            stats['by_namespace'][namespace] = stats['by_namespace'].get(namespace, 0) + 1

            # TTL
            ttl = redis_client.ttl(key)
            if ttl > 0:
                stats['with_ttl'] += 1
            else:
                stats['without_ttl'] += 1

            # Memory (√©chantillon)
            try:
                mem = redis_client.memory_usage(key)
                if mem:
                    stats['total_memory'] += mem
            except:
                pass

            sampled += 1
            if sampled >= sample_size:
                break

        if cursor == 0:
            break

    stats['total_keys'] = sampled
    return stats

# Utilisation
stats = analyze_keyspace(r)
print(f"Total keys sampled: {stats['total_keys']:,}")
print(f"By type: {stats['by_type']}")
print(f"By namespace: {stats['by_namespace']}")
print(f"With TTL: {stats['with_ttl']}, Without: {stats['without_ttl']}")
print(f"Avg memory per key: {stats['total_memory'] / stats['total_keys']:.0f} bytes")
```

### 4. Backup s√©lectif

```python
def backup_keys_to_file(redis_client, pattern, output_file, format='json'):
    """Backup cl√©s vers fichier"""
    import json

    with open(output_file, 'w') as f:
        cursor = 0
        total = 0

        while True:
            cursor, keys = redis_client.scan(cursor, match=pattern, count=100)

            for key in keys:
                # R√©cup√®re type et valeur
                key_type = redis_client.type(key)
                ttl = redis_client.ttl(key)

                if key_type == b'string':
                    value = redis_client.get(key)
                elif key_type == b'hash':
                    value = redis_client.hgetall(key)
                elif key_type == b'list':
                    value = redis_client.lrange(key, 0, -1)
                elif key_type == b'set':
                    value = list(redis_client.smembers(key))
                elif key_type == b'zset':
                    value = redis_client.zrange(key, 0, -1, withscores=True)
                else:
                    continue

                # S√©rialise
                entry = {
                    'key': key.decode(),
                    'type': key_type.decode(),
                    'ttl': ttl,
                    'value': value
                }

                json.dump(entry, f)
                f.write('\n')
                total += 1

            if cursor == 0:
                break

        print(f"Backed up {total} keys to {output_file}")

# Utilisation
backup_keys_to_file(r, 'user:*', 'users_backup.jsonl')
```

## Bonnes pratiques

### DO ‚úÖ

```python
# ‚úÖ Utiliser SCAN en production
cursor = 0
while True:
    cursor, keys = redis.scan(cursor, match='user:*', count=100)
    process(keys)
    if cursor == 0:
        break

# ‚úÖ Pattern generator pour √©conomie m√©moire
for key in scan_keys_generator(r, 'cache:*'):
    process(key)

# ‚úÖ COUNT adapt√© √† la charge
# Charge √©lev√©e: COUNT=50
# Charge normale: COUNT=100
# Batch job: COUNT=1000

# ‚úÖ Timeout pour √©viter boucles infinies
scan_with_timeout(r, 'temp:*', timeout_seconds=30)

# ‚úÖ MATCH c√¥t√© serveur
redis.scan(cursor, match='user:*')  # Filtrage serveur

# ‚úÖ TYPE filter si Redis 6.0+
redis.scan(cursor, match='cache:*', _type='string')

# ‚úÖ Suppression par batch
def delete_by_pattern(redis, pattern, batch_size=1000):
    cursor = 0
    while True:
        cursor, keys = redis.scan(cursor, match=pattern, count=batch_size)
        if keys:
            redis.delete(*keys)
        if cursor == 0:
            break
```

### DON'T ‚ùå

```python
# ‚ùå KEYS en production
keys = redis.keys('user:*')  # BLOQUE !

# ‚ùå Collecte tous r√©sultats en m√©moire si √©norme
all_keys = []
cursor = 0
while True:
    cursor, batch = redis.scan(cursor)
    all_keys.extend(batch)  # 10M cl√©s = OOM
    if cursor == 0:
        break

# ‚ùå Filtrage complexe sans MATCH
cursor, keys = redis.scan(cursor)  # R√©cup√®re tout
filtered = [k for k in keys if complex_filter(k)]  # Filtre client

# ‚ùå COUNT trop petit
redis.scan(cursor, count=1)  # Trop de round-trips

# ‚ùå COUNT trop grand sur syst√®me charg√©
redis.scan(cursor, count=100000)  # Latence spike

# ‚ùå Pas de protection timeout
cursor = 0
while True:  # Peut boucler infiniment si bug
    cursor, keys = redis.scan(cursor)
    if cursor == 0:
        break

# ‚ùå Ignorer cursor != 0
cursor, keys = redis.scan(0)
# Process keys
# FIN - Incomplet si cursor != 0 !
```

## Monitoring SCAN

### M√©triques √† surveiller

```python
import time

class ScanMonitor:
    """Monitore performance SCAN"""

    def __init__(self):
        self.metrics = {
            'total_scans': 0,
            'total_iterations': 0,
            'total_keys': 0,
            'total_time': 0,
            'by_pattern': {}
        }

    def monitored_scan(self, redis_client, pattern, count=100):
        """SCAN avec monitoring"""
        start = time.time()
        cursor = 0
        keys = []
        iterations = 0

        while True:
            iter_start = time.time()
            cursor, batch = redis_client.scan(cursor, match=pattern, count=count)
            iter_time = time.time() - iter_start

            keys.extend(batch)
            iterations += 1

            # Log si it√©ration lente
            if iter_time > 0.1:
                print(f"Slow SCAN iteration: {iter_time:.3f}s")

            if cursor == 0:
                break

        elapsed = time.time() - start

        # Update metrics
        self.metrics['total_scans'] += 1
        self.metrics['total_iterations'] += iterations
        self.metrics['total_keys'] += len(keys)
        self.metrics['total_time'] += elapsed

        if pattern not in self.metrics['by_pattern']:
            self.metrics['by_pattern'][pattern] = {
                'count': 0,
                'avg_time': 0,
                'avg_keys': 0
            }

        pattern_stats = self.metrics['by_pattern'][pattern]
        pattern_stats['count'] += 1
        pattern_stats['avg_time'] = (
            (pattern_stats['avg_time'] * (pattern_stats['count'] - 1) + elapsed)
            / pattern_stats['count']
        )
        pattern_stats['avg_keys'] = (
            (pattern_stats['avg_keys'] * (pattern_stats['count'] - 1) + len(keys))
            / pattern_stats['count']
        )

        return keys

    def report(self):
        """G√©n√®re rapport"""
        print("SCAN Metrics:")
        print(f"  Total scans: {self.metrics['total_scans']}")
        print(f"  Total iterations: {self.metrics['total_iterations']}")
        print(f"  Total keys: {self.metrics['total_keys']}")
        print(f"  Total time: {self.metrics['total_time']:.2f}s")
        print(f"  Avg time per scan: {self.metrics['total_time'] / max(1, self.metrics['total_scans']):.3f}s")
        print("\nBy pattern:")
        for pattern, stats in self.metrics['by_pattern'].items():
            print(f"  {pattern}:")
            print(f"    Count: {stats['count']}")
            print(f"    Avg time: {stats['avg_time']:.3f}s")
            print(f"    Avg keys: {stats['avg_keys']:.0f}")

# Utilisation
monitor = ScanMonitor()
keys1 = monitor.monitored_scan(r, 'user:*')
keys2 = monitor.monitored_scan(r, 'cache:*')
monitor.report()
```

## Conclusion

### R√®gles absolues

1. **JAMAIS KEYS en production** : Sans exception
2. **TOUJOURS SCAN** : Pour toute it√©ration
3. **COUNT entre 100-1000** : Balance performance/latence
4. **MATCH c√¥t√© serveur** : R√©duit transfert r√©seau
5. **Generator pattern** : √âconomise m√©moire
6. **Timeout protection** : √âvite boucles infinies
7. **Monitoring** : Mesure performance SCAN

### R√©capitulatif SCAN

```bash
# Template production-ready
cursor = 0
total_processed = 0

while True:
    # SCAN avec MATCH et COUNT adapt√©
    cursor, keys = redis.scan(
        cursor,
        match='namespace:*',
        count=100  # Ajuster selon charge
    )

    # Traite batch
    if keys:
        process_batch(keys)
        total_processed += len(keys)

    # Fin d'it√©ration
    if cursor == 0:
        break

print(f"Processed {total_processed} keys")
```

### Migration checklist

```
‚òê Identifier tous usages de KEYS
‚òê Remplacer par SCAN avec generator
‚òê Ajouter monitoring
‚òê Tester en staging
‚òê D√©ployer avec feature flag
‚òê Activer progressivement
‚òê Supprimer ancien code
‚òê Documenter changement
```

SCAN n'est pas qu'une alternative √† KEYS : c'est la **seule fa√ßon s√ªre** d'it√©rer sur le keyspace Redis en production. Le co√ªt de d√©veloppement suppl√©mentaire est n√©gligeable compar√© au risque d'un incident production caus√© par KEYS.

Cette section conclut le module sur le cycle de vie de la donn√©e. Vous avez maintenant toutes les cl√©s pour g√©rer efficacement les donn√©es dans Redis, de leur cr√©ation √† leur suppression, en passant par l'expiration, l'√©viction et l'exploration du keyspace.

‚è≠Ô∏è [Persistance et fiabilit√© des donn√©es](/05-persistance-fiabilite/README.md)
