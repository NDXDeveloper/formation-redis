ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 6.1 Caching Patterns : Cache-Aside, Write-Through, Write-Back

## Introduction

Le caching est la raison d'Ãªtre principale de Redis dans la plupart des architectures modernes. Un cache bien conÃ§u peut rÃ©duire la latence de 100ms Ã  moins de 1ms et diviser la charge sur la base de donnÃ©es primaire par 10, voire 100. Cependant, tous les patterns de caching ne se valent pas : le choix du bon pattern dÃ©pend de vos besoins en cohÃ©rence, performance et complexitÃ©.

Cette section explore les trois patterns de caching fondamentaux et leurs variantes, avec leurs trade-offs respectifs.

## Vue d'ensemble des patterns

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CACHING PATTERNS COMPARISON                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Pattern          â”‚ Reads      â”‚ Writes     â”‚ Consistency       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚  Cache-Aside     â”‚ âš¡âš¡âš¡     â”‚ ğŸ’¾         â”‚ ğŸŸ¡ Eventually
â”‚  (Lazy Loading)  â”‚ Fast       â”‚ Direct DB  â”‚    Consistent      â”‚
â”‚                  â”‚            â”‚            â”‚                    â”‚
â”‚  Write-Through   â”‚ âš¡âš¡       â”‚ ğŸ’¾ğŸ’¾      â”‚ ğŸŸ¢ Strong
â”‚  (Write Cache)   â”‚ Fast       â”‚ Slower     â”‚    Consistent      â”‚
â”‚                  â”‚            â”‚            â”‚                    â”‚
â”‚  Write-Back      â”‚ âš¡âš¡âš¡     â”‚ âš¡âš¡âš¡     â”‚ ğŸ”´ Weak
â”‚  (Write Behind)  â”‚ Fast       â”‚ Very Fast  â”‚    Risk of Loss    â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Choix rapide selon votre cas d'usage

| Cas d'usage | Pattern recommandÃ© | Raison |
|------------|-------------------|---------|
| Read-heavy, eventual consistency OK | Cache-Aside | Simple, performant, rÃ©silient |
| Write-heavy, strong consistency | Write-Through | Garantit la cohÃ©rence |
| Write-heavy, latency critique | Write-Back | Maximum de performance |
| API REST standard | Cache-Aside | Pattern le plus courant |
| E-commerce checkout | Write-Through | Pas de perte de donnÃ©es |
| Analytics real-time | Write-Back | DÃ©bit d'Ã©criture Ã©levÃ© |

---

## Pattern 1 : Cache-Aside (Lazy Loading)

### Concept

**Cache-Aside** est le pattern le plus rÃ©pandu. L'application gÃ¨re explicitement le cache : elle vÃ©rifie d'abord Redis, et si la donnÃ©e n'existe pas (cache miss), elle la charge depuis la base de donnÃ©es et la met en cache.

### Flux de lecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ APPLICATION  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ 1. GET key
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    REDIS     â”‚  â† Cache
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â†’ Cache HIT â”€â”€â”€â”€â”€â†’ Return data âœ“
       â”‚
       â””â”€â†’ Cache MISS
              â”‚
              â”‚ 2. SELECT * FROM ...
              â†“
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚  DATABASE    â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â”‚ 3. Return data
              â†“
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚    REDIS     â”‚
       â”‚ SET key data â”‚  â† 4. Populate cache
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â””â”€â†’ Return data âœ“
```

### Flux d'Ã©criture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ APPLICATION  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ 1. UPDATE/INSERT
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DATABASE    â”‚  â† Primary store
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ 2. Success
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    REDIS     â”‚
â”‚   DEL key    â”‚  â† 3. Invalidate cache (optional)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â””â”€â†’ Return success âœ“
```

### ImplÃ©mentation Python

```python
import redis
import psycopg2
import json
from typing import Optional, Dict, Any
from datetime import timedelta

class CacheAsidePattern:
    def __init__(self, redis_client: redis.Redis, db_connection):
        self.cache = redis_client
        self.db = db_connection
        self.default_ttl = 3600  # 1 heure

    def get_user(self, user_id: int) -> Optional[Dict[str, Any]]:
        """
        RÃ©cupÃ¨re un utilisateur avec Cache-Aside pattern
        """
        cache_key = f"user:{user_id}"

        # 1. Essayer de lire depuis le cache
        cached_data = self.cache.get(cache_key)

        if cached_data:
            # Cache HIT
            print(f"âœ“ Cache HIT for user {user_id}")
            return json.loads(cached_data)

        # 2. Cache MISS - Charger depuis la DB
        print(f"âœ— Cache MISS for user {user_id}")
        user = self._load_user_from_db(user_id)

        if user:
            # 3. Peupler le cache avec TTL
            self.cache.setex(
                cache_key,
                self.default_ttl,
                json.dumps(user)
            )
            print(f"â†’ Cached user {user_id} for {self.default_ttl}s")

        return user

    def update_user(self, user_id: int, user_data: Dict[str, Any]) -> bool:
        """
        Met Ã  jour un utilisateur et invalide le cache
        """
        cache_key = f"user:{user_id}"

        # 1. Ã‰crire dans la base de donnÃ©es
        success = self._update_user_in_db(user_id, user_data)

        if success:
            # 2. Invalider le cache (stratÃ©gie d'invalidation)
            self.cache.delete(cache_key)
            print(f"â†’ Invalidated cache for user {user_id}")

            # Alternative : Update immÃ©diat du cache (Write-Through hybrid)
            # self.cache.setex(cache_key, self.default_ttl, json.dumps(user_data))

        return success

    def _load_user_from_db(self, user_id: int) -> Optional[Dict[str, Any]]:
        """Charge depuis PostgreSQL"""
        cursor = self.db.cursor()
        cursor.execute(
            "SELECT id, name, email, created_at FROM users WHERE id = %s",
            (user_id,)
        )
        row = cursor.fetchone()

        if row:
            return {
                'id': row[0],
                'name': row[1],
                'email': row[2],
                'created_at': str(row[3])
            }
        return None

    def _update_user_in_db(self, user_id: int, data: Dict[str, Any]) -> bool:
        """Met Ã  jour dans PostgreSQL"""
        cursor = self.db.cursor()
        try:
            cursor.execute(
                "UPDATE users SET name = %s, email = %s WHERE id = %s",
                (data.get('name'), data.get('email'), user_id)
            )
            self.db.commit()
            return True
        except Exception as e:
            self.db.rollback()
            print(f"Error updating user: {e}")
            return False


# ===== VARIANTE : Cache-Aside avec Refresh automatique =====

class CacheAsideWithRefresh(CacheAsidePattern):
    """
    Variante qui rafraÃ®chit automatiquement le cache sur lecture
    (Sliding Window Expiration)
    """

    def get_user(self, user_id: int) -> Optional[Dict[str, Any]]:
        cache_key = f"user:{user_id}"

        # Lire depuis le cache
        cached_data = self.cache.get(cache_key)

        if cached_data:
            # Cache HIT - RafraÃ®chir le TTL (Sliding Window)
            self.cache.expire(cache_key, self.default_ttl)
            print(f"âœ“ Cache HIT for user {user_id} (TTL refreshed)")
            return json.loads(cached_data)

        # Cache MISS - Comportement standard
        return super().get_user(user_id)


# ===== VARIANTE : Cache-Aside avec Probabilistic Early Expiration =====

import random
import time

class CacheAsideWithProbabilisticExpiration(CacheAsidePattern):
    """
    Ã‰vite les Cache Stampedes en rechargeant le cache avant expiration
    avec une probabilitÃ© croissante
    """

    def get_user(self, user_id: int) -> Optional[Dict[str, Any]]:
        cache_key = f"user:{user_id}"

        cached_data = self.cache.get(cache_key)

        if cached_data:
            # VÃ©rifier le TTL restant
            ttl = self.cache.ttl(cache_key)

            if ttl > 0:
                # Calculer la probabilitÃ© de refresh anticipÃ©
                # Plus le TTL est proche de 0, plus la probabilitÃ© est Ã©levÃ©e
                expiry_threshold = self.default_ttl * 0.2  # 20% du TTL

                if ttl < expiry_threshold:
                    # ProbabilitÃ© = (1 - ttl/threshold)
                    probability = 1 - (ttl / expiry_threshold)

                    if random.random() < probability:
                        print(f"ğŸ”„ Probabilistic refresh for user {user_id} (TTL: {ttl}s)")
                        # Refresh en arriÃ¨re-plan (async dans un vrai systÃ¨me)
                        self._refresh_cache_async(user_id)

            return json.loads(cached_data)

        return super().get_user(user_id)

    def _refresh_cache_async(self, user_id: int):
        """RafraÃ®chit le cache en arriÃ¨re-plan"""
        # Dans un vrai systÃ¨me, utiliser une queue ou un thread pool
        user = self._load_user_from_db(user_id)
        if user:
            cache_key = f"user:{user_id}"
            self.cache.setex(cache_key, self.default_ttl, json.dumps(user))


# ===== Exemple d'utilisation =====

if __name__ == '__main__':
    # Setup
    r = redis.Redis(host='localhost', port=6379, decode_responses=True)
    db = psycopg2.connect("dbname=myapp user=postgres")

    cache = CacheAsidePattern(r, db)

    # Premier appel : Cache MISS
    user = cache.get_user(123)
    print(f"User: {user}")

    # DeuxiÃ¨me appel : Cache HIT
    user = cache.get_user(123)
    print(f"User: {user}")

    # Mise Ã  jour : Invalide le cache
    cache.update_user(123, {'name': 'John Doe', 'email': 'john@example.com'})

    # TroisiÃ¨me appel : Cache MISS Ã  nouveau
    user = cache.get_user(123)
    print(f"User: {user}")
```

### ImplÃ©mentation Node.js

```javascript
const Redis = require('ioredis');
const { Pool } = require('pg');

class CacheAsidePattern {
    constructor(redisClient, dbPool) {
        this.cache = redisClient;
        this.db = dbPool;
        this.defaultTTL = 3600; // 1 heure
    }

    async getUser(userId) {
        const cacheKey = `user:${userId}`;

        // 1. Essayer de lire depuis le cache
        const cachedData = await this.cache.get(cacheKey);

        if (cachedData) {
            // Cache HIT
            console.log(`âœ“ Cache HIT for user ${userId}`);
            return JSON.parse(cachedData);
        }

        // 2. Cache MISS - Charger depuis la DB
        console.log(`âœ— Cache MISS for user ${userId}`);
        const user = await this._loadUserFromDB(userId);

        if (user) {
            // 3. Peupler le cache avec TTL
            await this.cache.setex(
                cacheKey,
                this.defaultTTL,
                JSON.stringify(user)
            );
            console.log(`â†’ Cached user ${userId} for ${this.defaultTTL}s`);
        }

        return user;
    }

    async updateUser(userId, userData) {
        const cacheKey = `user:${userId}`;

        // 1. Ã‰crire dans la base de donnÃ©es
        const success = await this._updateUserInDB(userId, userData);

        if (success) {
            // 2. Invalider le cache
            await this.cache.del(cacheKey);
            console.log(`â†’ Invalidated cache for user ${userId}`);
        }

        return success;
    }

    async _loadUserFromDB(userId) {
        const result = await this.db.query(
            'SELECT id, name, email, created_at FROM users WHERE id = $1',
            [userId]
        );

        if (result.rows.length > 0) {
            const row = result.rows[0];
            return {
                id: row.id,
                name: row.name,
                email: row.email,
                created_at: row.created_at.toISOString()
            };
        }

        return null;
    }

    async _updateUserInDB(userId, data) {
        try {
            await this.db.query(
                'UPDATE users SET name = $1, email = $2 WHERE id = $3',
                [data.name, data.email, userId]
            );
            return true;
        } catch (error) {
            console.error('Error updating user:', error);
            return false;
        }
    }
}

// ===== VARIANTE : Cache-Aside avec batching =====

class CacheAsideBatched extends CacheAsidePattern {
    async getUsers(userIds) {
        const cacheKeys = userIds.map(id => `user:${id}`);

        // 1. Batch read depuis Redis avec MGET
        const cachedData = await this.cache.mget(...cacheKeys);

        const results = [];
        const missedIds = [];

        // 2. Identifier les hits et les misses
        userIds.forEach((userId, index) => {
            if (cachedData[index]) {
                // Cache HIT
                results[index] = JSON.parse(cachedData[index]);
                console.log(`âœ“ Cache HIT for user ${userId}`);
            } else {
                // Cache MISS
                missedIds.push(userId);
                console.log(`âœ— Cache MISS for user ${userId}`);
            }
        });

        // 3. Charger les misses depuis la DB (batch query)
        if (missedIds.length > 0) {
            const users = await this._loadUsersFromDB(missedIds);

            // 4. Peupler le cache avec pipeline
            const pipeline = this.cache.pipeline();

            users.forEach(user => {
                const cacheKey = `user:${user.id}`;
                pipeline.setex(
                    cacheKey,
                    this.defaultTTL,
                    JSON.stringify(user)
                );

                // Ajouter au rÃ©sultat
                const originalIndex = userIds.indexOf(user.id);
                results[originalIndex] = user;
            });

            await pipeline.exec();
            console.log(`â†’ Cached ${users.length} users`);
        }

        return results;
    }

    async _loadUsersFromDB(userIds) {
        const result = await this.db.query(
            'SELECT id, name, email, created_at FROM users WHERE id = ANY($1)',
            [userIds]
        );

        return result.rows.map(row => ({
            id: row.id,
            name: row.name,
            email: row.email,
            created_at: row.created_at.toISOString()
        }));
    }
}

// ===== Exemple d'utilisation =====

(async () => {
    const redis = new Redis();
    const db = new Pool({
        host: 'localhost',
        database: 'myapp',
        user: 'postgres'
    });

    const cache = new CacheAsidePattern(redis, db);

    // Premier appel : Cache MISS
    let user = await cache.getUser(123);
    console.log('User:', user);

    // DeuxiÃ¨me appel : Cache HIT
    user = await cache.getUser(123);
    console.log('User:', user);

    // Test du batching
    const batchedCache = new CacheAsideBatched(redis, db);
    const users = await batchedCache.getUsers([123, 456, 789]);
    console.log('Users:', users);
})();
```

### Avantages et inconvÃ©nients

**âœ… Avantages**
- **SimplicitÃ©** : Pattern le plus simple Ã  comprendre et implÃ©menter
- **RÃ©silience** : Si Redis tombe, l'application continue de fonctionner (en lisant la DB)
- **Lazy loading** : Ne cache que ce qui est rÃ©ellement lu
- **ContrÃ´le fin** : L'application dÃ©cide quoi et quand cacher

**âŒ InconvÃ©nients**
- **Cache Stampede** : Si une clÃ© populaire expire, plusieurs requÃªtes peuvent charger simultanÃ©ment
- **CohÃ©rence** : FenÃªtre oÃ¹ le cache peut Ãªtre stale aprÃ¨s une mise Ã  jour
- **Initial latency** : Premier accÃ¨s toujours lent (cache miss)
- **Code duplication** : Logique de cache dispersÃ©e dans le code applicatif

### Bonnes pratiques

```python
# âœ… BON : TTL appropriÃ© selon la volatilitÃ© des donnÃ©es
TTL_CONFIG = {
    'user_profile': 3600,      # 1h - Peu volatile
    'product_price': 300,       # 5min - Volatile
    'user_cart': 1800,          # 30min - Session-based
    'static_content': 86400     # 24h - TrÃ¨s stable
}

# âœ… BON : Gestion des cache misses multiples (Stampede protection)
import threading

class CacheWithStampedeProtection:
    def __init__(self, redis_client, db):
        self.cache = redis_client
        self.db = db
        self.locks = {}  # En mÃ©moire, ou utiliser Redis SET NX

    def get_with_lock(self, key, load_fn, ttl=3600):
        # Essayer le cache
        cached = self.cache.get(key)
        if cached:
            return json.loads(cached)

        # AcquÃ©rir un lock local pour Ã©viter les requÃªtes multiples
        lock = self.locks.setdefault(key, threading.Lock())

        with lock:
            # Double-check aprÃ¨s acquisition du lock
            cached = self.cache.get(key)
            if cached:
                return json.loads(cached)

            # Charger la donnÃ©e
            data = load_fn()
            if data:
                self.cache.setex(key, ttl, json.dumps(data))

            return data

# âœ… BON : Namespace et versioning
def build_cache_key(entity, id, version='v1'):
    return f"{version}:{entity}:{id}"

# Si le schÃ©ma change, incrÃ©menter la version
key = build_cache_key('user', 123, 'v2')  # Invalide automatiquement v1

# âœ… BON : Metrics et observabilitÃ©
class CacheWithMetrics:
    def __init__(self, cache, db):
        self.cache = cache
        self.db = db
        self.hits = 0
        self.misses = 0

    def get(self, key, load_fn, ttl=3600):
        cached = self.cache.get(key)

        if cached:
            self.hits += 1
            return json.loads(cached)

        self.misses += 1
        data = load_fn()
        if data:
            self.cache.setex(key, ttl, json.dumps(data))
        return data

    def hit_rate(self):
        total = self.hits + self.misses
        return (self.hits / total * 100) if total > 0 else 0
```

---

## Pattern 2 : Write-Through

### Concept

Dans le pattern **Write-Through**, toute Ã©criture passe d'abord par le cache, qui se charge ensuite de persister dans la base de donnÃ©es. Le cache et la DB sont toujours synchronisÃ©s : aucune donnÃ©e ne peut exister dans la DB sans Ãªtre dans le cache.

### Flux de lecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ APPLICATION  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ 1. GET key
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    REDIS     â”‚  â† Always in sync with DB
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â†’ Cache HIT â”€â”€â”€â”€â”€â†’ Return data âœ“
       â”‚
       â””â”€â†’ Cache MISS
              â”‚
              â”‚ 2. SELECT * FROM ...
              â†“
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚  DATABASE    â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â”‚ 3. Return data
              â†“
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚    REDIS     â”‚
       â”‚ SET key data â”‚  â† 4. Populate cache
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flux d'Ã©criture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ APPLICATION  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ 1. UPDATE data
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    REDIS     â”‚  â† Write to cache first
â”‚ SET key data â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ 2. Write to DB
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DATABASE    â”‚  â† Then persist
â”‚    UPDATE    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â””â”€â†’ Return success âœ“

    âš ï¸ If DB write fails â†’ Rollback cache
```

### ImplÃ©mentation Python

```python
import redis
import psycopg2
import json
from typing import Optional, Dict, Any
from contextlib import contextmanager

class WriteThroughPattern:
    def __init__(self, redis_client: redis.Redis, db_connection):
        self.cache = redis_client
        self.db = db_connection
        self.default_ttl = 3600

    def get_user(self, user_id: int) -> Optional[Dict[str, Any]]:
        """
        Lecture similaire Ã  Cache-Aside
        """
        cache_key = f"user:{user_id}"

        # Essayer le cache
        cached_data = self.cache.get(cache_key)
        if cached_data:
            print(f"âœ“ Cache HIT for user {user_id}")
            return json.loads(cached_data)

        # Cache MISS - Charger depuis DB
        print(f"âœ— Cache MISS for user {user_id}")
        user = self._load_user_from_db(user_id)

        if user:
            self.cache.setex(cache_key, self.default_ttl, json.dumps(user))

        return user

    def update_user(self, user_id: int, user_data: Dict[str, Any]) -> bool:
        """
        Write-Through : Cache d'abord, puis DB
        """
        cache_key = f"user:{user_id}"

        # 1. Ã‰crire dans le cache d'abord
        try:
            self.cache.setex(
                cache_key,
                self.default_ttl,
                json.dumps(user_data)
            )
            print(f"â†’ Wrote to cache for user {user_id}")
        except Exception as e:
            print(f"âœ— Cache write failed: {e}")
            return False

        # 2. Puis persister dans la DB
        try:
            success = self._update_user_in_db(user_id, user_data)

            if not success:
                # ROLLBACK : Supprimer du cache si DB Ã©choue
                self.cache.delete(cache_key)
                print(f"âš ï¸  Rolled back cache for user {user_id}")
                return False

            print(f"âœ“ Wrote to DB for user {user_id}")
            return True

        except Exception as e:
            # ROLLBACK du cache
            self.cache.delete(cache_key)
            print(f"âœ— DB write failed, rolled back cache: {e}")
            return False

    def create_user(self, user_data: Dict[str, Any]) -> Optional[int]:
        """
        CrÃ©ation avec Write-Through
        """
        # 1. CrÃ©er dans la DB pour obtenir l'ID
        user_id = self._create_user_in_db(user_data)

        if user_id:
            # 2. Peupler immÃ©diatement le cache
            cache_key = f"user:{user_id}"
            user_data['id'] = user_id

            self.cache.setex(
                cache_key,
                self.default_ttl,
                json.dumps(user_data)
            )
            print(f"âœ“ Created user {user_id} in DB and cache")

        return user_id

    def _load_user_from_db(self, user_id: int) -> Optional[Dict[str, Any]]:
        cursor = self.db.cursor()
        cursor.execute(
            "SELECT id, name, email, created_at FROM users WHERE id = %s",
            (user_id,)
        )
        row = cursor.fetchone()

        if row:
            return {
                'id': row[0],
                'name': row[1],
                'email': row[2],
                'created_at': str(row[3])
            }
        return None

    def _update_user_in_db(self, user_id: int, data: Dict[str, Any]) -> bool:
        cursor = self.db.cursor()
        try:
            cursor.execute(
                "UPDATE users SET name = %s, email = %s WHERE id = %s",
                (data.get('name'), data.get('email'), user_id)
            )
            self.db.commit()
            return True
        except Exception as e:
            self.db.rollback()
            raise e

    def _create_user_in_db(self, data: Dict[str, Any]) -> Optional[int]:
        cursor = self.db.cursor()
        try:
            cursor.execute(
                "INSERT INTO users (name, email) VALUES (%s, %s) RETURNING id",
                (data.get('name'), data.get('email'))
            )
            user_id = cursor.fetchone()[0]
            self.db.commit()
            return user_id
        except Exception as e:
            self.db.rollback()
            print(f"Error creating user: {e}")
            return None


# ===== VARIANTE : Write-Through avec Transaction distribuÃ©e =====

class WriteThroughWithTransaction(WriteThroughPattern):
    """
    Utilise WATCH pour garantir la cohÃ©rence entre Redis et la DB
    """

    def update_user_atomic(self, user_id: int, user_data: Dict[str, Any]) -> bool:
        cache_key = f"user:{user_id}"

        # DÃ©marrer une transaction Redis avec WATCH
        with self.cache.pipeline() as pipe:
            try:
                # WATCH la clÃ© pour dÃ©tecter les modifications concurrentes
                pipe.watch(cache_key)

                # Lire la valeur actuelle
                current_value = pipe.get(cache_key)

                # Ã‰crire dans la DB
                success = self._update_user_in_db(user_id, user_data)

                if not success:
                    pipe.unwatch()
                    return False

                # Transaction MULTI/EXEC
                pipe.multi()
                pipe.setex(cache_key, self.default_ttl, json.dumps(user_data))
                pipe.execute()

                print(f"âœ“ Atomic write for user {user_id}")
                return True

            except redis.WatchError:
                # La clÃ© a Ã©tÃ© modifiÃ©e pendant la transaction
                print(f"âš ï¸  Concurrent modification detected for user {user_id}")
                return False


# ===== VARIANTE : Write-Through avec Queue pour la DB =====

import queue
import threading

class WriteThroughWithQueue(WriteThroughPattern):
    """
    Ã‰crit dans Redis immÃ©diatement, mais utilise une queue pour la DB
    (Hybride entre Write-Through et Write-Back)
    """

    def __init__(self, redis_client, db_connection):
        super().__init__(redis_client, db_connection)
        self.write_queue = queue.Queue()
        self.worker_thread = threading.Thread(target=self._process_writes)
        self.worker_thread.daemon = True
        self.worker_thread.start()

    def update_user(self, user_id: int, user_data: Dict[str, Any]) -> bool:
        cache_key = f"user:{user_id}"

        # 1. Ã‰criture immÃ©diate dans Redis
        try:
            self.cache.setex(cache_key, self.default_ttl, json.dumps(user_data))
            print(f"â†’ Wrote to cache for user {user_id}")
        except Exception as e:
            print(f"âœ— Cache write failed: {e}")
            return False

        # 2. Enqueuer pour Ã©criture DB asynchrone
        self.write_queue.put(('update', user_id, user_data))
        print(f"â†’ Enqueued DB write for user {user_id}")

        return True

    def _process_writes(self):
        """Worker thread qui traite les Ã©critures DB"""
        while True:
            try:
                operation, user_id, data = self.write_queue.get(timeout=1)

                if operation == 'update':
                    success = self._update_user_in_db(user_id, data)

                    if not success:
                        # En cas d'Ã©chec, on pourrait re-enqueuer ou alerter
                        print(f"âš ï¸  DB write failed for user {user_id}")
                    else:
                        print(f"âœ“ DB write completed for user {user_id}")

                self.write_queue.task_done()

            except queue.Empty:
                continue
            except Exception as e:
                print(f"Error in write worker: {e}")


# ===== Exemple d'utilisation =====

if __name__ == '__main__':
    r = redis.Redis(host='localhost', port=6379, decode_responses=True)
    db = psycopg2.connect("dbname=myapp user=postgres")

    cache = WriteThroughPattern(r, db)

    # CrÃ©er un utilisateur (Write-Through)
    user_id = cache.create_user({
        'name': 'Alice',
        'email': 'alice@example.com'
    })
    print(f"Created user: {user_id}")

    # Lecture : dÃ©jÃ  en cache
    user = cache.get_user(user_id)
    print(f"User: {user}")

    # Mise Ã  jour (Write-Through)
    success = cache.update_user(user_id, {
        'name': 'Alice Smith',
        'email': 'alice.smith@example.com'
    })
    print(f"Update success: {success}")

    # Lecture : cache Ã  jour
    user = cache.get_user(user_id)
    print(f"Updated user: {user}")
```

### ImplÃ©mentation Node.js

```javascript
const Redis = require('ioredis');
const { Pool } = require('pg');

class WriteThroughPattern {
    constructor(redisClient, dbPool) {
        this.cache = redisClient;
        this.db = dbPool;
        this.defaultTTL = 3600;
    }

    async getUser(userId) {
        const cacheKey = `user:${userId}`;

        // Essayer le cache
        const cachedData = await this.cache.get(cacheKey);
        if (cachedData) {
            console.log(`âœ“ Cache HIT for user ${userId}`);
            return JSON.parse(cachedData);
        }

        // Cache MISS
        console.log(`âœ— Cache MISS for user ${userId}`);
        const user = await this._loadUserFromDB(userId);

        if (user) {
            await this.cache.setex(
                cacheKey,
                this.defaultTTL,
                JSON.stringify(user)
            );
        }

        return user;
    }

    async updateUser(userId, userData) {
        const cacheKey = `user:${userId}`;

        // 1. Ã‰crire dans le cache d'abord
        try {
            await this.cache.setex(
                cacheKey,
                this.defaultTTL,
                JSON.stringify(userData)
            );
            console.log(`â†’ Wrote to cache for user ${userId}`);
        } catch (error) {
            console.error(`âœ— Cache write failed: ${error}`);
            return false;
        }

        // 2. Puis persister dans la DB
        try {
            const success = await this._updateUserInDB(userId, userData);

            if (!success) {
                // ROLLBACK : Supprimer du cache
                await this.cache.del(cacheKey);
                console.log(`âš ï¸  Rolled back cache for user ${userId}`);
                return false;
            }

            console.log(`âœ“ Wrote to DB for user ${userId}`);
            return true;

        } catch (error) {
            // ROLLBACK du cache
            await this.cache.del(cacheKey);
            console.error(`âœ— DB write failed, rolled back cache: ${error}`);
            return false;
        }
    }

    async createUser(userData) {
        // 1. CrÃ©er dans la DB pour obtenir l'ID
        const userId = await this._createUserInDB(userData);

        if (userId) {
            // 2. Peupler immÃ©diatement le cache
            const cacheKey = `user:${userId}`;
            userData.id = userId;

            await this.cache.setex(
                cacheKey,
                this.defaultTTL,
                JSON.stringify(userData)
            );
            console.log(`âœ“ Created user ${userId} in DB and cache`);
        }

        return userId;
    }

    async _loadUserFromDB(userId) {
        const result = await this.db.query(
            'SELECT id, name, email, created_at FROM users WHERE id = $1',
            [userId]
        );

        if (result.rows.length > 0) {
            const row = result.rows[0];
            return {
                id: row.id,
                name: row.name,
                email: row.email,
                created_at: row.created_at.toISOString()
            };
        }

        return null;
    }

    async _updateUserInDB(userId, data) {
        const client = await this.db.connect();
        try {
            await client.query(
                'UPDATE users SET name = $1, email = $2 WHERE id = $3',
                [data.name, data.email, userId]
            );
            return true;
        } catch (error) {
            throw error;
        } finally {
            client.release();
        }
    }

    async _createUserInDB(data) {
        const client = await this.db.connect();
        try {
            const result = await client.query(
                'INSERT INTO users (name, email) VALUES ($1, $2) RETURNING id',
                [data.name, data.email]
            );
            return result.rows[0].id;
        } catch (error) {
            console.error('Error creating user:', error);
            return null;
        } finally {
            client.release();
        }
    }
}

// ===== VARIANTE : Write-Through avec Retry Logic =====

class WriteThroughWithRetry extends WriteThroughPattern {
    async updateUserWithRetry(userId, userData, maxRetries = 3) {
        let attempts = 0;

        while (attempts < maxRetries) {
            try {
                return await this.updateUser(userId, userData);
            } catch (error) {
                attempts++;
                console.log(`Retry attempt ${attempts}/${maxRetries}`);

                if (attempts >= maxRetries) {
                    throw error;
                }

                // Exponential backoff
                await new Promise(resolve =>
                    setTimeout(resolve, Math.pow(2, attempts) * 100)
                );
            }
        }
    }
}

// ===== Exemple d'utilisation =====

(async () => {
    const redis = new Redis();
    const db = new Pool({
        host: 'localhost',
        database: 'myapp',
        user: 'postgres'
    });

    const cache = new WriteThroughPattern(redis, db);

    // CrÃ©er un utilisateur
    const userId = await cache.createUser({
        name: 'Bob',
        email: 'bob@example.com'
    });
    console.log(`Created user: ${userId}`);

    // Mise Ã  jour Write-Through
    const success = await cache.updateUser(userId, {
        name: 'Bob Johnson',
        email: 'bob.johnson@example.com'
    });
    console.log(`Update success: ${success}`);

    // Lecture depuis le cache
    const user = await cache.getUser(userId);
    console.log('User:', user);
})();
```

### Avantages et inconvÃ©nients

**âœ… Avantages**
- **Strong consistency** : Cache et DB toujours synchronisÃ©s
- **Simple reasoning** : Pas de pÃ©riode de staleness
- **Cache toujours chaud** : Toutes les donnÃ©es rÃ©centes sont en cache
- **Protection des reads** : Cache protÃ¨ge la DB des lectures

**âŒ InconvÃ©nients**
- **Write latency** : Chaque Ã©criture attend la DB (plus lent)
- **Write amplification** : Toutes les Ã©critures passent par 2 systÃ¨mes
- **Single point of failure** : Si Redis tombe, les Ã©critures Ã©chouent
- **ComplexitÃ© de rollback** : GÃ©rer les Ã©checs DB nÃ©cessite du code

### Bonnes pratiques

```python
# âœ… BON : Transaction pour garantir atomicitÃ©
def update_with_transaction(redis_client, db, user_id, data):
    cache_key = f"user:{user_id}"

    # Utiliser une transaction DB
    cursor = db.cursor()
    try:
        cursor.execute("BEGIN")

        # Ã‰criture DB
        cursor.execute(
            "UPDATE users SET name = %s WHERE id = %s",
            (data['name'], user_id)
        )

        # Ã‰criture cache seulement si DB rÃ©ussit
        redis_client.setex(cache_key, 3600, json.dumps(data))

        cursor.execute("COMMIT")
        return True
    except Exception as e:
        cursor.execute("ROLLBACK")
        return False

# âœ… BON : Logging pour audit trail
import logging

class WriteThroughWithAudit(WriteThroughPattern):
    def __init__(self, redis_client, db):
        super().__init__(redis_client, db)
        self.logger = logging.getLogger(__name__)

    def update_user(self, user_id, data):
        self.logger.info(f"Updating user {user_id}: {data}")

        success = super().update_user(user_id, data)

        if success:
            self.logger.info(f"Successfully updated user {user_id}")
        else:
            self.logger.error(f"Failed to update user {user_id}")

        return success

# âœ… BON : Health check pour dÃ©tecter les dÃ©synchronisations
def verify_consistency(redis_client, db, user_id):
    """VÃ©rifie que cache et DB sont synchronisÃ©s"""
    cache_key = f"user:{user_id}"

    # Lire depuis cache
    cached = redis_client.get(cache_key)

    # Lire depuis DB
    cursor = db.cursor()
    cursor.execute("SELECT name, email FROM users WHERE id = %s", (user_id,))
    db_data = cursor.fetchone()

    if cached and db_data:
        cached_obj = json.loads(cached)
        return (cached_obj['name'] == db_data[0] and
                cached_obj['email'] == db_data[1])

    return cached is None and db_data is None
```

---

## Pattern 3 : Write-Back (Write-Behind)

### Concept

Le pattern **Write-Back** Ã©crit d'abord dans Redis, puis persiste dans la base de donnÃ©es de maniÃ¨re **asynchrone** et **batched**. C'est le pattern le plus performant mais aussi le plus risquÃ© : les donnÃ©es peuvent Ãªtre perdues si Redis crash avant la persistance.

### Flux d'Ã©criture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ APPLICATION  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ 1. UPDATE data (fast!)
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    REDIS     â”‚  â† Immediate write
â”‚ SET key data â”‚
â”‚ LPUSH queue  â”‚  â† Add to write queue
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â””â”€â†’ Return success immediately âœ“


   [Background Worker Thread/Process]
              â”‚
              â”‚ 2. RPOP queue (batch)
              â†“
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ Write Buffer â”‚  â† Accumulate writes
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â”‚ 3. Batch INSERT/UPDATE
              â†“
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚  DATABASE    â”‚  â† Async persistence
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Architecture dÃ©taillÃ©e

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    WRITE-BACK ARCHITECTURE                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  [Client Requests]
         â”‚
         â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   Redis     â”‚
  â”‚  (Cache)    â”‚ â† 1. Immediate write (< 1ms)
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â†’ [Value] : user:123 â†’ {"name": "Alice"}
         â”‚
         â””â”€â†’ [Queue] : write_queue â†’ [
                 {"table": "users", "id": 123, "data": {...}},
                 {"table": "users", "id": 456, "data": {...}},
                 ...
             ]

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚      Background Worker (Async)          â”‚
  â”‚                                         â”‚
  â”‚  while True:                            â”‚
  â”‚    items = RPOP write_queue 100         â”‚
  â”‚    batch_write_to_db(items)             â”‚
  â”‚    sleep(1)                             â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ PostgreSQL  â”‚ â† 2. Batched write (periodic)
  â”‚  (Primary)  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ImplÃ©mentation Python

```python
import redis
import psycopg2
import json
import time
import threading
from typing import Dict, Any, List
from datetime import datetime

class WriteBackPattern:
    def __init__(self, redis_client: redis.Redis, db_connection):
        self.cache = redis_client
        self.db = db_connection
        self.default_ttl = 3600
        self.write_queue_key = "write_queue"
        self.batch_size = 100
        self.flush_interval = 1  # secondes

        # DÃ©marrer le worker en arriÃ¨re-plan
        self.worker_thread = threading.Thread(target=self._background_writer)
        self.worker_thread.daemon = True
        self.worker_thread.start()

        print("âœ“ Write-Back worker started")

    def get_user(self, user_id: int) -> Optional[Dict[str, Any]]:
        """
        Lecture depuis le cache (Write-Back maintient le cache chaud)
        """
        cache_key = f"user:{user_id}"

        cached_data = self.cache.get(cache_key)
        if cached_data:
            print(f"âœ“ Cache HIT for user {user_id}")
            return json.loads(cached_data)

        # Si pas en cache, charger depuis DB
        print(f"âœ— Cache MISS for user {user_id}")
        user = self._load_user_from_db(user_id)

        if user:
            self.cache.setex(cache_key, self.default_ttl, json.dumps(user))

        return user

    def update_user(self, user_id: int, user_data: Dict[str, Any]) -> bool:
        """
        Write-Back : Ã‰criture immÃ©diate dans Redis, async vers DB
        """
        cache_key = f"user:{user_id}"

        # 1. Ã‰criture IMMÃ‰DIATE dans Redis
        try:
            self.cache.setex(
                cache_key,
                self.default_ttl,
                json.dumps(user_data)
            )
            print(f"â†’ Wrote to cache for user {user_id}")
        except Exception as e:
            print(f"âœ— Cache write failed: {e}")
            return False

        # 2. Enqueuer pour Ã©criture DB asynchrone
        write_job = {
            'table': 'users',
            'operation': 'update',
            'id': user_id,
            'data': user_data,
            'timestamp': datetime.now().isoformat()
        }

        self.cache.lpush(self.write_queue_key, json.dumps(write_job))
        print(f"â†’ Enqueued DB write for user {user_id}")

        # Retourne immÃ©diatement (sans attendre la DB)
        return True

    def create_user(self, user_data: Dict[str, Any]) -> int:
        """
        CrÃ©ation avec Write-Back
        Note : L'ID doit Ãªtre gÃ©nÃ©rÃ© cÃ´tÃ© application ou via Redis INCR
        """
        # GÃ©nÃ©rer un ID avec Redis
        user_id = int(self.cache.incr("user_id_sequence"))
        user_data['id'] = user_id

        # Ã‰criture immÃ©diate dans Redis
        cache_key = f"user:{user_id}"
        self.cache.setex(cache_key, self.default_ttl, json.dumps(user_data))

        # Enqueuer pour DB
        write_job = {
            'table': 'users',
            'operation': 'insert',
            'id': user_id,
            'data': user_data,
            'timestamp': datetime.now().isoformat()
        }

        self.cache.lpush(self.write_queue_key, json.dumps(write_job))
        print(f"âœ“ Created user {user_id} (async to DB)")

        return user_id

    def _background_writer(self):
        """
        Worker thread qui traite la queue d'Ã©criture
        """
        while True:
            try:
                # RÃ©cupÃ©rer un batch d'Ã©critures
                jobs = self._pop_batch(self.batch_size)

                if jobs:
                    print(f"â†’ Processing batch of {len(jobs)} writes")
                    self._batch_write_to_db(jobs)
                    print(f"âœ“ Batch persisted to DB")

                # Attendre avant le prochain flush
                time.sleep(self.flush_interval)

            except Exception as e:
                print(f"âœ— Error in background writer: {e}")
                time.sleep(5)  # Backoff en cas d'erreur

    def _pop_batch(self, size: int) -> List[Dict[str, Any]]:
        """
        RÃ©cupÃ¨re un batch depuis la queue
        """
        jobs = []

        # Utiliser RPOP en boucle pour rÃ©cupÃ©rer plusieurs Ã©lÃ©ments
        # (Redis 6.2+ supporte RPOP avec count)
        for _ in range(size):
            job_json = self.cache.rpop(self.write_queue_key)
            if not job_json:
                break
            jobs.append(json.loads(job_json))

        return jobs

    def _batch_write_to_db(self, jobs: List[Dict[str, Any]]):
        """
        Ã‰crit un batch dans la DB (optimisÃ©)
        """
        if not jobs:
            return

        cursor = self.db.cursor()

        try:
            # Grouper par type d'opÃ©ration
            inserts = [j for j in jobs if j['operation'] == 'insert']
            updates = [j for j in jobs if j['operation'] == 'update']

            # Batch INSERT
            if inserts:
                insert_values = [
                    (j['id'], j['data']['name'], j['data']['email'])
                    for j in inserts
                ]

                cursor.executemany(
                    "INSERT INTO users (id, name, email) VALUES (%s, %s, %s) "
                    "ON CONFLICT (id) DO NOTHING",
                    insert_values
                )

            # Batch UPDATE
            if updates:
                update_values = [
                    (j['data']['name'], j['data']['email'], j['id'])
                    for j in updates
                ]

                cursor.executemany(
                    "UPDATE users SET name = %s, email = %s WHERE id = %s",
                    update_values
                )

            self.db.commit()

        except Exception as e:
            self.db.rollback()
            print(f"âœ— Batch write failed: {e}")

            # Re-enqueuer les jobs Ã©chouÃ©s (avec retry logic)
            for job in jobs:
                self.cache.lpush(self.write_queue_key, json.dumps(job))

            raise e

    def _load_user_from_db(self, user_id: int) -> Optional[Dict[str, Any]]:
        cursor = self.db.cursor()
        cursor.execute(
            "SELECT id, name, email, created_at FROM users WHERE id = %s",
            (user_id,)
        )
        row = cursor.fetchone()

        if row:
            return {
                'id': row[0],
                'name': row[1],
                'email': row[2],
                'created_at': str(row[3])
            }
        return None

    def force_flush(self):
        """
        Force le flush de toutes les Ã©critures en attente
        Utile lors d'un shutdown graceful
        """
        print("â†’ Forcing flush of pending writes...")

        while True:
            jobs = self._pop_batch(self.batch_size)
            if not jobs:
                break
            self._batch_write_to_db(jobs)

        print("âœ“ All pending writes flushed")


# ===== VARIANTE : Write-Back avec prioritÃ© =====

class WriteBackWithPriority(WriteBackPattern):
    """
    Supporte plusieurs queues avec diffÃ©rentes prioritÃ©s
    """

    def __init__(self, redis_client, db):
        super().__init__(redis_client, db)
        self.high_priority_queue = "write_queue:high"
        self.low_priority_queue = "write_queue:low"

    def update_user(self, user_id: int, user_data: Dict[str, Any], priority='normal') -> bool:
        cache_key = f"user:{user_id}"

        # Ã‰criture cache
        self.cache.setex(cache_key, self.default_ttl, json.dumps(user_data))

        # Choisir la queue selon prioritÃ©
        queue_key = (self.high_priority_queue if priority == 'high'
                    else self.write_queue_key)

        write_job = {
            'table': 'users',
            'operation': 'update',
            'id': user_id,
            'data': user_data,
            'priority': priority,
            'timestamp': datetime.now().isoformat()
        }

        self.cache.lpush(queue_key, json.dumps(write_job))
        print(f"â†’ Enqueued {priority} priority write for user {user_id}")

        return True

    def _background_writer(self):
        """Worker qui traite la high priority en premier"""
        while True:
            try:
                # Traiter high priority d'abord
                high_jobs = self._pop_batch_from_queue(
                    self.high_priority_queue,
                    self.batch_size
                )

                if high_jobs:
                    print(f"â†’ Processing HIGH priority batch: {len(high_jobs)}")
                    self._batch_write_to_db(high_jobs)

                # Puis normal priority
                normal_jobs = self._pop_batch(self.batch_size)

                if normal_jobs:
                    print(f"â†’ Processing NORMAL priority batch: {len(normal_jobs)}")
                    self._batch_write_to_db(normal_jobs)

                time.sleep(self.flush_interval)

            except Exception as e:
                print(f"âœ— Error in background writer: {e}")
                time.sleep(5)

    def _pop_batch_from_queue(self, queue_key: str, size: int) -> List[Dict]:
        jobs = []
        for _ in range(size):
            job_json = self.cache.rpop(queue_key)
            if not job_json:
                break
            jobs.append(json.loads(job_json))
        return jobs


# ===== VARIANTE : Write-Back avec Dead Letter Queue =====

class WriteBackWithDLQ(WriteBackPattern):
    """
    Ã‰critures qui Ã©chouent aprÃ¨s X retries vont dans une Dead Letter Queue
    """

    def __init__(self, redis_client, db):
        super().__init__(redis_client, db)
        self.dlq_key = "write_queue:dlq"
        self.max_retries = 3

    def _batch_write_to_db(self, jobs: List[Dict[str, Any]]):
        cursor = self.db.cursor()

        for job in jobs:
            retry_count = job.get('retry_count', 0)

            try:
                # Tenter l'Ã©criture
                if job['operation'] == 'update':
                    cursor.execute(
                        "UPDATE users SET name = %s, email = %s WHERE id = %s",
                        (job['data']['name'], job['data']['email'], job['id'])
                    )

                self.db.commit()

            except Exception as e:
                self.db.rollback()

                if retry_count < self.max_retries:
                    # Re-enqueuer avec retry_count incrÃ©mentÃ©
                    job['retry_count'] = retry_count + 1
                    self.cache.lpush(self.write_queue_key, json.dumps(job))
                    print(f"âš ï¸  Retry {retry_count + 1}/{self.max_retries} for job {job['id']}")
                else:
                    # DÃ©placer vers DLQ
                    self.cache.lpush(self.dlq_key, json.dumps(job))
                    print(f"âœ— Job {job['id']} moved to DLQ after {self.max_retries} retries")


# ===== Exemple d'utilisation =====

if __name__ == '__main__':
    r = redis.Redis(host='localhost', port=6379, decode_responses=True)
    db = psycopg2.connect("dbname=myapp user=postgres")

    cache = WriteBackPattern(r, db)

    # CrÃ©er plusieurs utilisateurs (trÃ¨s rapide!)
    for i in range(5):
        user_id = cache.create_user({
            'name': f'User {i}',
            'email': f'user{i}@example.com'
        })
        print(f"Created user {user_id}")

    # Les Ã©critures DB se font en arriÃ¨re-plan
    print("\nâ†’ Waiting for background writes...")
    time.sleep(3)

    # Lecture depuis le cache (toujours Ã  jour)
    user = cache.get_user(1)
    print(f"\nUser from cache: {user}")

    # Mise Ã  jour (retour immÃ©diat)
    cache.update_user(1, {
        'name': 'Updated User',
        'email': 'updated@example.com'
    })

    # Force flush avant shutdown
    print("\nâ†’ Shutting down...")
    cache.force_flush()
    print("âœ“ Shutdown complete")
```

### ImplÃ©mentation Node.js

```javascript
const Redis = require('ioredis');
const { Pool } = require('pg');

class WriteBackPattern {
    constructor(redisClient, dbPool) {
        this.cache = redisClient;
        this.db = dbPool;
        this.defaultTTL = 3600;
        this.writeQueueKey = 'write_queue';
        this.batchSize = 100;
        this.flushInterval = 1000; // ms

        // DÃ©marrer le worker
        this._startBackgroundWriter();
        console.log('âœ“ Write-Back worker started');
    }

    async getUser(userId) {
        const cacheKey = `user:${userId}`;

        const cachedData = await this.cache.get(cacheKey);
        if (cachedData) {
            console.log(`âœ“ Cache HIT for user ${userId}`);
            return JSON.parse(cachedData);
        }

        console.log(`âœ— Cache MISS for user ${userId}`);
        const user = await this._loadUserFromDB(userId);

        if (user) {
            await this.cache.setex(
                cacheKey,
                this.defaultTTL,
                JSON.stringify(user)
            );
        }

        return user;
    }

    async updateUser(userId, userData) {
        const cacheKey = `user:${userId}`;

        // 1. Ã‰criture IMMÃ‰DIATE dans Redis
        try {
            await this.cache.setex(
                cacheKey,
                this.defaultTTL,
                JSON.stringify(userData)
            );
            console.log(`â†’ Wrote to cache for user ${userId}`);
        } catch (error) {
            console.error(`âœ— Cache write failed: ${error}`);
            return false;
        }

        // 2. Enqueuer pour DB
        const writeJob = {
            table: 'users',
            operation: 'update',
            id: userId,
            data: userData,
            timestamp: new Date().toISOString()
        };

        await this.cache.lpush(
            this.writeQueueKey,
            JSON.stringify(writeJob)
        );
        console.log(`â†’ Enqueued DB write for user ${userId}`);

        return true; // Retour immÃ©diat
    }

    async createUser(userData) {
        // GÃ©nÃ©rer un ID avec Redis
        const userId = await this.cache.incr('user_id_sequence');
        userData.id = userId;

        // Ã‰criture cache
        const cacheKey = `user:${userId}`;
        await this.cache.setex(
            cacheKey,
            this.defaultTTL,
            JSON.stringify(userData)
        );

        // Enqueuer
        const writeJob = {
            table: 'users',
            operation: 'insert',
            id: userId,
            data: userData,
            timestamp: new Date().toISOString()
        };

        await this.cache.lpush(
            this.writeQueueKey,
            JSON.stringify(writeJob)
        );
        console.log(`âœ“ Created user ${userId} (async to DB)`);

        return userId;
    }

    _startBackgroundWriter() {
        this.writerInterval = setInterval(
            async () => {
                try {
                    const jobs = await this._popBatch(this.batchSize);

                    if (jobs.length > 0) {
                        console.log(`â†’ Processing batch of ${jobs.length} writes`);
                        await this._batchWriteToDB(jobs);
                        console.log(`âœ“ Batch persisted to DB`);
                    }
                } catch (error) {
                    console.error(`âœ— Error in background writer: ${error}`);
                }
            },
            this.flushInterval
        );
    }

    async _popBatch(size) {
        const jobs = [];

        for (let i = 0; i < size; i++) {
            const jobJson = await this.cache.rpop(this.writeQueueKey);
            if (!jobJson) break;
            jobs.push(JSON.parse(jobJson));
        }

        return jobs;
    }

    async _batchWriteToDB(jobs) {
        if (jobs.length === 0) return;

        const client = await this.db.connect();

        try {
            await client.query('BEGIN');

            // Grouper par opÃ©ration
            const inserts = jobs.filter(j => j.operation === 'insert');
            const updates = jobs.filter(j => j.operation === 'update');

            // Batch INSERT
            if (inserts.length > 0) {
                const values = inserts.map(j =>
                    `(${j.id}, '${j.data.name}', '${j.data.email}')`
                ).join(',');

                await client.query(
                    `INSERT INTO users (id, name, email) VALUES ${values}
                     ON CONFLICT (id) DO NOTHING`
                );
            }

            // Batch UPDATE
            for (const job of updates) {
                await client.query(
                    'UPDATE users SET name = $1, email = $2 WHERE id = $3',
                    [job.data.name, job.data.email, job.id]
                );
            }

            await client.query('COMMIT');

        } catch (error) {
            await client.query('ROLLBACK');
            console.error(`âœ— Batch write failed: ${error}`);

            // Re-enqueuer les jobs
            for (const job of jobs) {
                await this.cache.lpush(
                    this.writeQueueKey,
                    JSON.stringify(job)
                );
            }

            throw error;
        } finally {
            client.release();
        }
    }

    async _loadUserFromDB(userId) {
        const result = await this.db.query(
            'SELECT id, name, email, created_at FROM users WHERE id = $1',
            [userId]
        );

        if (result.rows.length > 0) {
            const row = result.rows[0];
            return {
                id: row.id,
                name: row.name,
                email: row.email,
                created_at: row.created_at.toISOString()
            };
        }

        return null;
    }

    async forceFlush() {
        console.log('â†’ Forcing flush of pending writes...');

        while (true) {
            const jobs = await this._popBatch(this.batchSize);
            if (jobs.length === 0) break;
            await this._batchWriteToDB(jobs);
        }

        console.log('âœ“ All pending writes flushed');
    }

    async shutdown() {
        clearInterval(this.writerInterval);
        await this.forceFlush();
    }
}

// ===== Exemple d'utilisation =====

(async () => {
    const redis = new Redis();
    const db = new Pool({
        host: 'localhost',
        database: 'myapp',
        user: 'postgres'
    });

    const cache = new WriteBackPattern(redis, db);

    // CrÃ©er plusieurs utilisateurs (trÃ¨s rapide!)
    for (let i = 0; i < 5; i++) {
        const userId = await cache.createUser({
            name: `User ${i}`,
            email: `user${i}@example.com`
        });
        console.log(`Created user ${userId}`);
    }

    // Les Ã©critures DB se font en arriÃ¨re-plan
    console.log('\nâ†’ Waiting for background writes...');
    await new Promise(resolve => setTimeout(resolve, 3000));

    // Lecture depuis le cache
    const user = await cache.getUser(1);
    console.log('\nUser from cache:', user);

    // Shutdown graceful
    console.log('\nâ†’ Shutting down...');
    await cache.shutdown();
    console.log('âœ“ Shutdown complete');

    process.exit(0);
})();
```

### Avantages et inconvÃ©nients

**âœ… Avantages**
- **Performance maximale** : Ã‰critures sub-milliseconde
- **DÃ©bit Ã©levÃ©** : Supporte des milliers d'Ã©critures/sec
- **Batching** : RÃ©duit la charge sur la DB (10x-100x moins de requÃªtes)
- **Smooth DB load** : Lisse les pics d'Ã©criture

**âŒ InconvÃ©nients**
- **Risque de perte** : Si Redis crash, les Ã©critures non persistÃ©es sont perdues
- **ComplexitÃ©** : Worker threads, queues, retry logic
- **CohÃ©rence faible** : Latence entre cache et DB
- **Debugging difficile** : Ã‰critures asynchrones compliquent le troubleshooting

### Quand utiliser Write-Back

```python
# âœ… Cas d'usage idÃ©aux pour Write-Back

# 1. Analytics / Metrics (perte acceptable)
def record_page_view(page_id, user_id):
    """Les vues de page peuvent tolÃ©rer une petite perte"""
    cache.incr(f"pageviews:{page_id}")
    enqueue_write('pageviews', page_id, user_id)

# 2. Logging haute frÃ©quence
def log_event(event_type, data):
    """Logs peuvent Ãªtre batched"""
    enqueue_write('events', event_type, data)

# 3. Leaderboards (eventual consistency OK)
def update_score(user_id, score):
    """Le leaderboard peut avoir quelques secondes de latence"""
    cache.zadd('leaderboard', {user_id: score})
    enqueue_write('scores', user_id, score)

# âŒ NE PAS utiliser Write-Back pour

# 1. Transactions financiÃ¨res (perte inacceptable)
def process_payment(amount):
    # Toujours Ã©crire directement en DB
    db.insert('payments', amount)

# 2. DonnÃ©es critiques (compliance, lÃ©gal)
def store_gdpr_consent(user_id, consent):
    # Write-Through ou direct DB
    write_through(user_id, consent)

# 3. DonnÃ©es de sÃ©curitÃ©
def log_security_event(event):
    # Direct DB avec ACK
    db.insert_with_confirmation('security_log', event)
```

---

## Comparaison finale et guide de choix

### Matrice de dÃ©cision

| CritÃ¨re | Cache-Aside | Write-Through | Write-Back |
|---------|-------------|---------------|------------|
| **Read Performance** | âš¡âš¡âš¡ Excellent | âš¡âš¡âš¡ Excellent | âš¡âš¡âš¡ Excellent |
| **Write Performance** | âš¡âš¡ Good | âš¡ Average | âš¡âš¡âš¡ Excellent |
| **Consistency** | ğŸŸ¡ Eventual | ğŸŸ¢ Strong | ğŸ”´ Weak |
| **Durability** | ğŸŸ¢ High | ğŸŸ¢ High | ğŸ”´ At Risk |
| **Complexity** | ğŸŸ¢ Simple | ğŸŸ¡ Medium | ğŸ”´ Complex |
| **DB Protection** | ğŸŸ¡ Partial | ğŸŸ¢ Full | ğŸŸ¢ Full |
| **Resilience** | ğŸŸ¢ High | ğŸŸ¡ Medium | ğŸ”´ Low |

### Arbre de dÃ©cision

```
                         Choisir un Caching Pattern
                                    â”‚
                                    â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ TolÃ©rance Ã  la perte ?    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                               â”‚
                 Non                              Oui
                    â”‚                               â”‚
                    â†“                               â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Writes > 1000/sec ?  â”‚       â”‚    Write-Back        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚  (Performance Max)   â”‚
                    â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                       â”‚
       Non                     Oui
        â”‚                       â”‚
        â†“                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cache-Aside    â”‚    â”‚  Write-Through     â”‚
â”‚ (Standard)     â”‚    â”‚ (High Consistency) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Patterns hybrides recommandÃ©s

```python
# Pattern Hybride : Cache-Aside + Write-Through selon le type de donnÃ©e

class HybridCachingPattern:
    def __init__(self, redis_client, db):
        self.cache = redis_client
        self.db = db

        # Configurer le pattern par type d'entitÃ©
        self.patterns = {
            'user_profile': 'cache_aside',    # Peu d'Ã©critures
            'product_inventory': 'write_through',  # Consistency critique
            'page_views': 'write_back',       # Haute frÃ©quence
            'session': 'cache_aside'          # Lecture intensive
        }

    def get(self, entity_type, entity_id):
        pattern = self.patterns.get(entity_type, 'cache_aside')

        if pattern == 'cache_aside':
            return self._cache_aside_get(entity_type, entity_id)
        else:
            return self._standard_get(entity_type, entity_id)

    def update(self, entity_type, entity_id, data):
        pattern = self.patterns.get(entity_type, 'cache_aside')

        if pattern == 'cache_aside':
            return self._cache_aside_update(entity_type, entity_id, data)
        elif pattern == 'write_through':
            return self._write_through_update(entity_type, entity_id, data)
        elif pattern == 'write_back':
            return self._write_back_update(entity_type, entity_id, data)
```

### Checklist finale

Avant de choisir votre pattern, posez-vous ces questions :

**Cache-Aside**
- âœ… Ratio lecture/Ã©criture > 10:1 ?
- âœ… TolÃ©rance Ã  des donnÃ©es lÃ©gÃ¨rement stale ?
- âœ… Besoin de simplicitÃ© ?
- âœ… RÃ©silience critique si Redis tombe ?

**Write-Through**
- âœ… Consistency forte requise ?
- âœ… Ratio lecture/Ã©criture < 5:1 ?
- âœ… Budget latency d'Ã©criture acceptable ?
- âœ… Toutes les donnÃ©es doivent Ãªtre en cache ?

**Write-Back**
- âœ… Volume d'Ã©critures > 1000/sec ?
- âœ… Latence d'Ã©criture critique (< 1ms) ?
- âœ… TolÃ©rance Ã  une petite perte de donnÃ©es ?
- âœ… Infrastructure pour worker threads ?

---

## MÃ©triques et monitoring

Pour tous les patterns, surveillez :

```python
# MÃ©triques clÃ©s Ã  tracker

class CacheMetrics:
    def __init__(self):
        self.reads = 0
        self.writes = 0
        self.cache_hits = 0
        self.cache_misses = 0
        self.write_latencies = []
        self.read_latencies = []

    def report(self):
        hit_rate = (self.cache_hits / (self.cache_hits + self.cache_misses)) * 100
        avg_read_latency = sum(self.read_latencies) / len(self.read_latencies)
        avg_write_latency = sum(self.write_latencies) / len(self.write_latencies)

        return {
            'hit_rate': f"{hit_rate:.2f}%",
            'read_latency_avg': f"{avg_read_latency:.2f}ms",
            'write_latency_avg': f"{avg_write_latency:.2f}ms",
            'total_operations': self.reads + self.writes
        }

# Alertes recommandÃ©es
ALERTS = {
    'hit_rate': {
        'threshold': 80,  # %
        'severity': 'warning'
    },
    'write_queue_length': {  # Pour Write-Back
        'threshold': 10000,
        'severity': 'critical'
    },
    'db_sync_lag': {  # Pour Write-Back
        'threshold': 60,  # secondes
        'severity': 'warning'
    }
}
```

---

## Conclusion

Les trois patterns de caching ont chacun leur place dans une architecture moderne :

- **Cache-Aside** : Le choix par dÃ©faut pour 80% des cas
- **Write-Through** : Quand la cohÃ©rence prime sur la performance
- **Write-Back** : Quand la performance est absolument critique

Le secret est souvent d'utiliser **plusieurs patterns dans la mÃªme application**, selon le type de donnÃ©es et les contraintes mÃ©tier.

---


â­ï¸ [StratÃ©gies anti-problÃ¨mes : Cache Avalanche, Stampede et Penetration](/06-patterns-developpement-avances/02-strategies-anti-problemes.md)
