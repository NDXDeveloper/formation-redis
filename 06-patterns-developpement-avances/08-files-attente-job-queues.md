üîù Retour au [Sommaire](/SOMMAIRE.md)

# 6.8 Files d'attente et Job Queues

## Introduction

Les **files d'attente** (queues) sont essentielles pour d√©coupler les composants d'une application, g√©rer des t√¢ches asynchrones, et construire des syst√®mes distribu√©s robustes. Redis, avec ses structures de donn√©es natives et ses op√©rations atomiques, est parfaitement adapt√© pour impl√©menter des syst√®mes de queues performants.

Cette section explore diff√©rents patterns de queues : simple queue, reliable queue, priority queue, delayed jobs, avec des impl√©mentations production-ready.

## Pourquoi utiliser des Job Queues ?

### Cas d'usage typiques

**1. Traitement asynchrone**

```text
Sc√©nario : Upload et traitement d'image

Sans queue (synchrone):
User uploads image (5MB)
    ‚Üì
Server processes:
  - Resize image (2s)
  - Generate thumbnails (3s)
  - Apply filters (2s)
  - Upload to CDN (3s)
    ‚Üì
Response after 10 seconds ‚ùå
Poor user experience

Avec queue (asynchrone):
User uploads image
    ‚Üì
Add job to queue (50ms)
    ‚Üì
Response immediately ‚úÖ
Good user experience
    ‚Üì
Workers process in background
```

**2. Peak Shaving (Lissage des pics)**

```text
Sc√©nario : Black Friday - 10,000 orders/minute

Sans queue:
Orders ‚Üí Direct processing
         ‚Üì
         Database overload ‚ùå
         System crash

Avec queue:
Orders ‚Üí Queue (buffer)
         ‚Üì
         Workers process at sustainable rate
         (100 orders/minute per worker)
         ‚Üì
         System stable ‚úÖ
```

**3. Task Distribution**

```text
Sc√©nario : Video encoding avec 10 workers

                  Queue
                    ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ           ‚îÇ           ‚îÇ
    Worker 1    Worker 2    Worker 3
    (Idle)      (Busy)      (Idle)
        ‚îÇ                       ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Get next job ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               from queue
```

**4. Retry Logic**

```text
Sc√©nario : Email sending avec failures

Direct send ‚Üí SMTP timeout ‚Üí Lost ‚ùå

Queue send ‚Üí Failed ‚Üí Retry (3 attempts) ‚Üí Success ‚úÖ
```

---

## Pattern 1: Simple Queue (FIFO)

### Principe avec LIST

Redis LIST est parfait pour impl√©menter une queue FIFO (First In, First Out).

```text
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    SIMPLE QUEUE (LIST)                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Producer                  Redis LIST                  Consumer
   ‚îÇ                    queue:jobs                        ‚îÇ
   ‚îÇ                                                      ‚îÇ
   ‚îÇ  LPUSH job1                                          ‚îÇ
   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> [job1]                            ‚îÇ
   ‚îÇ                                                      ‚îÇ
   ‚îÇ  LPUSH job2                                          ‚îÇ
   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> [job2, job1]                      ‚îÇ
   ‚îÇ                                                      ‚îÇ
   ‚îÇ  LPUSH job3                                          ‚îÇ
   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> [job3, job2, job1]                ‚îÇ
   ‚îÇ                                                      ‚îÇ
   ‚îÇ                    [job3, job2, job1]                ‚îÇ
   ‚îÇ                                       BRPOP ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
   ‚îÇ                    [job3, job2] <‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
   ‚îÇ                                     (got job1)       ‚îÇ
   ‚îÇ                                                      ‚îÇ
   ‚îÇ                    [job3, job2]                      ‚îÇ
   ‚îÇ                                       BRPOP ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
   ‚îÇ                    [job3] <‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
   ‚îÇ                                     (got job2)       ‚îÇ

Operations:
- LPUSH queue:jobs job_data  ‚Üí Add job (Producer)
- BRPOP queue:jobs timeout   ‚Üí Get job (Consumer, blocking)
- LLEN queue:jobs            ‚Üí Queue size
```

### Impl√©mentation Python

```python
import redis
import json
import uuid
import time
from datetime import datetime
from typing import Dict, Any, Optional

class SimpleQueue:
    """
    Simple FIFO queue avec Redis LIST
    """

    def __init__(self, redis_client, queue_name='jobs'):
        self.redis = redis_client
        self.queue_name = f"queue:{queue_name}"

    def enqueue(self, job_data: Dict[str, Any]) -> str:
        """
        Ajoute un job dans la queue

        Args:
            job_data: Donn√©es du job

        Returns:
            str: Job ID
        """
        job_id = str(uuid.uuid4())

        job = {
            'id': job_id,
            'data': job_data,
            'enqueued_at': time.time(),
            'status': 'pending'
        }

        # Ajouter √† la queue (LPUSH = add to left/head)
        self.redis.lpush(self.queue_name, json.dumps(job))

        print(f"‚úì Job enqueued: {job_id}")
        print(f"  Queue size: {self.size()}")

        return job_id

    def dequeue(self, timeout=0) -> Optional[Dict[str, Any]]:
        """
        R√©cup√®re un job de la queue

        Args:
            timeout: Temps d'attente en secondes (0 = non-bloquant)

        Returns:
            dict or None: Job data
        """
        if timeout > 0:
            # BRPOP = Blocking Right POP (from right/tail)
            result = self.redis.brpop(self.queue_name, timeout=timeout)
        else:
            # RPOP = Non-blocking
            result = self.redis.rpop(self.queue_name)
            if result:
                result = (self.queue_name, result)
            else:
                return None

        if not result:
            return None

        _, job_json = result
        job = json.loads(job_json)

        print(f"‚úì Job dequeued: {job['id']}")

        return job

    def size(self) -> int:
        """Retourne la taille de la queue"""
        return self.redis.llen(self.queue_name)

    def clear(self):
        """Vide la queue"""
        deleted = self.redis.delete(self.queue_name)
        print(f"‚úì Queue cleared ({deleted} keys deleted)")


# ============================================================
# PRODUCER (Producteur de jobs)
# ============================================================

class JobProducer:
    """Producteur de jobs"""

    def __init__(self, queue):
        self.queue = queue

    def submit_email_job(self, to, subject, body):
        """Soumettre un job d'envoi d'email"""
        job_data = {
            'type': 'send_email',
            'to': to,
            'subject': subject,
            'body': body
        }

        return self.queue.enqueue(job_data)

    def submit_image_processing_job(self, image_url, operations):
        """Soumettre un job de traitement d'image"""
        job_data = {
            'type': 'process_image',
            'image_url': image_url,
            'operations': operations
        }

        return self.queue.enqueue(job_data)


# ============================================================
# CONSUMER / WORKER
# ============================================================

class JobWorker:
    """Worker qui traite les jobs"""

    def __init__(self, queue, worker_id='worker-1'):
        self.queue = queue
        self.worker_id = worker_id
        self.running = False

    def start(self):
        """D√©marre le worker"""
        self.running = True
        print(f"üöÄ Worker {self.worker_id} started")

        while self.running:
            # Attendre un job (bloquant, timeout 5s)
            job = self.queue.dequeue(timeout=5)

            if job:
                self.process_job(job)
            else:
                print(f"  {self.worker_id}: No jobs, waiting...")

    def stop(self):
        """Arr√™te le worker"""
        self.running = False
        print(f"üõë Worker {self.worker_id} stopped")

    def process_job(self, job):
        """Traite un job"""
        job_type = job['data'].get('type')

        print(f"\n{self.worker_id} processing job {job['id']} (type: {job_type})")

        try:
            if job_type == 'send_email':
                self._send_email(job['data'])
            elif job_type == 'process_image':
                self._process_image(job['data'])
            else:
                print(f"  Unknown job type: {job_type}")

            print(f"  ‚úì Job {job['id']} completed")

        except Exception as e:
            print(f"  ‚úó Job {job['id']} failed: {e}")

    def _send_email(self, data):
        """Simuler l'envoi d'un email"""
        print(f"  üìß Sending email to {data['to']}")
        time.sleep(1)  # Simuler le traitement
        print(f"  ‚úì Email sent")

    def _process_image(self, data):
        """Simuler le traitement d'image"""
        print(f"  üñºÔ∏è  Processing image: {data['image_url']}")
        time.sleep(2)  # Simuler le traitement
        print(f"  ‚úì Image processed")


# ============================================================
# EXEMPLE D'UTILISATION
# ============================================================

def example_simple_queue():
    """Exemple complet avec producer et consumer"""
    redis_client = redis.Redis(decode_responses=True)

    # Cr√©er la queue
    queue = SimpleQueue(redis_client, queue_name='email_jobs')

    # Producer : Ajouter des jobs
    producer = JobProducer(queue)

    print("=" * 60)
    print("ADDING JOBS TO QUEUE")
    print("=" * 60 + "\n")

    producer.submit_email_job(
        to='user@example.com',
        subject='Welcome!',
        body='Thanks for signing up'
    )

    producer.submit_email_job(
        to='admin@example.com',
        subject='New User',
        body='A new user signed up'
    )

    producer.submit_image_processing_job(
        image_url='https://example.com/image.jpg',
        operations=['resize', 'thumbnail']
    )

    print(f"\nQueue size: {queue.size()}")

    # Worker : Traiter les jobs
    print("\n" + "=" * 60)
    print("WORKER PROCESSING JOBS")
    print("=" * 60 + "\n")

    worker = JobWorker(queue)

    # Traiter quelques jobs manuellement
    for _ in range(3):
        job = queue.dequeue()
        if job:
            worker.process_job(job)
        else:
            print("No more jobs")
            break


# ============================================================
# MULTI-WORKER AVEC THREADING
# ============================================================

import threading

def run_worker(queue, worker_id, duration=10):
    """Ex√©cute un worker pour une dur√©e donn√©e"""
    worker = JobWorker(queue, worker_id=worker_id)

    def worker_thread():
        start_time = time.time()
        worker.running = True

        while worker.running and (time.time() - start_time) < duration:
            job = queue.dequeue(timeout=1)
            if job:
                worker.process_job(job)

    thread = threading.Thread(target=worker_thread)
    thread.start()
    return thread


def example_multi_worker():
    """Exemple avec plusieurs workers concurrents"""
    redis_client = redis.Redis(decode_responses=True)
    queue = SimpleQueue(redis_client, queue_name='tasks')
    producer = JobProducer(queue)

    print("=" * 60)
    print("MULTI-WORKER EXAMPLE")
    print("=" * 60 + "\n")

    # Ajouter 10 jobs
    print("Adding 10 jobs...")
    for i in range(10):
        producer.submit_email_job(
            to=f'user{i}@example.com',
            subject=f'Email {i}',
            body=f'Body {i}'
        )

    print(f"\nQueue size: {queue.size()}")

    # D√©marrer 3 workers
    print("\nStarting 3 workers...\n")
    threads = []
    for i in range(3):
        thread = run_worker(queue, f'worker-{i}', duration=15)
        threads.append(thread)

    # Attendre les workers
    for thread in threads:
        thread.join()

    print(f"\nFinal queue size: {queue.size()}")
```

---

## Pattern 2: Reliable Queue (BRPOPLPUSH)

### Probl√®me de la Simple Queue

```text
PROBL√àME : Job perdu si le worker crash

Worker                    Queue                   Processing
  ‚îÇ                    [job3, job2, job1]              ‚îÇ
  ‚îÇ                                                    ‚îÇ
  ‚îÇ  BRPOP                                             ‚îÇ
  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> [job3, job2]                    ‚îÇ
  ‚îÇ                                                    ‚îÇ
  ‚îÇ  Got job1                                          ‚îÇ
  ‚îÇ                                                    ‚îÇ
  ‚îÇ  Start processing ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> ‚îÇ
  ‚îÇ                                                    ‚îÇ
  üí• CRASH!                                            ‚îÇ
  ‚îÇ                                                    ‚îÇ
  ‚ùå job1 is LOST                                      ‚îÇ
  (removed from queue but not processed)

Solution: BRPOPLPUSH (atomic move)
```

### BRPOPLPUSH : Atomic Move

```text
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              RELIABLE QUEUE (BRPOPLPUSH)                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Operation: BRPOPLPUSH source destination timeout

Worker              Queue                Processing Queue
  ‚îÇ              queue:jobs              queue:processing
  ‚îÇ           [job3, job2, job1]               []
  ‚îÇ                                             ‚îÇ
  ‚îÇ  BRPOPLPUSH queue:jobs queue:processing     ‚îÇ
  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>
  ‚îÇ                                             ‚îÇ
  ‚îÇ           [job3, job2]                 [job1]
  ‚îÇ                                             ‚îÇ
  ‚îÇ  Processing job1...                         ‚îÇ
  ‚îÇ                                             ‚îÇ
  ‚îÇ  ‚úì Success                                  ‚îÇ
  ‚îÇ                                             ‚îÇ
  ‚îÇ  LREM queue:processing job1 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
  ‚îÇ                                        []   ‚îÇ

If worker crashes:
  üí• CRASH during processing

  Recovery worker checks queue:processing
  ‚îú‚îÄ Finds unfinished job1
  ‚îî‚îÄ Re-enqueue to queue:jobs ‚úÖ
```

### Impl√©mentation Python

```python
class ReliableQueue:
    """
    Reliable queue avec BRPOPLPUSH
    """

    def __init__(self, redis_client, queue_name='jobs'):
        self.redis = redis_client
        self.queue_name = f"queue:{queue_name}"
        self.processing_name = f"queue:{queue_name}:processing"
        self.worker_id = str(uuid.uuid4())[:8]

    def enqueue(self, job_data: Dict[str, Any]) -> str:
        """Ajoute un job"""
        job_id = str(uuid.uuid4())

        job = {
            'id': job_id,
            'data': job_data,
            'enqueued_at': time.time(),
            'attempts': 0,
            'max_attempts': 3
        }

        self.redis.lpush(self.queue_name, json.dumps(job))

        print(f"‚úì Job enqueued: {job_id}")
        return job_id

    def dequeue(self, timeout=0) -> Optional[Dict[str, Any]]:
        """
        R√©cup√®re un job de mani√®re fiable
        Utilise BRPOPLPUSH pour d√©placer atomiquement
        """
        if timeout > 0:
            # BRPOPLPUSH = atomic move from queue to processing
            result = self.redis.brpoplpush(
                self.queue_name,
                self.processing_name,
                timeout=timeout
            )
        else:
            result = self.redis.rpoplpush(
                self.queue_name,
                self.processing_name
            )

        if not result:
            return None

        job = json.loads(result)

        # Ajouter metadata du worker
        job['worker_id'] = self.worker_id
        job['started_at'] = time.time()

        print(f"‚úì Job dequeued: {job['id']} by {self.worker_id}")

        return job

    def ack(self, job: Dict[str, Any]):
        """
        Marque le job comme compl√©t√© (acknowledge)
        Supprime de la processing queue
        """
        job_json = json.dumps(job)

        # Supprimer de la processing queue
        removed = self.redis.lrem(self.processing_name, 1, job_json)

        if removed:
            print(f"‚úì Job acknowledged: {job['id']}")
        else:
            print(f"‚ö†Ô∏è  Job not found in processing queue: {job['id']}")

        return removed > 0

    def nack(self, job: Dict[str, Any], requeue=True):
        """
        Marque le job comme √©chou√© (negative acknowledge)

        Args:
            requeue: Si True, remettre dans la queue principale
        """
        job_json_old = json.dumps(job)

        # Supprimer de processing
        self.redis.lrem(self.processing_name, 1, job_json_old)

        if requeue and job.get('attempts', 0) < job.get('max_attempts', 3):
            # Incr√©menter attempts
            job['attempts'] = job.get('attempts', 0) + 1
            job['last_error_at'] = time.time()

            # Remettre dans la queue
            self.redis.lpush(self.queue_name, json.dumps(job))

            print(f"‚ö†Ô∏è  Job requeued: {job['id']} (attempt {job['attempts']})")
        else:
            print(f"‚úó Job failed permanently: {job['id']}")
            # Optionnel : ajouter √† une dead letter queue
            self._move_to_dlq(job)

    def _move_to_dlq(self, job):
        """D√©place vers Dead Letter Queue"""
        dlq_name = f"{self.queue_name}:dlq"
        self.redis.lpush(dlq_name, json.dumps(job))
        print(f"  ‚ûú Moved to DLQ")

    def recover_stale_jobs(self, timeout_seconds=300):
        """
        R√©cup√®re les jobs bloqu√©s dans processing
        (worker crashed avant de les terminer)
        """
        print("\nüîç Checking for stale jobs...")

        # R√©cup√©rer tous les jobs en processing
        processing_jobs = self.redis.lrange(self.processing_name, 0, -1)

        recovered = 0
        now = time.time()

        for job_json in processing_jobs:
            job = json.loads(job_json)
            started_at = job.get('started_at', 0)

            # Si le job tourne depuis trop longtemps
            if (now - started_at) > timeout_seconds:
                print(f"  Found stale job: {job['id']}")

                # Supprimer de processing
                self.redis.lrem(self.processing_name, 1, job_json)

                # Remettre dans la queue
                job['attempts'] = job.get('attempts', 0) + 1
                if job['attempts'] < job.get('max_attempts', 3):
                    self.redis.lpush(self.queue_name, json.dumps(job))
                    recovered += 1
                    print(f"    ‚Üª Recovered and requeued")
                else:
                    self._move_to_dlq(job)
                    print(f"    ‚úó Max attempts reached, moved to DLQ")

        print(f"‚úì Recovered {recovered} stale jobs\n")
        return recovered


# ============================================================
# RELIABLE WORKER
# ============================================================

class ReliableWorker:
    """Worker avec gestion d'erreurs et retry"""

    def __init__(self, queue, worker_id='worker-1'):
        self.queue = queue
        self.worker_id = worker_id
        self.running = False

    def start(self):
        """D√©marre le worker"""
        self.running = True
        print(f"üöÄ Reliable Worker {self.worker_id} started")

        # R√©cup√©rer les jobs bloqu√©s au d√©marrage
        self.queue.recover_stale_jobs()

        while self.running:
            job = self.queue.dequeue(timeout=5)

            if job:
                success = self.process_job(job)

                if success:
                    # Acknowledger le job
                    self.queue.ack(job)
                else:
                    # Negative acknowledge (requeue si attempts < max)
                    self.queue.nack(job, requeue=True)

    def process_job(self, job) -> bool:
        """
        Traite un job

        Returns:
            bool: True si succ√®s, False si √©chec
        """
        job_id = job['id']
        job_type = job['data'].get('type')

        print(f"\n{self.worker_id} processing {job_id} (attempt {job.get('attempts', 0) + 1})")

        try:
            # Simuler traitement
            if job_type == 'send_email':
                self._send_email(job['data'])
            elif job_type == 'fail_test':
                # Simuler un √©chec
                raise Exception("Simulated failure")
            else:
                print(f"  Processing {job_type}...")
                time.sleep(1)

            print(f"  ‚úì Job {job_id} completed")
            return True

        except Exception as e:
            print(f"  ‚úó Job {job_id} failed: {e}")
            return False

    def _send_email(self, data):
        """Simuler l'envoi d'email"""
        print(f"  üìß Sending email to {data['to']}")
        time.sleep(1)


# ============================================================
# EXEMPLE AVEC RETRY
# ============================================================

def example_reliable_queue():
    """Exemple avec reliable queue et retry"""
    redis_client = redis.Redis(decode_responses=True)
    queue = ReliableQueue(redis_client, queue_name='reliable_jobs')

    print("=" * 60)
    print("RELIABLE QUEUE WITH RETRY")
    print("=" * 60 + "\n")

    # Ajouter des jobs (dont un qui va fail)
    queue.enqueue({'type': 'send_email', 'to': 'user@example.com'})
    queue.enqueue({'type': 'fail_test', 'reason': 'Test retry'})
    queue.enqueue({'type': 'send_email', 'to': 'admin@example.com'})

    print(f"\nQueue size: {queue.size()}")

    # Worker traite les jobs
    worker = ReliableWorker(queue)

    # Traiter manuellement pour voir les retries
    for i in range(6):  # Le job failed sera retry 3 fois
        job = queue.dequeue(timeout=1)
        if job:
            success = worker.process_job(job)
            if success:
                queue.ack(job)
            else:
                queue.nack(job, requeue=True)
        else:
            print("No more jobs")
            break
        time.sleep(0.5)

    # V√©rifier la DLQ
    dlq_size = redis_client.llen(f"{queue.queue_name}:dlq")
    print(f"\nDead Letter Queue size: {dlq_size}")
```

---

## Pattern 3: Priority Queue

### Principe avec Sorted Set

```text
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    PRIORITY QUEUE (ZSET)                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Redis ZSET (Sorted Set):
- Members: job_id
- Score: priority (lower = higher priority)

Example:
ZADD queue:priority 1 job_urgent     (priority 1 = highest)
ZADD queue:priority 5 job_normal
ZADD queue:priority 10 job_low

queue:priority (sorted by score):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Score  ‚îÇ  Member                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   1    ‚îÇ  job_urgent     ‚Üê Pop  ‚îÇ
‚îÇ   5    ‚îÇ  job_normal            ‚îÇ
‚îÇ  10    ‚îÇ  job_low               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Operations:
ZADD queue:priority priority job_id  ‚Üí Add job
ZPOPMIN queue:priority              ‚Üí Get highest priority
ZRANGE queue:priority 0 -1          ‚Üí List all jobs
```

### Impl√©mentation Python

```python
class PriorityQueue:
    """
    Priority queue avec Redis Sorted Set
    """

    PRIORITY_HIGH = 1
    PRIORITY_NORMAL = 5
    PRIORITY_LOW = 10

    def __init__(self, redis_client, queue_name='priority_jobs'):
        self.redis = redis_client
        self.queue_name = f"queue:{queue_name}"
        self.data_prefix = f"{self.queue_name}:data:"

    def enqueue(self, job_data: Dict[str, Any], priority: int = PRIORITY_NORMAL) -> str:
        """
        Ajoute un job avec priorit√©

        Args:
            job_data: Donn√©es du job
            priority: Priorit√© (1 = haute, 10 = basse)
        """
        job_id = str(uuid.uuid4())

        job = {
            'id': job_id,
            'data': job_data,
            'priority': priority,
            'enqueued_at': time.time()
        }

        # Stocker les donn√©es du job
        job_key = f"{self.data_prefix}{job_id}"
        self.redis.set(job_key, json.dumps(job))
        self.redis.expire(job_key, 3600)  # 1 hour TTL

        # Ajouter √† la sorted set avec priorit√© comme score
        self.redis.zadd(self.queue_name, {job_id: priority})

        priority_name = {
            self.PRIORITY_HIGH: 'HIGH',
            self.PRIORITY_NORMAL: 'NORMAL',
            self.PRIORITY_LOW: 'LOW'
        }.get(priority, 'CUSTOM')

        print(f"‚úì Job enqueued: {job_id} (priority: {priority_name})")
        print(f"  Queue size: {self.size()}")

        return job_id

    def dequeue(self, timeout=0) -> Optional[Dict[str, Any]]:
        """
        R√©cup√®re le job avec la plus haute priorit√©
        """
        if timeout > 0:
            # BZPOPMIN = Blocking ZPOPMIN
            result = self.redis.bzpopmin(self.queue_name, timeout=timeout)
        else:
            # ZPOPMIN = Pop minimum score (highest priority)
            result = self.redis.zpopmin(self.queue_name, count=1)
            if result:
                result = (self.queue_name, result[0][0], result[0][1])
            else:
                return None

        if not result:
            return None

        _, job_id, priority = result

        # R√©cup√©rer les donn√©es du job
        job_key = f"{self.data_prefix}{job_id}"
        job_json = self.redis.get(job_key)

        if not job_json:
            print(f"‚ö†Ô∏è  Job data not found: {job_id}")
            return None

        job = json.loads(job_json)

        # Supprimer les donn√©es (job pris en charge)
        self.redis.delete(job_key)

        print(f"‚úì Job dequeued: {job_id} (priority: {priority})")

        return job

    def size(self) -> int:
        """Retourne la taille de la queue"""
        return self.redis.zcard(self.queue_name)

    def peek(self, count=10):
        """
        Affiche les N prochains jobs sans les retirer
        """
        jobs = self.redis.zrange(
            self.queue_name,
            0,
            count - 1,
            withscores=True
        )

        print(f"\nNext {len(jobs)} jobs:")
        for job_id, priority in jobs:
            print(f"  - {job_id} (priority: {priority})")


# ============================================================
# EXEMPLE D'UTILISATION
# ============================================================

def example_priority_queue():
    """Exemple avec priority queue"""
    redis_client = redis.Redis(decode_responses=True)
    queue = PriorityQueue(redis_client)

    print("=" * 60)
    print("PRIORITY QUEUE EXAMPLE")
    print("=" * 60 + "\n")

    # Ajouter des jobs avec diff√©rentes priorit√©s
    queue.enqueue(
        {'type': 'send_email', 'to': 'user1@example.com'},
        priority=PriorityQueue.PRIORITY_LOW
    )

    queue.enqueue(
        {'type': 'send_email', 'to': 'urgent@example.com'},
        priority=PriorityQueue.PRIORITY_HIGH
    )

    queue.enqueue(
        {'type': 'send_email', 'to': 'user2@example.com'},
        priority=PriorityQueue.PRIORITY_NORMAL
    )

    queue.enqueue(
        {'type': 'send_email', 'to': 'critical@example.com'},
        priority=PriorityQueue.PRIORITY_HIGH
    )

    # Voir l'ordre
    queue.peek(count=10)

    # Traiter dans l'ordre de priorit√©
    print("\nProcessing jobs in priority order:")
    while queue.size() > 0:
        job = queue.dequeue()
        if job:
            print(f"  Processing: {job['data']['to']}")
            time.sleep(0.5)
```

---

## Pattern 4: Delayed/Scheduled Jobs

### Principe avec Sorted Set + Timestamp

```text
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ               DELAYED JOBS (ZSET + Timestamp)                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Concept: Score = Unix timestamp when job should execute

Example:
Current time: 1670000000

ZADD queue:delayed 1670000060 job1  (execute in 60s)
ZADD queue:delayed 1670000120 job2  (execute in 120s)
ZADD queue:delayed 1670000030 job3  (execute in 30s)

queue:delayed (sorted by timestamp):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Timestamp  ‚îÇ  Job                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 1670000030 ‚îÇ  job3 ‚Üê Ready now!     ‚îÇ
‚îÇ 1670000060 ‚îÇ  job1                  ‚îÇ
‚îÇ 1670000120 ‚îÇ  job2                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Scheduler checks periodically:
ZRANGEBYSCORE queue:delayed -inf now
  ‚Üí Get jobs ready to execute
ZREM queue:delayed job3
  ‚Üí Remove from delayed queue
LPUSH queue:ready job3
  ‚Üí Add to ready queue
```

### Impl√©mentation Python

```python
class DelayedQueue:
    """
    Queue avec support de delayed/scheduled jobs
    """

    def __init__(self, redis_client, queue_name='delayed_jobs'):
        self.redis = redis_client
        self.delayed_queue = f"queue:{queue_name}:delayed"
        self.ready_queue = f"queue:{queue_name}:ready"
        self.data_prefix = f"queue:{queue_name}:data:"

    def enqueue(self, job_data: Dict[str, Any], delay_seconds: int = 0) -> str:
        """
        Ajoute un job avec d√©lai

        Args:
            job_data: Donn√©es du job
            delay_seconds: D√©lai avant ex√©cution (0 = imm√©diat)
        """
        job_id = str(uuid.uuid4())
        execute_at = time.time() + delay_seconds

        job = {
            'id': job_id,
            'data': job_data,
            'enqueued_at': time.time(),
            'execute_at': execute_at,
            'delay': delay_seconds
        }

        # Stocker les donn√©es
        job_key = f"{self.data_prefix}{job_id}"
        self.redis.set(job_key, json.dumps(job))
        self.redis.expire(job_key, delay_seconds + 3600)

        if delay_seconds > 0:
            # Ajouter √† la delayed queue
            self.redis.zadd(self.delayed_queue, {job_id: execute_at})
            print(f"‚úì Job scheduled: {job_id} (in {delay_seconds}s)")
        else:
            # Ajouter directement √† la ready queue
            self.redis.lpush(self.ready_queue, job_id)
            print(f"‚úì Job enqueued: {job_id} (immediate)")

        return job_id

    def dequeue(self, timeout=0) -> Optional[Dict[str, Any]]:
        """R√©cup√®re un job pr√™t"""
        if timeout > 0:
            result = self.redis.brpop(self.ready_queue, timeout=timeout)
        else:
            result = self.redis.rpop(self.ready_queue)
            if result:
                result = (self.ready_queue, result)

        if not result:
            return None

        _, job_id = result

        # R√©cup√©rer les donn√©es
        job_key = f"{self.data_prefix}{job_id}"
        job_json = self.redis.get(job_key)

        if not job_json:
            return None

        job = json.loads(job_json)
        self.redis.delete(job_key)

        return job

    def process_delayed_jobs(self) -> int:
        """
        D√©place les jobs pr√™ts de delayed ‚Üí ready
        √Ä appeler p√©riodiquement par un scheduler

        Returns:
            int: Nombre de jobs d√©plac√©s
        """
        now = time.time()

        # R√©cup√©rer tous les jobs pr√™ts (score <= now)
        ready_job_ids = self.redis.zrangebyscore(
            self.delayed_queue,
            '-inf',
            now
        )

        if not ready_job_ids:
            return 0

        moved = 0
        for job_id in ready_job_ids:
            # Supprimer de delayed queue
            removed = self.redis.zrem(self.delayed_queue, job_id)

            if removed:
                # Ajouter √† ready queue
                self.redis.lpush(self.ready_queue, job_id)
                moved += 1
                print(f"  ‚è∞ Job ready: {job_id}")

        if moved > 0:
            print(f"‚úì Moved {moved} delayed jobs to ready queue")

        return moved

    def start_scheduler(self, interval=1):
        """
        D√©marre le scheduler qui d√©place les jobs delayed ‚Üí ready

        Args:
            interval: Intervalle de v√©rification (secondes)
        """
        import threading

        def scheduler_loop():
            print(f"üïê Scheduler started (interval: {interval}s)")
            while True:
                try:
                    self.process_delayed_jobs()
                except Exception as e:
                    print(f"‚ö†Ô∏è  Scheduler error: {e}")
                time.sleep(interval)

        thread = threading.Thread(target=scheduler_loop, daemon=True)
        thread.start()
        return thread


# ============================================================
# EXEMPLE D'UTILISATION
# ============================================================

def example_delayed_queue():
    """Exemple avec delayed jobs"""
    redis_client = redis.Redis(decode_responses=True)
    queue = DelayedQueue(redis_client, queue_name='scheduled')

    print("=" * 60)
    print("DELAYED QUEUE EXAMPLE")
    print("=" * 60 + "\n")

    # Ajouter des jobs avec diff√©rents d√©lais
    queue.enqueue(
        {'type': 'send_reminder', 'to': 'user@example.com'},
        delay_seconds=5
    )

    queue.enqueue(
        {'type': 'send_email', 'to': 'immediate@example.com'},
        delay_seconds=0  # Imm√©diat
    )

    queue.enqueue(
        {'type': 'cleanup_temp_files'},
        delay_seconds=10
    )

    # D√©marrer le scheduler
    scheduler_thread = queue.start_scheduler(interval=1)

    # Worker traite les jobs pr√™ts
    print("\nWorker waiting for jobs...\n")

    for i in range(15):
        job = queue.dequeue(timeout=1)
        if job:
            print(f"‚úì Processing: {job['data']['type']}")
        else:
            print("  No jobs ready, waiting...")
        time.sleep(1)


# ============================================================
# CRON-LIKE SCHEDULING
# ============================================================

class CronQueue(DelayedQueue):
    """
    Queue avec support de scheduling type cron
    """

    def schedule_daily(self, job_data: Dict[str, Any], hour: int, minute: int = 0) -> str:
        """
        Schedule un job quotidien √† une heure donn√©e

        Args:
            hour: Heure (0-23)
            minute: Minute (0-59)
        """
        from datetime import datetime, timedelta

        now = datetime.now()
        scheduled_time = now.replace(hour=hour, minute=minute, second=0, microsecond=0)

        # Si l'heure est d√©j√† pass√©e aujourd'hui, programmer pour demain
        if scheduled_time <= now:
            scheduled_time += timedelta(days=1)

        delay_seconds = int((scheduled_time - now).total_seconds())

        print(f"Scheduling daily job for {scheduled_time.strftime('%H:%M')}")
        print(f"  First execution in {delay_seconds}s")

        return self.enqueue(job_data, delay_seconds=delay_seconds)

    def schedule_weekly(self, job_data: Dict[str, Any], day_of_week: int,
                       hour: int = 9, minute: int = 0) -> str:
        """
        Schedule un job hebdomadaire

        Args:
            day_of_week: Jour (0=Lundi, 6=Dimanche)
            hour: Heure
            minute: Minute
        """
        from datetime import datetime, timedelta

        now = datetime.now()
        days_ahead = day_of_week - now.weekday()

        if days_ahead <= 0:
            days_ahead += 7

        scheduled_time = now + timedelta(days=days_ahead)
        scheduled_time = scheduled_time.replace(
            hour=hour,
            minute=minute,
            second=0,
            microsecond=0
        )

        delay_seconds = int((scheduled_time - now).total_seconds())

        days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday',
                'Friday', 'Saturday', 'Sunday']

        print(f"Scheduling weekly job for {days[day_of_week]} at {hour:02d}:{minute:02d}")
        print(f"  First execution in {delay_seconds}s ({delay_seconds/3600:.1f} hours)")

        return self.enqueue(job_data, delay_seconds=delay_seconds)
```

---

## Impl√©mentation Node.js avec Bull/BullMQ

### Bull Queue (Production-Ready)

```javascript
const Queue = require('bull');
const Redis = require('ioredis');

// ============================================================
// BULL QUEUE SETUP
// ============================================================

// Cr√©er une connexion Redis
const redis = new Redis({
    host: 'localhost',
    port: 6379,
    maxRetriesPerRequest: null,
    enableReadyCheck: false
});

// Cr√©er une queue
const emailQueue = new Queue('email-jobs', {
    redis: {
        host: 'localhost',
        port: 6379
    },
    defaultJobOptions: {
        attempts: 3,              // Retry 3 fois
        backoff: {
            type: 'exponential',  // Backoff exponentiel
            delay: 2000           // D√©part √† 2s
        },
        removeOnComplete: true,   // Nettoyer les jobs compl√©t√©s
        removeOnFail: false       // Garder les jobs √©chou√©s
    }
});

// ============================================================
// PRODUCER
// ============================================================

async function addJobs() {
    console.log('='.repeat(60));
    console.log('ADDING JOBS TO BULL QUEUE');
    console.log('='.repeat(60) + '\n');

    // Job imm√©diat
    await emailQueue.add('send-email', {
        to: 'user@example.com',
        subject: 'Welcome!',
        body: 'Thanks for signing up'
    });

    // Job avec priorit√©
    await emailQueue.add('send-email', {
        to: 'urgent@example.com',
        subject: 'Urgent!',
        body: 'Important message'
    }, {
        priority: 1  // Plus haute priorit√©
    });

    // Job delayed (5 secondes)
    await emailQueue.add('send-email', {
        to: 'delayed@example.com',
        subject: 'Delayed',
        body: 'This arrives in 5 seconds'
    }, {
        delay: 5000  // 5 secondes
    });

    // Job r√©p√©t√© (cron-like)
    await emailQueue.add('daily-report', {
        type: 'report',
        recipients: ['admin@example.com']
    }, {
        repeat: {
            cron: '0 9 * * *',  // Tous les jours √† 9h
            tz: 'Europe/Paris'
        }
    });

    console.log('‚úì Jobs added\n');
}

// ============================================================
// CONSUMER / WORKER
// ============================================================

// Process email jobs
emailQueue.process('send-email', 5, async (job) => {
    // 5 = concurrency (5 jobs en parall√®le)

    console.log(`\nProcessing job ${job.id}`);
    console.log(`  Type: ${job.name}`);
    console.log(`  Data:`, job.data);
    console.log(`  Attempts: ${job.attemptsMade + 1}/${job.opts.attempts}`);

    // Simuler l'envoi d'email
    await sendEmail(job.data);

    // Mettre √† jour le progress
    await job.progress(50);

    // Simuler plus de traitement
    await new Promise(resolve => setTimeout(resolve, 1000));

    await job.progress(100);

    console.log(`  ‚úì Job ${job.id} completed`);

    // Retourner un r√©sultat
    return { sent: true, timestamp: Date.now() };
});

// Process daily reports
emailQueue.process('daily-report', async (job) => {
    console.log(`\nüìä Generating daily report...`);
    console.log(`  Recipients:`, job.data.recipients);

    // Simuler g√©n√©ration de rapport
    await new Promise(resolve => setTimeout(resolve, 2000));

    console.log(`  ‚úì Report sent`);

    return { reportGenerated: true };
});

async function sendEmail(data) {
    console.log(`  üìß Sending email to ${data.to}`);
    await new Promise(resolve => setTimeout(resolve, 500));

    // Simuler un √©chec al√©atoire (10% de chance)
    if (Math.random() < 0.1) {
        throw new Error('SMTP timeout');
    }

    console.log(`  ‚úì Email sent`);
}

// ============================================================
// EVENT LISTENERS
// ============================================================

emailQueue.on('completed', (job, result) => {
    console.log(`\n‚úÖ Job ${job.id} completed`);
    console.log(`   Result:`, result);
});

emailQueue.on('failed', (job, err) => {
    console.log(`\n‚ùå Job ${job.id} failed`);
    console.log(`   Error: ${err.message}`);
    console.log(`   Attempts: ${job.attemptsMade}/${job.opts.attempts}`);

    if (job.attemptsMade >= job.opts.attempts) {
        console.log(`   ‚ö†Ô∏è  Max attempts reached, moving to failed jobs`);
    }
});

emailQueue.on('progress', (job, progress) => {
    console.log(`üìä Job ${job.id} progress: ${progress}%`);
});

emailQueue.on('stalled', (job) => {
    console.log(`‚ö†Ô∏è  Job ${job.id} stalled (worker crashed?)`);
});

// ============================================================
// QUEUE MONITORING
// ============================================================

async function getQueueStats() {
    const counts = await emailQueue.getJobCounts();

    console.log('\n' + '='.repeat(60));
    console.log('QUEUE STATISTICS');
    console.log('='.repeat(60));
    console.log(`Waiting:    ${counts.waiting}`);
    console.log(`Active:     ${counts.active}`);
    console.log(`Completed:  ${counts.completed}`);
    console.log(`Failed:     ${counts.failed}`);
    console.log(`Delayed:    ${counts.delayed}`);
    console.log('='.repeat(60) + '\n');
}

// ============================================================
// ADMIN OPERATIONS
// ============================================================

async function retryFailedJobs() {
    console.log('üîÑ Retrying all failed jobs...');

    const failedJobs = await emailQueue.getFailed();
    console.log(`Found ${failedJobs.length} failed jobs`);

    for (const job of failedJobs) {
        await job.retry();
        console.log(`  ‚Üª Retrying job ${job.id}`);
    }
}

async function cleanOldJobs() {
    console.log('üßπ Cleaning old jobs...');

    // Nettoyer les jobs compl√©t√©s de plus de 24h
    await emailQueue.clean(24 * 3600 * 1000, 'completed');

    // Nettoyer les jobs √©chou√©s de plus de 7 jours
    await emailQueue.clean(7 * 24 * 3600 * 1000, 'failed');

    console.log('‚úì Cleanup completed');
}

async function pauseQueue() {
    await emailQueue.pause();
    console.log('‚è∏Ô∏è  Queue paused');
}

async function resumeQueue() {
    await emailQueue.resume();
    console.log('‚ñ∂Ô∏è  Queue resumed');
}

// ============================================================
// WEB DASHBOARD (Express + Bull Board)
// ============================================================

const express = require('express');
const { createBullBoard } = require('@bull-board/api');
const { BullAdapter } = require('@bull-board/api/bullAdapter');
const { ExpressAdapter } = require('@bull-board/express');

const app = express();

const serverAdapter = new ExpressAdapter();
serverAdapter.setBasePath('/admin/queues');

createBullBoard({
    queues: [new BullAdapter(emailQueue)],
    serverAdapter: serverAdapter,
});

app.use('/admin/queues', serverAdapter.getRouter());

// API endpoints
app.get('/api/queue/stats', async (req, res) => {
    const counts = await emailQueue.getJobCounts();
    res.json(counts);
});

app.post('/api/queue/pause', async (req, res) => {
    await emailQueue.pause();
    res.json({ success: true, message: 'Queue paused' });
});

app.post('/api/queue/resume', async (req, res) => {
    await emailQueue.resume();
    res.json({ success: true, message: 'Queue resumed' });
});

app.post('/api/queue/retry-failed', async (req, res) => {
    await retryFailedJobs();
    res.json({ success: true, message: 'Failed jobs retried' });
});

const PORT = 3000;
app.listen(PORT, () => {
    console.log(`\nüåê Dashboard: http://localhost:${PORT}/admin/queues`);
    console.log(`üìä API: http://localhost:${PORT}/api/queue/stats\n`);
});

// ============================================================
// MAIN
// ============================================================

async function main() {
    try {
        // Ajouter des jobs
        await addJobs();

        // Afficher les stats toutes les 5 secondes
        setInterval(async () => {
            await getQueueStats();
        }, 5000);

    } catch (error) {
        console.error('Error:', error);
    }
}

main();
```

---

## Monitoring et Observability

### M√©triques importantes

```python
class QueueMetrics:
    """
    Collecte de m√©triques pour monitoring
    """

    def __init__(self, redis_client, queue_name='jobs'):
        self.redis = redis_client
        self.queue_name = f"queue:{queue_name}"
        self.metrics_key = f"{self.queue_name}:metrics"

    def record_enqueue(self):
        """Enregistre un job ajout√©"""
        pipe = self.redis.pipeline()
        pipe.hincrby(self.metrics_key, 'total_enqueued', 1)
        pipe.hincrby(self.metrics_key, 'current_size', 1)
        pipe.execute()

    def record_dequeue(self):
        """Enregistre un job retir√©"""
        pipe = self.redis.pipeline()
        pipe.hincrby(self.metrics_key, 'total_dequeued', 1)
        pipe.hincrby(self.metrics_key, 'current_size', -1)
        pipe.execute()

    def record_success(self, duration_ms):
        """Enregistre un succ√®s"""
        pipe = self.redis.pipeline()
        pipe.hincrby(self.metrics_key, 'total_succeeded', 1)
        pipe.hincrbyfloat(self.metrics_key, 'total_duration_ms', duration_ms)
        pipe.execute()

    def record_failure(self):
        """Enregistre un √©chec"""
        self.redis.hincrby(self.metrics_key, 'total_failed', 1)

    def get_metrics(self) -> Dict[str, Any]:
        """R√©cup√®re toutes les m√©triques"""
        metrics = self.redis.hgetall(self.metrics_key)

        # Convertir en nombres
        for key in metrics:
            try:
                if '.' in metrics[key]:
                    metrics[key] = float(metrics[key])
                else:
                    metrics[key] = int(metrics[key])
            except:
                pass

        # Calculer des m√©triques d√©riv√©es
        if metrics.get('total_dequeued', 0) > 0:
            metrics['success_rate'] = (
                metrics.get('total_succeeded', 0) /
                metrics['total_dequeued'] * 100
            )

            metrics['avg_duration_ms'] = (
                metrics.get('total_duration_ms', 0) /
                metrics['total_succeeded']
            ) if metrics.get('total_succeeded', 0) > 0 else 0

        return metrics

    def display_metrics(self):
        """Affiche les m√©triques"""
        metrics = self.get_metrics()

        print("\n" + "=" * 60)
        print("QUEUE METRICS")
        print("=" * 60)
        print(f"Current size:     {metrics.get('current_size', 0)}")
        print(f"Total enqueued:   {metrics.get('total_enqueued', 0)}")
        print(f"Total dequeued:   {metrics.get('total_dequeued', 0)}")
        print(f"Total succeeded:  {metrics.get('total_succeeded', 0)}")
        print(f"Total failed:     {metrics.get('total_failed', 0)}")
        print(f"Success rate:     {metrics.get('success_rate', 0):.2f}%")
        print(f"Avg duration:     {metrics.get('avg_duration_ms', 0):.2f}ms")
        print("=" * 60 + "\n")


# ============================================================
# PROMETHEUS METRICS
# ============================================================

from prometheus_client import Counter, Histogram, Gauge

# D√©finir les m√©triques
queue_enqueued_total = Counter(
    'queue_enqueued_total',
    'Total jobs enqueued',
    ['queue_name']
)

queue_dequeued_total = Counter(
    'queue_dequeued_total',
    'Total jobs dequeued',
    ['queue_name']
)

queue_succeeded_total = Counter(
    'queue_succeeded_total',
    'Total jobs succeeded',
    ['queue_name']
)

queue_failed_total = Counter(
    'queue_failed_total',
    'Total jobs failed',
    ['queue_name']
)

queue_size = Gauge(
    'queue_size',
    'Current queue size',
    ['queue_name']
)

queue_processing_duration = Histogram(
    'queue_processing_duration_seconds',
    'Job processing duration',
    ['queue_name', 'job_type']
)


class MonitoredQueue(SimpleQueue):
    """Queue avec monitoring Prometheus"""

    def enqueue(self, job_data):
        job_id = super().enqueue(job_data)

        queue_enqueued_total.labels(queue_name=self.queue_name).inc()
        queue_size.labels(queue_name=self.queue_name).set(self.size())

        return job_id

    def dequeue(self, timeout=0):
        job = super().dequeue(timeout)

        if job:
            queue_dequeued_total.labels(queue_name=self.queue_name).inc()
            queue_size.labels(queue_name=self.queue_name).set(self.size())

        return job
```

---

## Bonnes pratiques

### Checklist de production

**Architecture :**
- ‚úÖ Utiliser Reliable Queue (BRPOPLPUSH) en production
- ‚úÖ Impl√©menter Dead Letter Queue pour jobs √©chou√©s
- ‚úÖ Retry logic avec exponential backoff
- ‚úÖ Priority queues pour jobs critiques
- ‚úÖ Delayed jobs pour scheduling

**Performance :**
- ‚úÖ Multiple workers pour scalabilit√©
- ‚úÖ Batch processing quand possible
- ‚úÖ Monitoring de la taille de la queue
- ‚úÖ Auto-scaling des workers selon la charge
- ‚úÖ Rate limiting sur les workers

**Fiabilit√© :**
- ‚úÖ Job timeout pour √©viter les workers bloqu√©s
- ‚úÖ Recovery des stale jobs au d√©marrage
- ‚úÖ Idempotence des jobs (safe de retry)
- ‚úÖ Logs d√©taill√©s pour debugging
- ‚úÖ Alertes si queue trop longue

**Monitoring :**
- ‚úÖ M√©triques : taille, throughput, latence, errors
- ‚úÖ Dashboard temps r√©el (Bull Board, Grafana)
- ‚úÖ Alerting sur anomalies
- ‚úÖ Distributed tracing pour jobs complexes

### Anti-patterns √† √©viter

```python
# ‚ùå BAD: Jobs non-idempotents
def process_payment(amount):
    charge_credit_card(amount)  # Appel√© 2x si retry!

# ‚úÖ GOOD: Jobs idempotents
def process_payment(payment_id):
    if not already_processed(payment_id):
        charge_credit_card(payment_id)
        mark_as_processed(payment_id)


# ‚ùå BAD: Pas de timeout
while True:
    job = queue.dequeue(timeout=None)  # Bloque ind√©finiment
    process(job)

# ‚úÖ GOOD: Timeout raisonnable
while True:
    job = queue.dequeue(timeout=5)
    if job:
        process(job)


# ‚ùå BAD: Pas de limite de retry
job['attempts'] = float('inf')  # Retry forever

# ‚úÖ GOOD: Limite de retry
job['max_attempts'] = 3
if job['attempts'] >= job['max_attempts']:
    move_to_dlq(job)


# ‚ùå BAD: Jobs trop gros
queue.enqueue({
    'image_data': base64_encoded_10mb_image  # 10MB dans Redis!
})

# ‚úÖ GOOD: R√©f√©rence aux donn√©es
queue.enqueue({
    'image_url': 's3://bucket/image.jpg'  # Juste l'URL
})
```

---

## Conclusion

Les **Job Queues** sont essentielles pour construire des applications scalables et r√©silientes. Les points cl√©s :

**Patterns impl√©ment√©s :**
1. **Simple Queue (LIST)** : Rapide mais non-fiable
2. **Reliable Queue (BRPOPLPUSH)** : Production-ready avec retry
3. **Priority Queue (ZSET)** : Jobs par ordre d'importance
4. **Delayed Queue (ZSET + timestamp)** : Scheduling et cron

**Biblioth√®ques recommand√©es :**
- **Python** : RQ (simple), Celery (complet), custom avec Redis
- **Node.js** : Bull/BullMQ (excellent, production-ready)

**Best Practices :**
- Toujours utiliser Reliable Queue en production
- Impl√©menter retry logic avec exponential backoff
- Dead Letter Queue pour jobs d√©finitivement √©chou√©s
- Monitoring obligatoire (m√©triques + alertes)
- Jobs idempotents (safe de retry)
- Multiple workers pour scalabilit√©

**Use Cases :**
- Background processing (emails, images, videos)
- Peak shaving (absorber les pics de charge)
- Scheduled tasks (cron-like)
- Microservices communication
- Long-running tasks

Redis est parfait pour les job queues : rapide, fiable, et simple √† utiliser !

---


‚è≠Ô∏è [Atomicit√© et programmabilit√©](/07-atomicite-programmabilite/README.md)
