ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 8.5 Redis Streams : XREAD vs XREADGROUP, Gestion des ACK

## Introduction

Redis Streams offre deux modÃ¨les de consommation fondamentalement diffÃ©rents : **XREAD** pour la lecture simple et **XREADGROUP** pour la lecture coordonnÃ©e avec consumer groups. Le choix entre les deux dÃ©termine les garanties de livraison, la gestion des Ã©checs, et la complexitÃ© de l'architecture.

### Vue d'ensemble

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      XREAD                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ â€¢ Lecture directe du stream                 â”‚        â”‚
â”‚  â”‚ â€¢ Pas de tracking                           â”‚        â”‚
â”‚  â”‚ â€¢ Pas d'ACK                                 â”‚        â”‚
â”‚  â”‚ â€¢ Multiple consumers = duplication          â”‚        â”‚
â”‚  â”‚ â€¢ Simple et performant                      â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   XREADGROUP                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ â€¢ Lecture via consumer group                â”‚        â”‚
â”‚  â”‚ â€¢ Tracking automatique (PEL)                â”‚        â”‚
â”‚  â”‚ â€¢ ACK obligatoires                          â”‚        â”‚
â”‚  â”‚ â€¢ Multiple consumers = distribution         â”‚        â”‚
â”‚  â”‚ â€¢ Fiable avec retry                         â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## XREAD : Lecture Simple

### CaractÃ©ristiques

```python
# XREAD : Lecture directe sans tracking
messages = redis.xread({'stream': last_id}, count=10)

# CaractÃ©ristiques :
âœ… SimplicitÃ© maximale
âœ… Performance optimale
âœ… Stateless (pas de state cÃ´tÃ© Redis)
âœ… RejouabilitÃ© totale

âŒ Pas de distribution automatique
âŒ Pas de tracking des messages traitÃ©s
âŒ Pas de retry automatique
âŒ Consumer doit gÃ©rer son state
```

### Cas d'usage XREAD

```python
import redis
import time
from typing import Optional

class SimpleStreamReader:
    """
    Lecteur simple avec XREAD
    IdÃ©al pour : logs, monitoring, debugging
    """

    def __init__(self, stream_name: str):
        self.redis = redis.Redis(host='localhost', port=6379, decode_responses=True)
        self.stream_name = stream_name
        self.last_id = '0-0'  # GÃ©rÃ© localement

    def read_from_beginning(self):
        """Lire tout le stream depuis le dÃ©but"""
        self.last_id = '0-0'
        return self.read_batch()

    def read_new_only(self):
        """Lire seulement les nouveaux messages"""
        self.last_id = '$'  # Position actuelle
        return self.read_batch()

    def read_batch(self, count: int = 100) -> list:
        """Lire un batch de messages"""
        messages = self.redis.xread(
            {self.stream_name: self.last_id},
            count=count
        )

        if messages:
            stream, entries = messages[0]

            # Mettre Ã  jour last_id pour prochain read
            if entries:
                self.last_id = entries[-1][0]

            return entries

        return []

    def tail_forever(self, callback):
        """Tail -f style : suivre le stream en continu"""
        print(f"Tailing {self.stream_name}...")

        while True:
            messages = self.redis.xread(
                {self.stream_name: self.last_id},
                count=10,
                block=1000  # Block 1 seconde
            )

            if messages:
                stream, entries = messages[0]

                for msg_id, data in entries:
                    callback(msg_id, data)
                    self.last_id = msg_id

# Exemple d'utilisation
reader = SimpleStreamReader('logs:app')

# Tail style
reader.read_new_only()
reader.tail_forever(lambda msg_id, data: print(f"{msg_id}: {data}"))
```

### Pattern : Single Consumer avec Checkpointing

```python
class CheckpointedReader:
    """
    XREAD avec checkpointing manuel pour fiabilitÃ©
    """

    def __init__(self, stream_name: str, consumer_id: str):
        self.redis = redis.Redis(host='localhost', port=6379, decode_responses=True)
        self.stream_name = stream_name
        self.consumer_id = consumer_id
        self.checkpoint_key = f"checkpoint:{stream_name}:{consumer_id}"

    def load_checkpoint(self) -> str:
        """Charger derniÃ¨re position sauvegardÃ©e"""
        checkpoint = self.redis.get(self.checkpoint_key)
        return checkpoint if checkpoint else '0-0'

    def save_checkpoint(self, message_id: str):
        """Sauvegarder position"""
        self.redis.set(self.checkpoint_key, message_id)

    def process_stream(self, handler):
        """Traiter stream avec checkpointing"""
        last_id = self.load_checkpoint()
        print(f"Starting from checkpoint: {last_id}")

        while True:
            messages = self.redis.xread(
                {self.stream_name: last_id},
                count=10,
                block=5000
            )

            if messages:
                stream, entries = messages[0]

                for msg_id, data in entries:
                    try:
                        # Traiter message
                        handler(msg_id, data)

                        # Sauvegarder checkpoint (commit)
                        self.save_checkpoint(msg_id)
                        last_id = msg_id

                    except Exception as e:
                        print(f"Error processing {msg_id}: {e}")
                        # Ne pas sauvegarder checkpoint = retry au restart
                        return

# Utilisation
reader = CheckpointedReader('orders:stream', 'processor-1')

def process_order(msg_id, data):
    print(f"Processing order: {data}")
    # Si exception â†’ pas de checkpoint â†’ rejouera au restart
    time.sleep(0.1)

reader.process_stream(process_order)
```

### Avantages et InconvÃ©nients XREAD

| Avantages | InconvÃ©nients |
|-----------|---------------|
| âœ… SimplicitÃ© maximale | âŒ Pas de coordination entre consumers |
| âœ… Performance optimale | âŒ Duplication si multiple readers |
| âœ… Stateless cÃ´tÃ© Redis | âŒ State management manuel |
| âœ… RejouabilitÃ© facile | âŒ Pas de retry automatique |
| âœ… Pas de cleanup nÃ©cessaire | âŒ Pas de load balancing |
| âœ… Debugging facile | âŒ Pas de dead letter queue |

---

## XREADGROUP : Lecture CoordonnÃ©e

### CaractÃ©ristiques

```python
# XREADGROUP : Lecture avec consumer group
messages = redis.xreadgroup(
    'mygroup',
    'consumer-1',
    {'stream': '>'},
    count=10
)

# CaractÃ©ristiques :
âœ… Distribution automatique
âœ… Tracking via PEL (Pending Entry List)
âœ… ACK obligatoires
âœ… Retry automatique
âœ… Load balancing natif
âœ… Dead letter queue possible

âŒ Plus complexe
âŒ State cÃ´tÃ© Redis
âŒ NÃ©cessite cleanup (XACK)
âš ï¸ Performance lÃ©gÃ¨rement infÃ©rieure
```

### Cas d'usage XREADGROUP

```python
class ConsumerGroupReader:
    """
    Lecteur avec consumer group
    IdÃ©al pour : job processing, order processing, events
    """

    def __init__(
        self,
        stream_name: str,
        group_name: str,
        consumer_name: str
    ):
        self.redis = redis.Redis(host='localhost', port=6379, decode_responses=True)
        self.stream_name = stream_name
        self.group_name = group_name
        self.consumer_name = consumer_name

    def read_new_messages(self, count: int = 10):
        """Lire nouveaux messages (non encore livrÃ©s au group)"""
        messages = self.redis.xreadgroup(
            self.group_name,
            self.consumer_name,
            {self.stream_name: '>'},  # > = nouveaux messages
            count=count,
            block=1000
        )

        if messages:
            return messages[0][1]  # Liste de (msg_id, data)

        return []

    def read_pending_messages(self, count: int = 10):
        """Lire messages pending de CE consumer"""
        messages = self.redis.xreadgroup(
            self.group_name,
            self.consumer_name,
            {self.stream_name: '0'},  # 0 = pending
            count=count
        )

        if messages:
            return messages[0][1]

        return []

    def acknowledge(self, message_id: str):
        """Acquitter un message traitÃ©"""
        return self.redis.xack(
            self.stream_name,
            self.group_name,
            message_id
        )

    def process_with_ack(self, handler):
        """Traiter avec ACK automatique"""
        while True:
            # 1. Lire nouveaux messages
            messages = self.read_new_messages(count=10)

            for msg_id, data in messages:
                try:
                    # Traiter
                    handler(msg_id, data)

                    # ACK seulement si succÃ¨s
                    self.acknowledge(msg_id)
                    print(f"âœ“ ACKed {msg_id}")

                except Exception as e:
                    print(f"âœ— Error processing {msg_id}: {e}")
                    # Pas d'ACK â†’ reste en pending

# Utilisation
reader = ConsumerGroupReader('orders:stream', 'processors', 'worker-1')

def process_order(msg_id, data):
    print(f"Processing: {data}")
    # Si exception â†’ pas d'ACK â†’ retry automatique

reader.process_with_ack(process_order)
```

### Avantages et InconvÃ©nients XREADGROUP

| Avantages | InconvÃ©nients |
|-----------|---------------|
| âœ… Distribution automatique | âŒ Plus complexe Ã  setup |
| âœ… Load balancing natif | âŒ State cÃ´tÃ© Redis (PEL) |
| âœ… Retry automatique | âŒ NÃ©cessite cleanup (XACK) |
| âœ… Tracking des Ã©checs | âš ï¸ Performance lÃ©gÃ¨rement moindre |
| âœ… Dead letter queue | âŒ Debugging plus complexe |
| âœ… Scalable horizontalement | âŒ CoÃ»t mÃ©moire du PEL |

---

## Gestion des ACK : Patterns et Garanties

### Les 3 Niveaux de Garantie

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. AT-MOST-ONCE (â‰¤ 1 delivery)                         â”‚
â”‚     â€¢ XREAD sans checkpoint                             â”‚
â”‚     â€¢ XREADGROUP avec NOACK                             â”‚
â”‚     â€¢ Perte possible, pas de duplication                â”‚
â”‚     â€¢ Performance maximale                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. AT-LEAST-ONCE (â‰¥ 1 delivery)                        â”‚
â”‚     â€¢ XREADGROUP avec ACK aprÃ¨s traitement              â”‚
â”‚     â€¢ Pas de perte, duplication possible                â”‚
â”‚     â€¢ Standard pour la plupart des cas                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. EXACTLY-ONCE (= 1 delivery)                         â”‚
â”‚     â€¢ XREADGROUP + Idempotency key                      â”‚
â”‚     â€¢ Ni perte ni duplication                           â”‚
â”‚     â€¢ ComplexitÃ© maximale                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Pattern 1 : At-Most-Once (Fire and Forget)

```python
class AtMostOnceProcessor:
    """
    Traitement at-most-once avec NOACK
    Message peut Ãªtre perdu mais jamais dupliquÃ©
    """

    def __init__(self, stream_name: str, group_name: str, consumer_name: str):
        self.redis = redis.Redis(host='localhost', port=6379, decode_responses=True)
        self.stream_name = stream_name
        self.group_name = group_name
        self.consumer_name = consumer_name

    def process_fire_and_forget(self, handler):
        """Traiter sans ACK (NOACK)"""
        while True:
            # NOACK = pas d'ajout Ã  PEL
            messages = self.redis.execute_command(
                'XREADGROUP',
                'GROUP', self.group_name, self.consumer_name,
                'COUNT', '10',
                'BLOCK', '1000',
                'NOACK',  # â† Pas de tracking!
                'STREAMS', self.stream_name, '>'
            )

            if messages and messages[0]:
                stream_data = messages[0][1]

                for msg in stream_data:
                    msg_id = msg[0]
                    fields = msg[1]
                    data = dict(zip(fields[::2], fields[1::2]))

                    try:
                        handler(msg_id, data)
                    except Exception as e:
                        # Message perdu si erreur!
                        print(f"âœ— Lost message {msg_id}: {e}")

# Utilisation
processor = AtMostOnceProcessor('metrics:stream', 'analytics', 'worker-1')

def process_metric(msg_id, data):
    print(f"Metric: {data}")
    # Si crash ici â†’ message perdu

processor.process_fire_and_forget(process_metric)
```

**Quand utiliser** :
- Metrics non-critiques
- Logs
- Best-effort notifications
- Performance critique > fiabilitÃ©

### Pattern 2 : At-Least-Once (Standard)

```python
class AtLeastOnceProcessor:
    """
    Traitement at-least-once
    Garantie de traitement, duplication possible
    """

    def __init__(self, stream_name: str, group_name: str, consumer_name: str):
        self.redis = redis.Redis(host='localhost', port=6379, decode_responses=True)
        self.stream_name = stream_name
        self.group_name = group_name
        self.consumer_name = consumer_name

    def process_at_least_once(self, handler):
        """
        Pattern standard :
        1. Lire message (ajoutÃ© Ã  PEL)
        2. Traiter
        3. ACK seulement si succÃ¨s
        """
        while True:
            messages = self.redis.xreadgroup(
                self.group_name,
                self.consumer_name,
                {self.stream_name: '>'},
                count=10,
                block=1000
            )

            if messages:
                for stream, entries in messages:
                    for msg_id, data in entries:
                        try:
                            # Traiter
                            handler(msg_id, data)

                            # ACK APRÃˆS traitement
                            self.redis.xack(
                                self.stream_name,
                                self.group_name,
                                msg_id
                            )
                            print(f"âœ“ Processed & ACKed {msg_id}")

                        except Exception as e:
                            print(f"âœ— Error {msg_id}: {e}")
                            # Pas d'ACK â†’ reste en PEL â†’ retry

# Utilisation
processor = AtLeastOnceProcessor('orders:stream', 'processors', 'worker-1')

def process_order(msg_id, data):
    print(f"Processing order: {data}")

    # Traitement NON-idempotent
    # Peut Ãªtre appelÃ© plusieurs fois pour mÃªme message!

    # Si crash AVANT ACK â†’ message retraitÃ©
    # Si crash APRÃˆS ACK â†’ message dÃ©finitivement traitÃ©

processor.process_at_least_once(process_order)
```

**Quand utiliser** :
- Cas d'usage standard (80% des cas)
- Order processing
- Payment processing (avec idempotency)
- Email sending (avec deduplication)

### Pattern 3 : Exactly-Once Semantic

```python
class ExactlyOnceProcessor:
    """
    Traitement exactly-once avec idempotency
    Ni perte ni duplication
    """

    def __init__(
        self,
        stream_name: str,
        group_name: str,
        consumer_name: str
    ):
        self.redis = redis.Redis(host='localhost', port=6379, decode_responses=True)
        self.stream_name = stream_name
        self.group_name = group_name
        self.consumer_name = consumer_name

    def process_exactly_once(self, handler):
        """
        Pattern exactly-once :
        1. Lire message
        2. VÃ©rifier idempotency key
        3. Traiter si pas dÃ©jÃ  fait
        4. Sauvegarder idempotency key + ACK atomiquement
        """
        while True:
            messages = self.redis.xreadgroup(
                self.group_name,
                self.consumer_name,
                {self.stream_name: '>'},
                count=10,
                block=1000
            )

            if messages:
                for stream, entries in messages:
                    for msg_id, data in entries:
                        idempotency_key = f"processed:{self.stream_name}:{msg_id}"

                        try:
                            # VÃ©rifier si dÃ©jÃ  traitÃ©
                            if self.redis.exists(idempotency_key):
                                print(f"âŠ— Already processed {msg_id}, skipping")
                                self.redis.xack(
                                    self.stream_name,
                                    self.group_name,
                                    msg_id
                                )
                                continue

                            # Traiter
                            result = handler(msg_id, data)

                            # Transaction atomique : save result + ACK
                            pipe = self.redis.pipeline()

                            # Marquer comme traitÃ© (TTL pour cleanup)
                            pipe.setex(
                                idempotency_key,
                                86400,  # 24h
                                '1'
                            )

                            # ACK
                            pipe.xack(
                                self.stream_name,
                                self.group_name,
                                msg_id
                            )

                            pipe.execute()

                            print(f"âœ“ Exactly-once processed {msg_id}")

                        except Exception as e:
                            print(f"âœ— Error {msg_id}: {e}")
                            # Pas d'idempotency key ni ACK â†’ retry

# Utilisation
processor = ExactlyOnceProcessor('payments:stream', 'processors', 'worker-1')

def process_payment(msg_id, data):
    print(f"Processing payment: {data}")

    # Traitement peut Ãªtre non-idempotent
    # GrÃ¢ce Ã  idempotency key, exÃ©cutÃ© une seule fois

    amount = float(data['amount'])
    # DÃ©biter compte, etc.

    return {'status': 'success', 'amount': amount}

processor.process_exactly_once(process_payment)
```

**Quand utiliser** :
- Payments / financial transactions
- Inventory management
- Account balance updates
- Tout systÃ¨me oÃ¹ duplication = problÃ¨me mÃ©tier

### Pattern 4 : ACK optimiste (avant traitement)

```python
class OptimisticAckProcessor:
    """
    ACK optimiste : ACK AVANT traitement
    âš ï¸ Dangereux : perte possible si crash
    """

    def __init__(self, stream_name: str, group_name: str, consumer_name: str):
        self.redis = redis.Redis(host='localhost', port=6379, decode_responses=True)
        self.stream_name = stream_name
        self.group_name = group_name
        self.consumer_name = consumer_name

    def process_optimistic(self, handler):
        """ACK immÃ©diatement, traiter aprÃ¨s"""
        while True:
            messages = self.redis.xreadgroup(
                self.group_name,
                self.consumer_name,
                {self.stream_name: '>'},
                count=10,
                block=1000
            )

            if messages:
                for stream, entries in messages:
                    for msg_id, data in entries:
                        # ACK IMMÃ‰DIATEMENT
                        self.redis.xack(
                            self.stream_name,
                            self.group_name,
                            msg_id
                        )

                        try:
                            # Traiter APRÃˆS ACK
                            handler(msg_id, data)
                            print(f"âœ“ Processed {msg_id}")

                        except Exception as e:
                            # âš ï¸ Message dÃ©jÃ  ACKÃ© = PERDU!
                            print(f"âœ— LOST message {msg_id}: {e}")

                            # Fallback : Ã©crire vers DLQ
                            self.save_to_dlq(msg_id, data, str(e))

    def save_to_dlq(self, msg_id, data, error):
        """Sauvegarder en DLQ pour investigation manuelle"""
        self.redis.xadd(
            f"{self.stream_name}:dlq",
            {
                'original_id': msg_id,
                'data': json.dumps(data),
                'error': error,
                'timestamp': str(time.time())
            }
        )

# âš ï¸ UTILISER SEULEMENT SI :
# - Performance critique
# - Perte acceptable
# - Avec fallback DLQ
```

**Quand utiliser** :
- Performance > fiabilitÃ©
- Messages non-critiques
- Avec DLQ comme safety net
- Jamais pour financial/critical data

---

## Comparaison DÃ©taillÃ©e

### Tableau de DÃ©cision Complet

| CritÃ¨re | XREAD | XREADGROUP |
|---------|-------|------------|
| **Setup** | Aucun | CrÃ©er group |
| **State management** | Manuel (app) | Automatique (Redis) |
| **Multiple consumers** | Duplication | Distribution |
| **Load balancing** | âŒ Manuel | âœ… Automatique |
| **ACK requis** | âŒ Non | âœ… Oui |
| **Retry automatique** | âŒ Non | âœ… Via PEL |
| **Dead messages** | âŒ Non | âœ… XPENDING |
| **RejouabilitÃ©** | âœ… Totale | âœ… Partielle |
| **Performance** | âœ…âœ… Optimale | âœ… Excellente |
| **Overhead mÃ©moire** | Minimal | PEL storage |
| **ComplexitÃ©** | TrÃ¨s simple | Moyenne |
| **Debugging** | Facile | Plus complexe |
| **Guarantees** | At-most-once | At-least-once |
| **Idempotence** | Pas requis | RecommandÃ© |

### Matrice de Choix

```python
def choose_consumption_model(requirements):
    """
    Aide Ã  choisir entre XREAD et XREADGROUP
    """

    # XREADGROUP obligatoire si :
    if (requirements['multiple_consumers'] and
        requirements['load_balancing']):
        return "XREADGROUP"

    if requirements['guaranteed_processing']:
        return "XREADGROUP"

    if requirements['retry_on_failure']:
        return "XREADGROUP"

    # XREAD suffit si :
    if (requirements['single_consumer'] and
        requirements['simple_processing']):
        return "XREAD"

    if requirements['read_only'] or requirements['debugging']:
        return "XREAD"

    if requirements['tail_logs']:
        return "XREAD"

    # Par dÃ©faut : XREADGROUP pour robustesse
    return "XREADGROUP"

# Exemples
print(choose_consumption_model({
    'multiple_consumers': True,
    'load_balancing': True,
    'guaranteed_processing': True,
    'retry_on_failure': True
}))
# â†’ "XREADGROUP"

print(choose_consumption_model({
    'single_consumer': True,
    'simple_processing': True,
    'read_only': True,
    'debugging': False
}))
# â†’ "XREAD"
```

---

## Patterns de Migration

### De XREAD vers XREADGROUP

```python
class MigrationHelper:
    """
    Helper pour migrer de XREAD vers XREADGROUP
    """

    def __init__(self, stream_name: str):
        self.redis = redis.Redis(host='localhost', port=6379, decode_responses=True)
        self.stream_name = stream_name

    def find_last_processed_id(self, checkpoint_key: str) -> str:
        """Trouver dernier message traitÃ© (depuis checkpoint XREAD)"""
        last_id = self.redis.get(checkpoint_key)
        return last_id if last_id else '0-0'

    def create_group_from_checkpoint(
        self,
        group_name: str,
        checkpoint_key: str
    ):
        """CrÃ©er group en continuant depuis checkpoint"""
        last_processed = self.find_last_processed_id(checkpoint_key)

        try:
            self.redis.xgroup_create(
                self.stream_name,
                group_name,
                last_processed,  # Partir d'oÃ¹ XREAD s'Ã©tait arrÃªtÃ©
                mkstream=True
            )
            print(f"âœ“ Created group from checkpoint {last_processed}")
        except redis.exceptions.ResponseError as e:
            if 'BUSYGROUP' in str(e):
                print(f"Group already exists")
            else:
                raise

    def dual_mode_processor(
        self,
        group_name: str,
        consumer_name: str,
        checkpoint_key: str,
        use_group: bool
    ):
        """
        Processor qui supporte les deux modes
        Permet migration progressive
        """
        if use_group:
            # Mode XREADGROUP
            messages = self.redis.xreadgroup(
                group_name,
                consumer_name,
                {self.stream_name: '>'},
                count=10,
                block=1000
            )

            if messages:
                for stream, entries in messages:
                    for msg_id, data in entries:
                        # Traiter
                        self.process_message(msg_id, data)

                        # ACK
                        self.redis.xack(self.stream_name, group_name, msg_id)

        else:
            # Mode XREAD
            last_id = self.find_last_processed_id(checkpoint_key)

            messages = self.redis.xread(
                {self.stream_name: last_id},
                count=10,
                block=1000
            )

            if messages:
                stream, entries = messages[0]

                for msg_id, data in entries:
                    # Traiter
                    self.process_message(msg_id, data)

                    # Checkpoint
                    self.redis.set(checkpoint_key, msg_id)

    def process_message(self, msg_id, data):
        """Logique de traitement (identique dans les 2 modes)"""
        print(f"Processing {msg_id}: {data}")

# Plan de migration
helper = MigrationHelper('orders:stream')

# Ã‰tape 1 : Mode XREAD actuel
# helper.dual_mode_processor('group', 'consumer-1', 'checkpoint:old', use_group=False)

# Ã‰tape 2 : CrÃ©er group depuis checkpoint
helper.create_group_from_checkpoint('processors', 'checkpoint:old')

# Ã‰tape 3 : Basculer vers XREADGROUP
# helper.dual_mode_processor('processors', 'consumer-1', 'checkpoint:old', use_group=True)
```

---

## Best Practices

### 1. Quand utiliser XREAD

```python
# âœ… BON : Lecture pour debugging
def tail_stream_for_debug(stream_name):
    reader = SimpleStreamReader(stream_name)
    reader.read_new_only()
    reader.tail_forever(lambda id, data: print(f"{id}: {data}"))

# âœ… BON : Single consumer, traitement simple
def process_logs(stream_name):
    reader = SimpleStreamReader(stream_name)
    while True:
        messages = reader.read_batch(count=100)
        for msg_id, data in messages:
            save_to_elasticsearch(data)

# âœ… BON : Fan-out avec plusieurs XREAD indÃ©pendants
def multiple_independent_readers(stream_name):
    # Reader 1 : Analytics
    analytics = SimpleStreamReader(stream_name)

    # Reader 2 : Archiving
    archiver = SimpleStreamReader(stream_name)

    # Les deux lisent TOUT le stream indÃ©pendamment
    # Pas de coordination nÃ©cessaire
```

### 2. Quand utiliser XREADGROUP

```python
# âœ… BON : Multiple workers, load balancing
def scalable_job_processing(stream_name, group_name):
    # Worker 1
    worker1 = ConsumerGroupReader(stream_name, group_name, 'worker-1')

    # Worker 2
    worker2 = ConsumerGroupReader(stream_name, group_name, 'worker-2')

    # Messages distribuÃ©s automatiquement

# âœ… BON : Guaranteed processing
def critical_order_processing(stream_name, group_name):
    processor = AtLeastOnceProcessor(stream_name, group_name, 'processor-1')
    processor.process_at_least_once(handle_order)

# âœ… BON : Retry automatique
def resilient_processing(stream_name, group_name):
    consumer = StreamConsumer(
        stream_name,
        group_name,
        'consumer-1',
        max_retries=3
    )
    consumer.start(handler)
```

### 3. Gestion des ACK

```python
class ACKBestPractices:
    """Best practices pour ACK"""

    def __init__(self, stream_name, group_name, consumer_name):
        self.redis = redis.Redis(host='localhost', port=6379, decode_responses=True)
        self.stream_name = stream_name
        self.group_name = group_name
        self.consumer_name = consumer_name

    def pattern_safe_ack(self, handler):
        """
        Pattern safe : ACK aprÃ¨s traitement complet
        """
        messages = self.redis.xreadgroup(
            self.group_name,
            self.consumer_name,
            {self.stream_name: '>'},
            count=10
        )

        if messages:
            for stream, entries in messages:
                for msg_id, data in entries:
                    try:
                        # 1. Traiter complÃ¨tement
                        handler(msg_id, data)

                        # 2. Persister rÃ©sultat si nÃ©cessaire
                        # save_result(msg_id, result)

                        # 3. ACK seulement si tout OK
                        self.redis.xack(
                            self.stream_name,
                            self.group_name,
                            msg_id
                        )

                    except Exception as e:
                        # Pas d'ACK = retry automatique
                        logger.error(f"Error {msg_id}: {e}")

    def pattern_batch_ack(self, handler):
        """
        Pattern batch : ACK plusieurs messages d'un coup
        """
        messages = self.redis.xreadgroup(
            self.group_name,
            self.consumer_name,
            {self.stream_name: '>'},
            count=100  # Batch plus grand
        )

        if messages:
            stream, entries = messages[0]

            processed_ids = []

            for msg_id, data in entries:
                try:
                    handler(msg_id, data)
                    processed_ids.append(msg_id)

                except Exception as e:
                    logger.error(f"Error {msg_id}: {e}")

            # ACK batch
            if processed_ids:
                self.redis.xack(
                    self.stream_name,
                    self.group_name,
                    *processed_ids
                )
                logger.info(f"Batch ACKed {len(processed_ids)} messages")

    def pattern_deferred_ack(self, handler):
        """
        Pattern deferred : ACK aprÃ¨s async operation
        """
        import asyncio

        messages = self.redis.xreadgroup(
            self.group_name,
            self.consumer_name,
            {self.stream_name: '>'},
            count=10
        )

        if messages:
            tasks = []

            for stream, entries in messages:
                for msg_id, data in entries:
                    # Lancer async
                    task = self._process_async(msg_id, data, handler)
                    tasks.append(task)

            # Attendre toutes les tasks
            results = asyncio.run(asyncio.gather(*tasks))

            # ACK celles qui ont rÃ©ussi
            for msg_id, success in results:
                if success:
                    self.redis.xack(
                        self.stream_name,
                        self.group_name,
                        msg_id
                    )

    async def _process_async(self, msg_id, data, handler):
        """Process async avec ACK diffÃ©rÃ©"""
        try:
            await handler(msg_id, data)
            return (msg_id, True)
        except Exception as e:
            logger.error(f"Error {msg_id}: {e}")
            return (msg_id, False)
```

### 4. Monitoring des ACK

```python
class ACKMonitor:
    """Monitoring de la santÃ© des ACK"""

    def __init__(self, stream_name: str, group_name: str):
        self.redis = redis.Redis(host='localhost', port=6379, decode_responses=True)
        self.stream_name = stream_name
        self.group_name = group_name

    def get_pending_stats(self) -> dict:
        """Statistiques des messages pending"""
        pending = self.redis.xpending(self.stream_name, self.group_name)

        if not pending or pending[0] == 0:
            return {'total_pending': 0}

        return {
            'total_pending': pending[0],
            'min_id': pending[1],
            'max_id': pending[2],
            'consumers': pending[3]
        }

    def get_unacked_messages(self, min_idle_ms: int = 60000) -> list:
        """Trouver messages non-ACKÃ©s depuis longtemps"""
        pending_details = self.redis.xpending_range(
            self.stream_name,
            self.group_name,
            '-',
            '+',
            count=100
        )

        unacked = []
        for entry in pending_details:
            if entry['time_since_delivered'] >= min_idle_ms:
                unacked.append({
                    'message_id': entry['message_id'],
                    'consumer': entry['consumer'],
                    'idle_ms': entry['time_since_delivered'],
                    'delivery_count': entry['times_delivered']
                })

        return unacked

    def alert_if_stuck(self, threshold: int = 100):
        """Alerter si trop de messages stuck"""
        stats = self.get_pending_stats()

        if stats['total_pending'] > threshold:
            print(f"âš ï¸  ALERT: {stats['total_pending']} pending messages")

            unacked = self.get_unacked_messages()
            print(f"   {len(unacked)} stuck messages (idle > 60s)")

            return True

        return False

    def print_health(self):
        """Afficher santÃ© globale"""
        stats = self.get_pending_stats()

        print(f"\n{'=' * 60}")
        print(f"ACK Health: {self.stream_name} / {self.group_name}")
        print(f"{'=' * 60}")
        print(f"Total pending: {stats.get('total_pending', 0)}")

        if stats.get('total_pending', 0) > 0:
            unacked = self.get_unacked_messages()
            print(f"Stuck messages (>60s): {len(unacked)}")

            if unacked:
                print(f"\nTop 5 stuck messages:")
                for msg in unacked[:5]:
                    print(f"  {msg['message_id']}:")
                    print(f"    Consumer: {msg['consumer']}")
                    print(f"    Idle: {msg['idle_ms']/1000:.1f}s")
                    print(f"    Attempts: {msg['delivery_count']}")

# Utilisation
monitor = ACKMonitor('orders:stream', 'processors')
monitor.print_health()

# Alerting pÃ©riodique
import schedule
schedule.every(1).minutes.do(monitor.alert_if_stuck)
```

---

## Cas d'Usage et Recommandations

### Matrice de DÃ©cision Finale

| Scenario | Recommandation | Garantie | Pourquoi |
|----------|---------------|----------|----------|
| **Logs aggregation** | XREAD | At-most-once | Simple, perte acceptable |
| **Metrics collection** | XREAD ou XREADGROUP+NOACK | At-most-once | Performance > fiabilitÃ© |
| **Job processing** | XREADGROUP | At-least-once | Distribution + retry |
| **Order processing** | XREADGROUP + idempotency | Exactly-once | Critical data |
| **Email sending** | XREADGROUP + dedup | At-least-once | Retry OK si dedupliquÃ© |
| **Payment processing** | XREADGROUP + idempotency | Exactly-once | Financial critical |
| **Analytics pipeline** | Multiple XREAD | N/A | Fan-out indÃ©pendant |
| **Debugging** | XREAD | N/A | Read-only, pas de side effects |

### Checklist de DÃ©cision

```python
def decision_checklist():
    """
    Checklist pour choisir pattern de consommation
    """

    questions = {
        'multiple_consumers': "Plusieurs consumers en parallÃ¨le ?",
        'load_balancing': "Besoin de load balancing automatique ?",
        'guaranteed_delivery': "Garantie de traitement requise ?",
        'retry_needed': "Retry automatique nÃ©cessaire ?",
        'idempotent': "Traitement idempotent ?",
        'critical_data': "DonnÃ©es critiques (finance, etc.) ?",
        'performance_critical': "Performance ultra-critique ?",
        'simple_use_case': "Cas d'usage simple (logs, debug) ?"
    }

    # Score
    use_xreadgroup = 0
    use_xread = 0

    # RÃ©pondre aux questions
    for key, question in questions.items():
        # answer = input(f"{question} (y/n): ").lower() == 'y'
        answer = True  # Exemple

        if key in ['multiple_consumers', 'load_balancing',
                   'guaranteed_delivery', 'retry_needed']:
            if answer:
                use_xreadgroup += 1

        if key in ['simple_use_case', 'performance_critical']:
            if answer:
                use_xread += 1

    # DÃ©cision
    if use_xreadgroup > use_xread:
        print("\nâœ“ Recommandation : XREADGROUP")

        # Niveau de garantie
        if questions['critical_data']:
            print("  â†’ Avec exactly-once (idempotency)")
        else:
            print("  â†’ Avec at-least-once (standard)")
    else:
        print("\nâœ“ Recommandation : XREAD")
        print("  â†’ Avec checkpointing si fiabilitÃ© importante")
```

---

## Conclusion

### RÃ¨gles d'Or

1. **Par dÃ©faut â†’ XREADGROUP**
   - Plus robuste, scalable, maintenance facile
   - Overhead acceptable pour la plupart des cas

2. **XREAD pour cas spÃ©cifiques**
   - Debugging, monitoring, logs
   - Single consumer avec traitement simple
   - Performance absolument critique

3. **Garanties de livraison**
   - At-most-once â†’ XREAD ou NOACK
   - At-least-once â†’ XREADGROUP standard
   - Exactly-once â†’ XREADGROUP + idempotency

4. **ACK Strategy**
   - ACK aprÃ¨s traitement complet (safe)
   - Batch ACK si performance critique
   - Never ACK avant traitement (sauf cas trÃ¨s spÃ©cifiques)

5. **Monitoring**
   - Toujours monitorer PEL size
   - Alerter sur messages stuck
   - Cleanup rÃ©gulier des dead consumers

### Points ClÃ©s

- ğŸ¯ **XREAD** = SimplicitÃ©, performance, stateless
- ğŸ¯ **XREADGROUP** = Distribution, fiabilitÃ©, retry
- ğŸ¯ **ACK** = Garanties de traitement
- ğŸ¯ **Idempotency** = Exactly-once semantic
- ğŸ¯ **PEL** = Ã‰tat des messages en attente

**En pratique** : 80% des cas utilisent XREADGROUP avec at-least-once, 15% utilisent XREAD, 5% nÃ©cessitent exactly-once.

---


â­ï¸ [Choix architectural : Pub/Sub vs Streams vs Lists vs Kafka](/08-communication-flux-donnees/06-choix-architectural-pubsub-streams-kafka.md)
