ðŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 11.1 Redis Cluster : Concepts (Sharding, Hash Slots, Gossip Protocol)

## Introduction

Redis Cluster reprÃ©sente la solution native de Redis pour le scaling horizontal et la haute disponibilitÃ© sans point de dÃ©faillance unique (Single Point of Failure - SPOF). Contrairement aux architectures traditionnelles basÃ©es sur un proxy central ou une configuration master-replica simple, Redis Cluster implÃ©mente une architecture dÃ©centralisÃ©e oÃ¹ chaque nÅ“ud est autonome et communique directement avec les autres via un protocole de gossip.

Cette section explore les trois piliers conceptuels qui fondent Redis Cluster :
1. **Le sharding** : Comment les donnÃ©es sont partitionnÃ©es
2. **Les hash slots** : Le mÃ©canisme de distribution dÃ©terministe
3. **Le protocole Gossip** : Comment les nÅ“uds maintiennent une vue cohÃ©rente du cluster

## Qu'est-ce que Redis Cluster ?

### DÃ©finition et objectifs

Redis Cluster est une implÃ©mentation distribuÃ©e de Redis qui permet :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Objectifs de Redis Cluster                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âœ“ Partitionnement automatique des donnÃ©es (Sharding)        â”‚
â”‚ âœ“ Scaling horizontal : Ajouter de la capacitÃ© en ajoutant   â”‚
â”‚   des nÅ“uds                                                 â”‚
â”‚ âœ“ Haute disponibilitÃ© : Failover automatique via replicas   â”‚
â”‚ âœ“ Performance linÃ©aire : O(n) avec n nÅ“uds                  â”‚
â”‚ âœ“ Architecture dÃ©centralisÃ©e : Pas de SPOF                  â”‚
â”‚ âœ“ TolÃ©rance aux pannes : Continue Ã  fonctionner avec        â”‚
â”‚   dÃ©faillance partielle                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Architecture conceptuelle

```
                    Redis Cluster Architecture

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Application Layer                      â”‚
â”‚              (Smart Client - Cluster Aware)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â–¼               â–¼               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Master A   â”‚ â”‚   Master B   â”‚ â”‚   Master C   â”‚
    â”‚              â”‚ â”‚              â”‚ â”‚              â”‚
    â”‚ Slots:       â”‚ â”‚ Slots:       â”‚ â”‚ Slots:       â”‚
    â”‚ 0-5460       â”‚ â”‚ 5461-10922   â”‚ â”‚ 10923-16383  â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                â”‚                â”‚
           â”‚ Replication    â”‚ Replication    â”‚ Replication
           â–¼                â–¼                â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Replica A1  â”‚ â”‚  Replica B1  â”‚ â”‚  Replica C1  â”‚
    â”‚              â”‚ â”‚              â”‚ â”‚              â”‚
    â”‚ (Failover)   â”‚ â”‚ (Failover)   â”‚ â”‚ (Failover)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º
              Gossip Protocol (Bus Cluster)
         Tous les nÅ“uds communiquent entre eux
```

### CaractÃ©ristiques fondamentales

**DÃ©centralisation totale :**
- Aucun coordinateur central
- Chaque nÅ“ud connaÃ®t l'Ã©tat complet du cluster
- DÃ©tection de pannes par consensus distribuÃ©
- Pas de proxy requis (client-side routing)

**Garanties de cohÃ©rence :**
- Pas de garantie de cohÃ©rence forte (Strong Consistency)
- CohÃ©rence Ã©ventuelle (Eventual Consistency)
- PossibilitÃ© de perte de donnÃ©es lors de partitionnement rÃ©seau
- Mode synchrone optionnel avec `WAIT` pour les Ã©critures critiques

## Le Sharding : Partitionnement horizontal des donnÃ©es

### Principe du sharding

Le sharding (ou partitionnement) consiste Ã  diviser un dataset en sous-ensembles distribuÃ©s sur plusieurs serveurs. Chaque serveur (ou nÅ“ud) ne stocke qu'une fraction des donnÃ©es totales.

```
                    DonnÃ©es complÃ¨tes (100%)
                            â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                   â”‚                   â”‚
        â–¼                   â–¼                   â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Shard 1â”‚          â”‚ Shard 2â”‚          â”‚ Shard 3â”‚
    â”‚  33%   â”‚          â”‚  33%   â”‚          â”‚  34%   â”‚
    â”‚ NÅ“ud A â”‚          â”‚ NÅ“ud B â”‚          â”‚ NÅ“ud C â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Avantages :
â”œâ”€ CapacitÃ© totale = Somme des capacitÃ©s individuelles
â”œâ”€ Performance = AgrÃ©gation des performances des nÅ“uds
â””â”€ ParallÃ©lisation des lectures/Ã©critures
```

### Types de sharding

Redis Cluster utilise un **sharding basÃ© sur une fonction de hachage** :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Types de Sharding (Comparaison)                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚ 1. RANGE SHARDING                                           â”‚
â”‚    user:0001-5000  â†’ Shard 1                                â”‚
â”‚    user:5001-10000 â†’ Shard 2                                â”‚
â”‚    â””â”€> ProblÃ¨me : Risque de hot spots (rÃ©partition inÃ©gale) â”‚
â”‚                                                             â”‚
â”‚ 2. HASH SHARDING (Redis Cluster) âœ“                          â”‚
â”‚    CRC16(key) mod 16384 â†’ Hash Slot â†’ Shard                 â”‚
â”‚    â””â”€> Avantage : Distribution uniforme et prÃ©visible       â”‚
â”‚                                                             â”‚
â”‚ 3. DIRECTORY-BASED SHARDING                                 â”‚
â”‚    Lookup table : key â†’ Shard                               â”‚
â”‚    â””â”€> ProblÃ¨me : Lookup table devient un bottleneck        â”‚
â”‚                                                             â”‚
â”‚ 4. GEO SHARDING                                             â”‚
â”‚    DonnÃ©es par rÃ©gion gÃ©ographique                          â”‚
â”‚    â””â”€> Cas spÃ©cifique, non implÃ©mentÃ© nativement            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Avantages du sharding dans Redis Cluster

**1. Scaling horizontal illimitÃ© (thÃ©orique)**
```
1 nÅ“ud   = 256 GB RAM max
10 nÅ“uds = 2.5 TB RAM disponible
100 nÅ“uds = 25 TB RAM disponible

Formule : CapacitÃ© totale = N Ã— CapacitÃ©_par_nÅ“ud
```

**2. Performance linÃ©aire**
```
Ops/sec = N Ã— Ops_par_nÅ“ud

Exemple :
1 nÅ“ud  â†’ 100,000 ops/sec
3 nÅ“uds â†’ 300,000 ops/sec
10 nÅ“uds â†’ 1,000,000 ops/sec

Note : En pratique, lÃ©gÃ¨re overhead due au routing et gossip
```

**3. Isolation des pannes**
```
Si un nÅ“ud tombe :
â””â”€> Seules les clÃ©s de ce nÅ“ud sont impactÃ©es
    (1/N des donnÃ©es avec N nÅ“uds)

â””â”€> Les autres nÅ“uds continuent Ã  servir leurs donnÃ©es

â””â”€> Si replica disponible : Failover automatique
```

## Hash Slots : Le mÃ©canisme de distribution

### Concept des hash slots

Redis Cluster divise l'espace de clÃ©s en **16384 hash slots** numÃ©rotÃ©s de 0 Ã  16383. Chaque clÃ© est assignÃ©e Ã  un slot via une fonction de hachage dÃ©terministe.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Architecture des Hash Slots                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

           Espace complet : 16384 slots (0-16383)
                              â”‚
                              â”‚ Fonction : CRC16(key) & 16383
                              â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                     â”‚                     â”‚
        â–¼                     â–¼                     â–¼
   Slot 0-5460          Slot 5461-10922       Slot 10923-16383
        â”‚                     â”‚                     â”‚
        â”‚                     â”‚                     â”‚
        â–¼                     â–¼                     â–¼
    Master A              Master B              Master C
    (NÅ“ud 1)             (NÅ“ud 2)              (NÅ“ud 3)
```

### Calcul du hash slot

```
Pour une clÃ© donnÃ©e, l'algorithme est :

HASH_SLOT = CRC16(key) mod 16384

Exemples :
â”€â”€â”€â”€â”€â”€â”€â”€â”€
user:1000        â†’ CRC16("user:1000") & 16383 = 5798  â†’ NÅ“ud A
session:abc123   â†’ CRC16("session:abc123") & 16383 = 12456 â†’ NÅ“ud C
product:9999     â†’ CRC16("product:9999") & 16383 = 7834 â†’ NÅ“ud B

PropriÃ©tÃ© importante : DÃ‰TERMINISTE
â””â”€> La mÃªme clÃ© sera toujours mappÃ©e au mÃªme slot
    peu importe le nÅ“ud qui effectue le calcul
```

### Distribution des slots aux nÅ“uds

Lors de la crÃ©ation d'un cluster, les 16384 slots sont distribuÃ©s Ã©quitablement :

```
Configuration initiale (3 masters) :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Master A : 192.168.1.10:6379
â”œâ”€ Slots assignÃ©s : 0-5460 (5461 slots)
â””â”€ Pourcentage : ~33.3%

Master B : 192.168.1.11:6379
â”œâ”€ Slots assignÃ©s : 5461-10922 (5462 slots)
â””â”€ Pourcentage : ~33.3%

Master C : 192.168.1.12:6379
â”œâ”€ Slots assignÃ©s : 10923-16383 (5461 slots)
â””â”€ Pourcentage : ~33.4%

Total : 16384 slots = 100% du keyspace
```

### Avantages des hash slots

**1. PrÃ©visibilitÃ©**
```
Contrairement au consistent hashing :
â””â”€> Pas de virtual nodes
â””â”€> Pas de redistribution massive lors d'ajout/suppression
â””â”€> Calcul instantanÃ© du slot d'une clÃ©
```

**2. GranularitÃ© fine pour le resharding**
```
16384 slots permettent :
â””â”€> DÃ©placement progressif lors d'ajout de nÅ“uds
â””â”€> Migration par petits lots (ex: 100 slots Ã  la fois)
â””â”€> ContrÃ´le prÃ©cis de la rÃ©partition
```

**3. Taille optimale pour le gossip**
```
16384 slots = 2048 octets (2 KB) en bitmap
â””â”€> Chaque nÅ“ud maintient un bitmap de tous les slots
â””â”€> Transmission rapide via le bus cluster
â””â”€> Faible overhead mÃ©moire
```

### Hash tags : ContrÃ´le de la localisation

Pour forcer plusieurs clÃ©s Ã  Ãªtre dans le mÃªme slot (nÃ©cessaire pour les transactions et opÃ©rations multi-clÃ©s) :

```
Syntaxe : {tag}

Exemples :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
{user:1000}:profile    â†’ Hash sur "user:1000"
{user:1000}:friends    â†’ Hash sur "user:1000"  â† MÃªme slot !
{user:1000}:sessions   â†’ Hash sur "user:1000"  â† MÃªme slot !

Cela permet :
â””â”€> MGET {user:1000}:profile {user:1000}:friends
â””â”€> MULTI/EXEC sur plusieurs clÃ©s liÃ©es
â””â”€> Scripts Lua accÃ©dant Ã  plusieurs clÃ©s
```

## Le Gossip Protocol : Communication dÃ©centralisÃ©e

### Principe du protocole Gossip

Le protocole Gossip (ou "epidemic protocol") est un mÃ©canisme de communication peer-to-peer oÃ¹ chaque nÅ“ud :
1. Maintient une vue de l'Ã©tat du cluster
2. Ã‰change rÃ©guliÃ¨rement des informations avec d'autres nÅ“uds alÃ©atoires
3. Propage les changements de maniÃ¨re virale

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Gossip Protocol Flow                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    NÅ“ud A                  NÅ“ud B                  NÅ“ud C
       â”‚                       â”‚                       â”‚
       â”‚                       â”‚                       â”‚
  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
  â”‚  Ã‰tat    â”‚            â”‚  Ã‰tat    â”‚            â”‚  Ã‰tat    â”‚
  â”‚ Cluster  â”‚            â”‚ Cluster  â”‚            â”‚ Cluster  â”‚
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
       â”‚                       â”‚                       â”‚
       â”‚  Heartbeat (PING)     â”‚                       â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚                       â”‚
       â”‚                       â”‚                       â”‚
       â”‚  Response (PONG)      â”‚                       â”‚
       â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                       â”‚
       â”‚                       â”‚                       â”‚
       â”‚                       â”‚  Heartbeat (PING)     â”‚
       â”‚                       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
       â”‚                       â”‚                       â”‚
       â”‚                       â”‚  Response (PONG)      â”‚
       â”‚                       â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
       â”‚                       â”‚                       â”‚
       â”‚  Update propagation   â”‚                       â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚                       â”‚
       â”‚                       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
       â”‚                       â”‚                       â”‚

Chaque nÅ“ud "gossipe" avec quelques nÅ“uds alÃ©atoires
â””â”€> Convergence exponentielle : O(log N) rounds
```

### Architecture du Bus Cluster

Redis Cluster utilise un second port (port + 10000) pour le bus cluster :

```
Configuration typique :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Port 6379 : Client connections (Redis Protocol)
â”œâ”€ GÃ¨re les requÃªtes clients
â”œâ”€ GET, SET, MGET, etc.
â””â”€ Communication synchrone

Port 16379 : Cluster Bus (Binary Protocol)
â”œâ”€ Gossip entre nÅ“uds
â”œâ”€ DÃ©tection de pannes
â”œâ”€ Resharding
â”œâ”€ Failover
â””â”€ Communication asynchrone


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         NÅ“ud Redis A (192.168.1.10)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                      â”‚
â”‚  Port 6379  â—„â”€â”€â”€ Clients (Applications)              â”‚
â”‚     â”‚                                                â”‚
â”‚     â””â”€â–º Redis Core (Data Operations)                 â”‚
â”‚                                                      â”‚
â”‚  Port 16379 â—„â”€â”€â”€ Cluster Bus                         â”‚
â”‚     â”‚                                                â”‚
â”‚     â””â”€â–º Gossip Handler                               â”‚
â”‚         â”œâ”€ Heartbeats                                â”‚
â”‚         â”œâ”€ Configuration sync                        â”‚
â”‚         â””â”€ Failure detection                         â”‚
â”‚                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Messages Ã©changÃ©s via Gossip

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Types de messages Gossip                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚ PING                                                        â”‚
â”‚ â””â”€> Heartbeat rÃ©gulier envoyÃ© Ã  un nÅ“ud alÃ©atoire           â”‚
â”‚     Contient : Node ID, slots, epoch, Ã©tat                  â”‚
â”‚                                                             â”‚
â”‚ PONG                                                        â”‚
â”‚ â””â”€> RÃ©ponse au PING                                         â”‚
â”‚     Contient : Confirmation de rÃ©ception, Ã©tat local        â”‚
â”‚                                                             â”‚
â”‚ MEET                                                        â”‚
â”‚ â””â”€> IntÃ©gration d'un nouveau nÅ“ud au cluster                â”‚
â”‚     EnvoyÃ© via : CLUSTER MEET <ip> <port>                   â”‚
â”‚                                                             â”‚
â”‚ FAIL                                                        â”‚
â”‚ â””â”€> DÃ©claration qu'un nÅ“ud est en panne                     â”‚
â”‚     PropagÃ© Ã  tous aprÃ¨s consensus                          â”‚
â”‚                                                             â”‚
â”‚ PUBLISH                                                     â”‚
â”‚ â””â”€> Propagation d'un message pub/sub                        â”‚
â”‚                                                             â”‚
â”‚ UPDATE                                                      â”‚
â”‚ â””â”€> Changement de configuration (resharding, failover)      â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### FrÃ©quence et temporisation du Gossip

```bash
# ParamÃ¨tres par dÃ©faut du protocole Gossip

cluster-node-timeout 15000    # 15 secondes
â”œâ”€> DÃ©lai avant de considÃ©rer un nÅ“ud comme potentiellement en panne
â””â”€> Les nÅ“uds envoient des PING toutes les (timeout / 2) = 7.5 sec

cluster-replica-validity-factor 10
â”œâ”€> Facteur de multiplication du timeout pour les replicas
â””â”€> Replica valide si dernier contact < timeout Ã— factor

cluster-migration-barrier 1
â”œâ”€> Nombre minimum de replicas Ã  garder avant migration
â””â”€> EmpÃªche un master de perdre toutes ses replicas

# Exemple de calcul :
# Cluster de 10 nÅ“uds, timeout = 15s
# Chaque nÅ“ud envoie ~1 PING toutes les 7.5s
# Total : 10 nÅ“uds Ã— 1 PING/7.5s = ~1.3 messages/sec
# Plus les PONG, MEET, UPDATE â†’ ~10-20 messages/sec pour le cluster
```

### DÃ©tection de pannes avec Gossip

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Processus de dÃ©tection de panne                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Ã‰tape 1 : PFAIL (Probable Fail)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Un nÅ“ud A ne reÃ§oit plus de rÃ©ponse du nÅ“ud C
â””â”€> Marque C comme PFAIL localement
    (n'affecte pas encore le cluster)

    NÅ“ud A: "C est peut-Ãªtre down"
            (opinion subjective)


Ã‰tape 2 : Propagation via Gossip
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
A informe les autres nÅ“uds de son opinion sur C

    A â†’ B : "Je pense que C est PFAIL"
    A â†’ D : "Je pense que C est PFAIL"

    B et D font leur propre vÃ©rification


Ã‰tape 3 : FAIL (Consensus atteint)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Si MAJORITÃ‰ des masters marquent C comme PFAIL :

    Condition : (N/2 + 1) masters marquent C comme PFAIL

    â””â”€> C est marquÃ© FAIL par tout le cluster
        (changement d'Ã©tat global)


Ã‰tape 4 : Propagation FAIL
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Le message FAIL est propagÃ© immÃ©diatement Ã  tous les nÅ“uds

    A â†’ BROADCAST : "C est FAIL (confirmÃ©)"

    â””â”€> Tous les nÅ“uds marquent C comme FAIL
        â””â”€> DÃ©clenchement du failover si C est master


Ã‰tape 5 : Failover (si master)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Si C Ã©tait un master avec replicas :

    Replica C1 s'Ã©lit comme nouveau master
    â”œâ”€ Prend possession des slots de C
    â”œâ”€ Propage le changement via Gossip
    â””â”€> Cluster opÃ©rationnel aprÃ¨s ~15-30 secondes
```

### Vue cohÃ©rente du cluster

Chaque nÅ“ud maintient une structure de donnÃ©es complÃ¨te du cluster :

```
Structure interne (simplifiÃ©e) :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

struct clusterNode {
    char name[40];              // Node ID (SHA1)
    char ip[46];                // Adresse IP
    int port;                   // Port client
    int cport;                  // Port cluster
    int flags;                  // MASTER, SLAVE, PFAIL, FAIL
    mstime_t ping_sent;         // Timestamp dernier PING
    mstime_t pong_received;     // Timestamp dernier PONG
    unsigned char slots[16384/8]; // Bitmap des slots (2KB)
    int numslots;               // Nombre de slots
    struct clusterNode *slaveof; // Pointeur vers master (si replica)
    int numslaves;              // Nombre de replicas (si master)
    struct clusterNode **slaves; // Liste des replicas
    ...
};

Configuration complÃ¨te du cluster :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
struct clusterState {
    clusterNode *nodes[CLUSTER_SLOTS]; // Mapping slot â†’ node
    clusterNode *myself;                // Ce nÅ“ud
    uint64_t currentEpoch;              // Epoch actuel
    dict *nodes_black_list;             // NÅ“uds exclus temporairement
    ...
};
```

### Convergence et cohÃ©rence Ã©ventuelle

```
PropriÃ©tÃ©s du Gossip dans Redis Cluster :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

âœ“ Convergence rapide : O(log N) rounds
â”œâ”€> Avec 10 nÅ“uds : ~3-4 rounds
â”œâ”€> Avec 100 nÅ“uds : ~7-8 rounds
â””â”€> Avec 1000 nÅ“uds : ~10-11 rounds

âœ“ RÃ©silience aux partitions
â”œâ”€> Le cluster continue Ã  fonctionner si majoritÃ© accessible
â””â”€> DÃ©tection automatique du split-brain

âœ— Pas de cohÃ©rence forte
â”œâ”€> FenÃªtre de propagation (quelques secondes)
â”œâ”€> PossibilitÃ© de vues divergentes temporaires
â””â”€> RÃ©solution via epoch (vecteur d'horloge)

âœ“ Overhead rÃ©seau faible
â”œâ”€> Messages compacts (2KB par heartbeat)
â”œâ”€> FrÃ©quence contrÃ´lÃ©e (timeout/2)
â””â”€> Pas de broadcast (seulement gossip)
```

## Interaction entre Sharding, Slots et Gossip

### ScÃ©nario complet : Ajout d'un nÅ“ud

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Ajout d'un nÅ“ud : Orchestration complÃ¨te                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Ã‰tape 1 : MEET (Gossip)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
redis-cli --cluster add-node 192.168.1.13:6379 192.168.1.10:6379

â””â”€> NÅ“ud A envoie MEET Ã  NÅ“ud D
    â””â”€> D rejoint le cluster
        â””â”€> Propagation via Gossip : tous les nÅ“uds connaissent D

Ã‰tat : D est dans le cluster mais n'a aucun slot


Ã‰tape 2 : Resharding (Slots)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
redis-cli --cluster reshard 192.168.1.10:6379

â””â”€> DÃ©placement de slots de A, B, C vers D
    â”œâ”€ A transfert slots 0-1365 Ã  D
    â”œâ”€ B transfert slots 5461-6826 Ã  D
    â””â”€ C transfert slots 10923-12288 Ã  D

Ã‰tat : D possÃ¨de maintenant ~4096 slots (25% du keyspace)


Ã‰tape 3 : Propagation (Gossip)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â””â”€> Chaque changement de slot ownership est propagÃ©
    â”œâ”€> A â†’ Gossip : "J'ai transfÃ©rÃ© slots 0-1365 Ã  D"
    â”œâ”€> B â†’ Gossip : "J'ai transfÃ©rÃ© slots 5461-6826 Ã  D"
    â””â”€> C â†’ Gossip : "J'ai transfÃ©rÃ© slots 10923-12288 Ã  D"

â””â”€> Tous les nÅ“uds mettent Ã  jour leur table de routing

Ã‰tat : Cluster cohÃ©rent, D est opÃ©rationnel


Ã‰tape 4 : RÃ©plication (Slots + Gossip)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
redis-cli --cluster add-node 192.168.1.14:6379 192.168.1.10:6379 \
    --cluster-slave --cluster-master-id <D-node-id>

â””â”€> E rejoint comme replica de D
    â””â”€> Gossip propage : "E est replica de D"
        â””â”€> E rÃ©plique les slots de D en arriÃ¨re-plan

Ã‰tat final : D (master avec 4096 slots) + E (replica)
```

### ScÃ©nario : RequÃªte client avec redirection

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Client Request Flow avec Slot Routing               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Client exÃ©cute : GET user:5000

Ã‰tape 1 : Calcul du slot (Client-side ou Server-side)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CRC16("user:5000") & 16383 = 8754

Ã‰tape 2 : Client contacte un nÅ“ud alÃ©atoire (ex: NÅ“ud A)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Client â†’ NÅ“ud A : GET user:5000

Ã‰tape 3 : NÅ“ud A vÃ©rifie s'il possÃ¨de le slot 8754
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
NÅ“ud A consulte sa table :
â”œâ”€ Slot 8754 appartient Ã  NÅ“ud B
â””â”€> NÅ“ud A ne peut pas servir cette requÃªte

Ã‰tape 4 : Redirection -MOVED
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
NÅ“ud A â†’ Client : -MOVED 8754 192.168.1.11:6379

Ã‰tape 5 : Client suit la redirection
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Client â†’ NÅ“ud B : GET user:5000
NÅ“ud B â†’ Client : "John Doe"

Ã‰tape 6 : Client met en cache le mapping
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Client mÃ©morise : Slot 8754 â†’ NÅ“ud B
â””â”€> Prochaine requÃªte sur user:5000 ira directement Ã  B


Optimisation (Smart Client) :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Un client intelligent tÃ©lÃ©charge la table complÃ¨te de slots :

CLUSTER SLOTS â†’ RÃ©cupÃ¨re tous les mappings
â””â”€> Client route directement vers le bon nÅ“ud
    â””â”€> Pas de redirection = latence rÃ©duite
```

### ScÃ©nario : Failover automatique

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Automatic Failover Process                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Ã‰tat initial :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Master B (slots 5461-10922) + Replica B1

Timeline :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

T0 : Master B crash
     â”‚
     â”‚  Gossip: Tous les nÅ“uds envoient PING Ã  B
     â–¼

T0+7.5s : Premiers timeouts
          â”‚
          â”‚  Plusieurs nÅ“uds marquent B comme PFAIL
          â–¼

T0+15s : Consensus PFAIL â†’ FAIL
         â”‚
         â”‚  MajoritÃ© des masters confirment : B est FAIL
         â”‚  Message FAIL propagÃ© instantanÃ©ment
         â–¼

T0+16s : Replica B1 dÃ©tecte la panne du master
         â”‚
         â”‚  B1 dÃ©marre l'Ã©lection de failover
         â”‚  B1 : "Je candidate pour devenir master"
         â–¼

T0+17s : Ã‰lection
         â”‚
         â”‚  B1 demande des votes aux autres masters
         â”‚  Condition : MajoritÃ© des masters vote pour B1
         â–¼

T0+18s : B1 Ã©lu nouveau master
         â”‚
         â”‚  B1 exÃ©cute CLUSTER FAILOVER
         â”‚  â”œâ”€ Prend possession des slots 5461-10922
         â”‚  â”œâ”€ Se dÃ©clare master (epoch++)
         â”‚  â””â”€> Propage via Gossip
         â–¼

T0+20s : Cluster stabilisÃ©
         â”‚
         â”‚  Tous les nÅ“uds savent :
         â”‚  â”œâ”€ B est FAIL
         â”‚  â”œâ”€ B1 est le nouveau master
         â”‚  â””â”€ Slots 5461-10922 â†’ B1
         â”‚
         â”‚  Clients sont redirigÃ©s vers B1
         â–¼

RÃ©sultat : Downtime de ~15-20 secondes pour les slots de B
```

## Architecture dÃ©centralisÃ©e vs centralisÃ©e

### Comparaison avec architectures alternatives

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Redis Cluster vs Proxy-based Architecture           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚ PROXY-BASED (ex: Twemproxy, Codis)                          â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                    â”‚
â”‚                                                             â”‚
â”‚     Client â†’ Proxy â†’ Redis Instances                        â”‚
â”‚              â”‚                                              â”‚
â”‚              â””â”€> SPOF + Bottleneck                          â”‚
â”‚                                                             â”‚
â”‚  âœ“ Simple Ã  configurer                                      â”‚
â”‚  âœ— Proxy = point de dÃ©faillance unique                      â”‚
â”‚  âœ— Latence ajoutÃ©e (hop supplÃ©mentaire)                     â”‚
â”‚  âœ— Scaling limitÃ© par la capacitÃ© du proxy                  â”‚
â”‚  âœ— Pas de failover automatique natif                        â”‚
â”‚                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚ REDIS CLUSTER (DÃ©centralisÃ©)                                â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                             â”‚
â”‚                                                             â”‚
â”‚     Client â†’ NÅ“ud Redis (routing intelligent)               â”‚
â”‚              â”‚                                              â”‚
â”‚              â””â”€> Tous les nÅ“uds sont Ã©gaux                  â”‚
â”‚                                                             â”‚
â”‚  âœ“ Pas de SPOF                                              â”‚
â”‚  âœ“ Latence minimale (1 hop si client intelligent)           â”‚
â”‚  âœ“ Scaling linÃ©aire                                         â”‚
â”‚  âœ“ Failover automatique intÃ©grÃ©                             â”‚
â”‚  âœ— Plus complexe (client doit Ãªtre cluster-aware)           â”‚
â”‚  âœ— Overhead du protocole Gossip                             â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Garanties et limitations

### Garanties offertes par Redis Cluster

```
1. DISPONIBILITÃ‰
   â”œâ”€ Le cluster reste opÃ©rationnel si majoritÃ© des masters accessibles
   â”œâ”€ Failover automatique en cas de panne d'un master (si replica existe)
   â””â”€ Temps de basculement : ~15-30 secondes

2. PERFORMANCE
   â”œâ”€ Scaling quasi-linÃ©aire en lecture et Ã©criture
   â”œâ”€ ParallÃ©lisation des opÃ©rations sur diffÃ©rents slots
   â””â”€ O(1) pour la plupart des opÃ©rations (GET, SET, HGET, etc.)

3. TOLÃ‰RANCE AUX PANNES
   â”œâ”€ RÃ©siste Ã  la panne de (N/2 - 1) masters sans perte de donnÃ©es
   â”œâ”€ DÃ©tection automatique des pannes via consensus Gossip
   â””â”€ RÃ©cupÃ©ration automatique aprÃ¨s rÃ©paration

4. CONSISTANCE (limitÃ©e)
   â”œâ”€ CohÃ©rence Ã©ventuelle (Eventual Consistency)
   â”œâ”€ Pas de transactions distribuÃ©es (sauf si clÃ©s sur mÃªme slot)
   â””â”€ Option WAIT pour attendre rÃ©plication synchrone
```

### Limitations importantes

```
1. OPÃ‰RATIONS MULTI-CLÃ‰S
   â””â”€> LimitÃ©es aux clÃ©s du mÃªme slot (hash tag requis)
       Exemples non supportÃ©s :
       â”œâ”€ MGET user:1 user:2 user:3  (si slots diffÃ©rents)
       â”œâ”€ SUNION set:a set:b set:c   (si slots diffÃ©rents)
       â””â”€ Transactions sur clÃ©s de slots diffÃ©rents

2. DATABASE UNIQUE
   â””â”€> Pas de support de SELECT (toujours DB 0)
   â””â”€> Incompatible avec applications multi-tenant via DB

3. SCRIPTS LUA
   â””â”€> Les clÃ©s accessibles doivent Ãªtre passÃ©es en argument
   â””â”€> Toutes les clÃ©s doivent Ãªtre dans le mÃªme slot

4. PUB/SUB
   â””â”€> Les messages sont broadcast Ã  tous les nÅ“uds
   â””â”€> Pas de sharding des channels (overhead rÃ©seau)
   â””â”€> Solution : Sharded Pub/Sub (Redis 7+)

5. COHÃ‰RENCE
   â””â”€> Pas de cohÃ©rence forte garantie
   â””â”€> PossibilitÃ© de perte de donnÃ©es lors de partitionnement
   â””â”€> FenÃªtre de vulnÃ©rabilitÃ© pendant le failover
```

## ProcÃ©dures de maintenance courantes

### VÃ©rification de l'Ã©tat du cluster

```bash
# Commande de base pour vÃ©rifier la santÃ© du cluster
redis-cli --cluster check 192.168.1.10:6379

# Sortie attendue :
192.168.1.10:6379 (a1b2c3d4...) -> 10240 keys | 5461 slots | 1 slaves.
192.168.1.11:6379 (e5f6g7h8...) -> 10280 keys | 5462 slots | 1 slaves.
192.168.1.12:6379 (i9j0k1l2...) -> 10244 keys | 5461 slots | 1 slaves.
[OK] All 16384 slots covered.

# Obtenir des informations dÃ©taillÃ©es
redis-cli --cluster info 192.168.1.10:6379

# VÃ©rifier la propagation du Gossip
redis-cli -h 192.168.1.10 -p 6379 CLUSTER NODES
```

### Monitoring du protocole Gossip

```bash
# Voir les statistiques Gossip
redis-cli -h 192.168.1.10 -p 6379 CLUSTER INFO

# MÃ©triques importantes :
cluster_stats_messages_sent:1234567
cluster_stats_messages_received:1234560
cluster_stats_messages_ping_sent:45000
cluster_stats_messages_pong_sent:45000
cluster_stats_messages_meet_sent:3
cluster_stats_messages_fail_sent:0

# Analyser les latences Gossip
redis-cli -h 192.168.1.10 -p 6379 --latency-history

# Identifier les nÅ“uds lents dans le Gossip
redis-cli -h 192.168.1.10 -p 6379 CLUSTER NODES | grep -v connected
```

### Ajustement des paramÃ¨tres Gossip

```bash
# Modifier le timeout de dÃ©tection (redis.conf)
cluster-node-timeout 15000

# RÃ©duire pour failover plus rapide (attention aux faux positifs) :
cluster-node-timeout 5000

# Augmenter pour Ã©viter les faux positifs sur rÃ©seau lent :
cluster-node-timeout 30000

# Appliquer dynamiquement (sans redÃ©marrage)
redis-cli CONFIG SET cluster-node-timeout 20000

# VÃ©rifier la configuration actuelle
redis-cli CONFIG GET cluster-node-timeout
```

### Diagnostic des problÃ¨mes de sharding

```bash
# Identifier les hot spots (slots surchargÃ©s)
for i in {0..16383}; do
    count=$(redis-cli -h 192.168.1.10 CLUSTER COUNTKEYSINSLOT $i)
    echo "$i:$count"
done | sort -t: -k2 -n -r | head -20

# VÃ©rifier la rÃ©partition des slots
redis-cli --cluster check 192.168.1.10:6379 | grep "slots"

# RÃ©Ã©quilibrer si nÃ©cessaire
redis-cli --cluster rebalance 192.168.1.10:6379 \
    --cluster-threshold 5 \
    --cluster-use-empty-masters
```

### ProcÃ©dure de resharding planifiÃ©

```bash
# 1. Analyser l'Ã©tat actuel
redis-cli --cluster check 192.168.1.10:6379

# 2. Calculer la nouvelle distribution souhaitÃ©e
# Exemple : 4 nÅ“uds â†’ 4096 slots chacun

# 3. ExÃ©cuter le resharding avec confirmation
redis-cli --cluster reshard 192.168.1.10:6379 \
    --cluster-from a1b2c3d4-source \
    --cluster-to m4n5o6p7-target \
    --cluster-slots 500 \
    --cluster-yes \
    --cluster-timeout 60000 \
    --cluster-pipeline 10

# 4. Monitorer la progression
watch -n 5 'redis-cli --cluster check 192.168.1.10:6379'

# 5. VÃ©rifier l'intÃ©gritÃ© post-resharding
redis-cli --cluster fix 192.168.1.10:6379
```

### RÃ©cupÃ©ration aprÃ¨s panne de Gossip

```bash
# ScÃ©nario : Cluster en Ã©tat "fail" suite Ã  problÃ¨me Gossip

# 1. Identifier les nÅ“uds problÃ©matiques
redis-cli -h 192.168.1.10 CLUSTER NODES | grep fail

# 2. Tenter une rÃ©cupÃ©ration automatique
redis-cli --cluster fix 192.168.1.10:6379

# 3. Si Ã©chec, reset d'un nÅ“ud (ATTENTION : DESTRUCTIF)
redis-cli -h 192.168.1.11 CLUSTER RESET SOFT

# 4. RÃ©intÃ©grer le nÅ“ud
redis-cli -h 192.168.1.10 CLUSTER MEET 192.168.1.11 6379

# 5. RÃ©assigner les slots si nÃ©cessaire
redis-cli --cluster reshard 192.168.1.10:6379
```

## Bonnes pratiques opÃ©rationnelles

### Configuration rÃ©seau optimale

```bash
# Firewall rules pour Cluster
# Autoriser ports client (6379) et cluster bus (16379)

iptables -A INPUT -p tcp --dport 6379 -j ACCEPT
iptables -A INPUT -p tcp --dport 16379 -j ACCEPT

# S'assurer que le bus cluster est accessible entre tous les nÅ“uds
# Test de connectivitÃ© :
nc -zv 192.168.1.11 16379

# Latence rÃ©seau acceptable : < 1ms (LAN), < 10ms (WAN)
ping -c 10 192.168.1.11
```

### Dimensionnement du cluster

```
RÃ¨gles de dimensionnement :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Nombre de nÅ“uds :
â”œâ”€ Minimum : 3 masters (pour avoir majoritÃ© = 2)
â”œâ”€ RecommandÃ© : 6 nÅ“uds (3 masters + 3 replicas)
â”œâ”€ Maximum pratique : ~1000 nÅ“uds
â””â”€> Au-delÃ , overhead Gossip devient significatif

Nombre de slots par nÅ“ud :
â”œâ”€ IdÃ©al : ~2000-5000 slots par nÅ“ud
â”œâ”€ Minimum : ~500 slots (granularitÃ© suffisante)
â””â”€> 16384 / N nÅ“uds = slots par nÅ“ud

MÃ©moire par nÅ“ud :
â”œâ”€ Standard : 8-64 GB par nÅ“ud
â”œâ”€ Large : 64-256 GB (attention Ã  la fragmentation)
â””â”€> Ã‰viter >256GB (temps de fork Ã©levÃ©)

Replicas :
â”œâ”€ Minimum : 1 replica par master
â”œâ”€ RecommandÃ© : 1-2 replicas par master
â””â”€> Plus de 3 replicas rarement utile
```

### Checklist de mise en production

```
âœ… AVANT LE DÃ‰PLOIEMENT
   â”œâ”€ VÃ©rifier que tous les nÅ“uds peuvent communiquer (ports 6379 + 16379)
   â”œâ”€ Configurer cluster-node-timeout appropriÃ© (15000-30000 ms)
   â”œâ”€ Activer persistence (AOF + RDB) sur tous les nÅ“uds
   â”œâ”€ Configurer maxmemory et maxmemory-policy
   â””â”€ Tester le failover en environnement de staging

âœ… CONFIGURATION RÃ‰SEAU
   â”œâ”€ Latence inter-nÅ“uds < 10ms
   â”œâ”€ Bande passante suffisante pour Gossip + rÃ©plication
   â”œâ”€ Pas de NAT entre les nÅ“uds du cluster
   â””â”€ DNS ou IPs fixes pour chaque nÅ“ud

âœ… MONITORING
   â”œâ”€ Alertes sur cluster_state != ok
   â”œâ”€ Alertes sur cluster_slots_ok < 16384
   â”œâ”€ Monitoring de la latence Gossip
   â”œâ”€ Tracking des redirections -MOVED (taux Ã©levÃ© = problÃ¨me)
   â””â”€ Dashboard avec Ã©tat de tous les nÅ“uds

âœ… OPÃ‰RATIONS
   â”œâ”€ ProcÃ©dure de resharding documentÃ©e
   â”œâ”€ ProcÃ©dure de failover manuel documentÃ©e
   â”œâ”€ Plan de reprise d'activitÃ© (DRP)
   â””â”€ Runbook pour les scÃ©narios d'incident courants
```

## Conclusion

Redis Cluster reprÃ©sente une implÃ©mentation Ã©lÃ©gante et performante du scaling horizontal pour Redis, basÃ©e sur trois piliers fondamentaux :

1. **Le sharding via hash slots** offre un partitionnement dÃ©terministe et efficace des donnÃ©es
2. **Le protocole Gossip** assure une coordination dÃ©centralisÃ©e sans point de dÃ©faillance unique
3. **L'architecture distribuÃ©e** permet un scaling linÃ©aire et une haute disponibilitÃ© native

Ces mÃ©canismes, bien que sophistiquÃ©s, sont conÃ§us pour fonctionner de maniÃ¨re transparente tout en offrant aux opÃ©rateurs un contrÃ´le prÃ©cis via des procÃ©dures de maintenance bien dÃ©finies. La comprÃ©hension approfondie de ces concepts est essentielle pour dÃ©ployer et maintenir un cluster Redis robuste en production.

---

**Points clÃ©s Ã  retenir :**

- **16384 hash slots** : UnitÃ© atomique de distribution, calculÃ©s via CRC16
- **Gossip = O(log N)** : Convergence rapide avec faible overhead
- **Consensus PFAIL â†’ FAIL** : DÃ©tection de pannes par majoritÃ©
- **Client-side routing** : Clients intelligents pour minimiser les redirections
- **Failover automatique** : ~15-30 secondes avec replicas
- **Pas de cohÃ©rence forte** : Accepter l'Ã©ventuelle cohÃ©rence
- **Limitation multi-clÃ©s** : Utiliser hash tags pour grouper
- **Monitoring crucial** : `cluster_state:ok` et `cluster_slots_ok:16384`

La section suivante (11.2) explorera l'architecture "Shared-Nothing" qui sous-tend ces concepts.

â­ï¸ [Architecture "Shared-Nothing" du Cluster](/11-architecture-distribuee-scaling/02-architecture-shared-nothing.md)
