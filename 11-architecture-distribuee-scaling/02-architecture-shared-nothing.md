ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 11.2 Architecture "Shared-Nothing" du Cluster

## Introduction

L'architecture "Shared-Nothing" (ou "non partagÃ©e") constitue le paradigme fondamental sur lequel repose Redis Cluster. Ce modÃ¨le architectural, popularisÃ© dans les annÃ©es 1980 par Michael Stonebraker, reprÃ©sente une rupture radicale avec les architectures traditionnelles de bases de donnÃ©es qui s'appuyaient sur des ressources partagÃ©es (mÃ©moire, disque, ou processeur).

Dans une architecture Shared-Nothing, chaque nÅ“ud du cluster est complÃ¨tement autonome et indÃ©pendant. Il possÃ¨de ses propres ressources (CPU, RAM, stockage) et ne partage rien avec les autres nÅ“uds, communiquant uniquement via des messages rÃ©seau. Cette approche Ã©limine les points de contention centralisÃ©s et permet un scaling horizontal quasi-illimitÃ©.

## Principes fondamentaux de l'architecture Shared-Nothing

### DÃ©finition formelle

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Architecture Shared-Nothing : DÃ©finition          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                            â”‚
â”‚  PropriÃ©tÃ©s caractÃ©ristiques :                             â”‚
â”‚                                                            â”‚
â”‚  1. AUTONOMIE COMPLÃˆTE                                     â”‚
â”‚     â””â”€> Chaque nÅ“ud = unitÃ© indÃ©pendante                   â”‚
â”‚         â”œâ”€ Processus distincts                             â”‚
â”‚         â”œâ”€ Espace mÃ©moire isolÃ©                            â”‚
â”‚         â””â”€ Stockage local dÃ©diÃ©                            â”‚
â”‚                                                            â”‚
â”‚  2. PAS DE RESSOURCE PARTAGÃ‰E                              â”‚
â”‚     â””â”€> Aucun accÃ¨s direct aux ressources d'autres nÅ“uds   â”‚
â”‚         â”œâ”€ Pas de mÃ©moire partagÃ©e (No Shared Memory)      â”‚
â”‚         â”œâ”€ Pas de disque partagÃ© (No Shared Disk)          â”‚
â”‚         â””â”€ Pas de bus systÃ¨me commun                       â”‚
â”‚                                                            â”‚
â”‚  3. COMMUNICATION PAR MESSAGES                             â”‚
â”‚     â””â”€> Ã‰change d'informations uniquement via rÃ©seau       â”‚
â”‚         â”œâ”€ Protocole de communication dÃ©fini               â”‚
â”‚         â”œâ”€ Messages asynchrones                            â”‚
â”‚         â””â”€ Pas de mÃ©moire transactionnelle distribuÃ©e      â”‚
â”‚                                                            â”‚
â”‚  4. PARTITIONNEMENT DES DONNÃ‰ES                            â”‚
â”‚     â””â”€> Chaque nÅ“ud possÃ¨de un sous-ensemble des donnÃ©es   â”‚
â”‚         â”œâ”€ Pas de duplication (sauf rÃ©plication explicite) â”‚
â”‚         â”œâ”€ Partition par clÃ© (sharding)                    â”‚
â”‚         â””â”€ LocalitÃ© des donnÃ©es                            â”‚
â”‚                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Comparaison avec les architectures alternatives

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Taxonomie des architectures de systÃ¨mes distribuÃ©s      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. SHARED-MEMORY (MÃ©moire PartagÃ©e)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚    MÃ©moire Commune (RAM)    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚         â”‚         â”‚
        â”Œâ”€â”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”
        â”‚ CPU 1 â”‚ â”‚ CPU 2 â”‚ â”‚ CPU 3 â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜

   Exemples : SMP (Symmetric Multi-Processing), NUMA
   âœ“ Latence trÃ¨s faible pour accÃ¨s mÃ©moire
   âœ— Scaling limitÃ© (~100 cores max)
   âœ— CoÃ»t trÃ¨s Ã©levÃ©
   âœ— Point de dÃ©faillance unique


2. SHARED-DISK (Disque PartagÃ©)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        â”Œâ”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”
        â”‚Node 1â”‚    â”‚Node 2â”‚    â”‚Node 3â”‚
        â”‚ RAM  â”‚    â”‚ RAM  â”‚    â”‚ RAM  â”‚
        â””â”€â”€â”€â”¬â”€â”€â”˜    â””â”€â”€â”€â”¬â”€â”€â”˜    â””â”€â”€â”€â”¬â”€â”€â”˜
            â”‚           â”‚           â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
                   â”‚   SAN   â”‚
                   â”‚ Storage â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   Exemples : Oracle RAC, IBM DB2 pureScale
   âœ“ Partage des donnÃ©es facilitÃ©
   âœ“ Pas de repartitionnement nÃ©cessaire
   âœ— Contention sur le stockage partagÃ©
   âœ— RÃ©seau SAN = coÃ»t Ã©levÃ© + SPOF
   âœ— Scaling limitÃ© par bande passante SAN


3. SHARED-NOTHING (Rien de PartagÃ©) âœ“ Redis Cluster
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Node 1  â”‚    â”‚  Node 2  â”‚    â”‚  Node 3  â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚   RAM    â”‚    â”‚   RAM    â”‚    â”‚   RAM    â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚   Disk   â”‚    â”‚   Disk   â”‚    â”‚   Disk   â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
         â”‚               â”‚               â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    Network Only

   Exemples : Redis Cluster, Cassandra, MongoDB sharded
   âœ“ Scaling horizontal quasi-illimitÃ©
   âœ“ Pas de SPOF ni de bottleneck centralisÃ©
   âœ“ Isolation des pannes par nÅ“ud
   âœ“ CoÃ»t linÃ©aire (commodity hardware)
   âœ— ComplexitÃ© accrue (partitionnement, routing)
   âœ— OpÃ©rations multi-partitions limitÃ©es
   âœ— CohÃ©rence Ã©ventuelle (trade-off CAP)
```

## ImplÃ©mentation dans Redis Cluster

### Architecture physique d'un nÅ“ud

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Anatomie d'un nÅ“ud Redis Cluster             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  Serveur Physique / VM / Container
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                                                           â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
  â”‚  â”‚        Processus Redis (Instance unique)             â”‚ â”‚
  â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
  â”‚  â”‚                                                      â”‚ â”‚
  â”‚  â”‚  Port 6379 : Client Protocol                         â”‚ â”‚
  â”‚  â”‚  â”œâ”€ Accepte connexions clients                       â”‚ â”‚
  â”‚  â”‚  â”œâ”€ GÃ¨re commandes Redis (GET, SET, ...)             â”‚ â”‚
  â”‚  â”‚  â””â”€ Retourne rÃ©sultats ou redirections               â”‚ â”‚
  â”‚  â”‚                                                      â”‚ â”‚
  â”‚  â”‚  Port 16379 : Cluster Bus (Gossip)                   â”‚ â”‚
  â”‚  â”‚  â”œâ”€ Communication inter-nÅ“uds                        â”‚ â”‚
  â”‚  â”‚  â”œâ”€ Heartbeats (PING/PONG)                           â”‚ â”‚
  â”‚  â”‚  â””â”€ Propagation d'Ã©tat                               â”‚ â”‚
  â”‚  â”‚                                                      â”‚ â”‚
  â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
  â”‚  â”‚           Espace MÃ©moire (RAM)                       â”‚ â”‚
  â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
  â”‚  â”‚  â€¢ Dataset local (hash slots assignÃ©s)               â”‚ â”‚
  â”‚  â”‚  â€¢ Table de routing (tous les slots â†’ nÅ“uds)         â”‚ â”‚
  â”‚  â”‚  â€¢ Ã‰tat du cluster (clusterState)                    â”‚ â”‚
  â”‚  â”‚  â€¢ Buffer rÃ©plication                                â”‚ â”‚
  â”‚  â”‚  â€¢ Buffer client output                              â”‚ â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
  â”‚                                                           â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
  â”‚  â”‚              Stockage Local (Disk)                   â”‚ â”‚
  â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
  â”‚  â”‚  â€¢ RDB snapshots (dump.rdb)                          â”‚ â”‚
  â”‚  â”‚  â€¢ AOF log (appendonly.aof)                          â”‚ â”‚
  â”‚  â”‚  â€¢ Cluster configuration (nodes.conf)                â”‚ â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
  â”‚                                                           â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Principe clÃ© : Aucun accÃ¨s direct aux ressources des autres nÅ“uds
```

### Isolation mÃ©moire complÃ¨te

Dans Redis Cluster, chaque nÅ“ud gÃ¨re sa propre mÃ©moire de maniÃ¨re totalement autonome :

```
Cluster de 3 nÅ“uds avec 64 GB RAM chacun :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Node A (192.168.1.10)             Node B (192.168.1.11)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   RAM : 64 GB       â”‚           â”‚   RAM : 64 GB       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Slots : 0-5460      â”‚           â”‚ Slots : 5461-10922  â”‚
â”‚ Keys  : ~10M        â”‚           â”‚ Keys  : ~10M        â”‚
â”‚ Used  : 42 GB       â”‚           â”‚ Used  : 43 GB       â”‚
â”‚ Free  : 22 GB       â”‚           â”‚ Free  : 21 GB       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                   â”‚
         â”‚                                   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
           Network Communication Only
                      â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   Node C (192.168.1.12)   â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
         â”‚   RAM : 64 GB             â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
         â”‚ Slots : 10923-16383       â”‚
         â”‚ Keys  : ~10M              â”‚
         â”‚ Used  : 41 GB             â”‚
         â”‚ Free  : 23 GB             â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

CaractÃ©ristiques :
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â€¢ Chaque nÅ“ud alloue et libÃ¨re sa mÃ©moire indÃ©pendamment
â€¢ Pas de synchronisation des allocations mÃ©moire
â€¢ Fragmentation locale Ã  chaque nÅ“ud
â€¢ Politique d'Ã©viction (maxmemory-policy) locale
â€¢ CapacitÃ© totale = Somme des capacitÃ©s individuelles
  â†’ 64 + 64 + 64 = 192 GB (vs 64 GB pour un seul nÅ“ud)
```

### Absence de transaction distribuÃ©e

ConsÃ©quence directe de l'architecture Shared-Nothing :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Transactions : Scope limitÃ© au nÅ“ud local            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

SCÃ‰NARIO SUPPORTÃ‰ (ClÃ©s sur le mÃªme slot)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Client exÃ©cute :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MULTI
SET {user:1000}:profile "John Doe"
SET {user:1000}:email "john@example.com"
INCR {user:1000}:login_count
EXEC

Calcul des slots :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
{user:1000}:profile      â†’ CRC16("user:1000") = Slot 5649
{user:1000}:email        â†’ CRC16("user:1000") = Slot 5649  âœ“
{user:1000}:login_count  â†’ CRC16("user:1000") = Slot 5649  âœ“

Toutes les clÃ©s â†’ mÃªme slot â†’ mÃªme nÅ“ud â†’ Transaction OK


SCÃ‰NARIO NON SUPPORTÃ‰ (ClÃ©s sur slots diffÃ©rents)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Client exÃ©cute :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MULTI
SET user:1000 "John"     â†’ Slot 5798 (Node A)
SET user:2000 "Jane"     â†’ Slot 8234 (Node B)
SET user:3000 "Bob"      â†’ Slot 12456 (Node C)
EXEC

RÃ©sultat :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âŒ CROSSSLOT Keys in request don't hash to the same slot

Raison :
â”€â”€â”€â”€â”€â”€â”€â”€
Redis Cluster ne peut pas garantir l'atomicitÃ© d'une transaction
qui touche plusieurs nÅ“uds (pas de 2PC - Two-Phase Commit)

Solutions :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Utiliser les hash tags pour co-localiser
2. Restructurer le modÃ¨le de donnÃ©es
3. Accepter plusieurs transactions sÃ©parÃ©es (sans atomicitÃ© globale)
```

## Avantages de l'architecture Shared-Nothing

### 1. Scaling horizontal illimitÃ© (thÃ©orique)

```
Performance et CapacitÃ© = Fonction linÃ©aire du nombre de nÅ“uds
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CapacitÃ© mÃ©moire :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1 nÅ“ud   â†’  64 GB
10 nÅ“uds â†’  640 GB
100 nÅ“uds â†’ 6.4 TB
1000 nÅ“uds â†’ 64 TB

Formule : CapacitÃ©_totale = N Ã— RAM_par_nÅ“ud


Throughput (ops/sec) :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1 nÅ“ud   â†’  100,000 ops/sec
10 nÅ“uds â†’  1,000,000 ops/sec
100 nÅ“uds â†’ 10,000,000 ops/sec

Formule : Throughput_total â‰ˆ N Ã— Throughput_par_nÅ“ud
(avec overhead minimal du routing et gossip)


Comparaison avec Shared-Disk :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Shared-Disk : Performance plafonne Ã  cause du storage bottleneck
Shared-Nothing : Performance scale linÃ©airement jusqu'aux limites rÃ©seau

         Performance (ops/sec)
              â”‚
              â”‚                    â•± Shared-Nothing
              â”‚                  â•±
              â”‚                â•±
  1,000,000 â”€â”€â”¤              â•±
              â”‚            â•±
              â”‚          â•±
    500,000 â”€â”€â”¤        â•±    â”Œâ”€â”€â”€â”€â”€â”€â”€ Shared-Disk (plateau)
              â”‚      â•±      â”‚
              â”‚    â•±â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚  â•±
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Nombre de nÅ“uds
                 1  10  20  30  40  50
```

### 2. Isolation complÃ¨te des pannes

```
DÃ©faillance d'un nÅ“ud = Impact localisÃ© uniquement
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Ã‰tat nominal (3 nÅ“uds) :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Node A : Slots 0-5460      â†’ 33.3% des donnÃ©es  âœ“ UP
Node B : Slots 5461-10922  â†’ 33.3% des donnÃ©es  âœ“ UP
Node C : Slots 10923-16383 â†’ 33.4% des donnÃ©es  âœ“ UP

CapacitÃ© disponible : 100%
Ops/sec disponibles : 100%


Node B tombe en panne :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Node A : Slots 0-5460      â†’ 33.3% des donnÃ©es  âœ“ UP
Node B : Slots 5461-10922  â†’ 33.3% des donnÃ©es  âŒ DOWN
Node C : Slots 10923-16383 â†’ 33.4% des donnÃ©es  âœ“ UP

Impact :
â”œâ”€ CapacitÃ© disponible : 66.7% (seules donnÃ©es de A et C)
â”œâ”€ Ops/sec disponibles : 66.7%
â”œâ”€ ClÃ©s slots 5461-10922 : INACCESSIBLES
â””â”€ ClÃ©s autres slots : ACCESSIBLES sans dÃ©gradation

Si replicas configurÃ©es :
â””â”€> Replica B1 promeut â†’ failover automatique
    â””â”€> CapacitÃ© : 100% restaurÃ©e en ~15-30 secondes


Contraste avec Shared-Disk :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Si disque partagÃ© tombe :
â””â”€> 100% des donnÃ©es INACCESSIBLES
    â””â”€> Tous les nÅ“uds impactÃ©s
        â””â”€> Cluster complÃ¨tement DOWN

Shared-Nothing = Isolation des pannes
```

### 3. Absence de point de contention unique

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Comparaison : Contention et Bottlenecks            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Architecture avec Proxy Central :
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    Clients (1000+)
         â”‚
         â”‚  Toutes les requÃªtes passent par le proxy
         â”‚
         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â—„â”€â”€â”€ BOTTLENECK (CPU + RÃ©seau)
    â”‚  Proxy  â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”
    â”‚    â”‚    â”‚
    â–¼    â–¼    â–¼
  Node  Node  Node
    A     B     C

Limites :
â”œâ”€ Proxy = SPOF
â”œâ”€ Saturation du proxy Ã  forte charge
â”œâ”€ Latence ajoutÃ©e (hop supplÃ©mentaire)
â””â”€> Scaling limitÃ© par capacitÃ© du proxy


Redis Cluster (Shared-Nothing) :
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    Clients (1000+)
         â”‚
    â”Œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”  Distribution intelligente
    â”‚    â”‚    â”‚  (Client-side routing)
    â–¼    â–¼    â–¼
  â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”
  â”‚Nodeâ”‚ â”‚Nodeâ”‚ â”‚Nodeâ”‚
  â”‚ A  â”‚ â”‚ B  â”‚ â”‚ C  â”‚
  â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜

Avantages :
â”œâ”€ Pas de SPOF
â”œâ”€ Charge distribuÃ©e uniformÃ©ment
â”œâ”€ Latence minimale (1 hop direct)
â”œâ”€ Chaque nÅ“ud = entry point valide
â””â”€> Scaling illimitÃ© par ajout de nÅ“uds
```

### 4. CoÃ»t linÃ©aire et hardware commodity

```
ModÃ¨le Ã©conomique : Shared-Nothing
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Configuration Shared-Disk (pour 600 GB RAM) :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ 3x Serveurs haute-performance : 180,000 â‚¬
â€¢ 1x SAN enterprise (FC/iSCSI) : 150,000 â‚¬
â€¢ Switch SAN : 50,000 â‚¬
â€¢ Total : ~380,000 â‚¬ + maintenance coÃ»teuse

Configuration Redis Cluster (pour 600 GB RAM) :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ 10x Serveurs commodity (64GB RAM) : 50,000 â‚¬
â€¢ Switch Ethernet 10Gb : 5,000 â‚¬
â€¢ Total : ~55,000 â‚¬ (7x moins cher !)

Scaling incrÃ©mental :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Besoin de +20% capacitÃ© ?
â””â”€> Shared-Disk : Upgrade SAN (coÃ»t ++)
â””â”€> Shared-Nothing : +2 nÅ“uds (coÃ»t +)

RÃ©silience :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Shared-Disk : Panne SAN = disaster
Shared-Nothing : Panne d'un nÅ“ud = impact 10%
```

## Trade-offs et limitations

### 1. ComplexitÃ© opÃ©rationnelle accrue

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    ComplexitÃ© : Shared-Nothing vs Single Instance           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Single Redis Instance (Simple) :
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â€¢ 1 processus Ã  monitorer
â€¢ 1 fichier de config
â€¢ 1 dump RDB Ã  backuper
â€¢ Latence : O(1) - accÃ¨s direct
â€¢ Troubleshooting : Logs centralisÃ©s


Redis Cluster (Complexe) :
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â€¢ N processus Ã  monitorer (ex: 6 nÅ“uds)
â€¢ N fichiers de config Ã  synchroniser
â€¢ N dumps RDB + 1 nodes.conf par nÅ“ud
â€¢ Latence : O(1) + overhead rÃ©seau + redirections
â€¢ Troubleshooting : Logs distribuÃ©s, correlation nÃ©cessaire
â€¢ Resharding manuel ou automatisÃ©
â€¢ Failover Ã  tester et valider
â€¢ Gossip protocol Ã  comprendre et monitorer

Compromis :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Scaling horizontal â‡” ComplexitÃ© opÃ©rationnelle
```

### 2. OpÃ©rations multi-clÃ©s limitÃ©es

Liste des limitations dues Ã  l'absence de coordination globale :

```
OPÃ‰RATIONS NON SUPPORTÃ‰ES (ou limitÃ©es) :
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. Transactions multi-slots :
   âŒ MULTI/EXEC sur clÃ©s de slots diffÃ©rents

2. Commandes multi-clÃ©s :
   âŒ MGET key1 key2 key3  (si slots diffÃ©rents)
   âŒ MSET key1 val1 key2 val2  (si slots diffÃ©rents)
   âŒ SUNION set1 set2 set3  (si slots diffÃ©rents)
   âŒ SDIFF set1 set2  (si slots diffÃ©rents)
   âŒ ZUNIONSTORE dest key1 key2  (si slots diffÃ©rents)

3. Scripts Lua :
   âŒ AccÃ¨s Ã  des clÃ©s de slots diffÃ©rents
   âš ï¸  Toutes les clÃ©s doivent Ãªtre passÃ©es en argument
       (pour vÃ©rification de co-localisation)

4. Rename entre slots :
   âŒ RENAME key1 key2  (si hash(key1) â‰  hash(key2))

5. Scan global :
   âš ï¸  SCAN nÃ©cessite N appels (1 par nÅ“ud)

6. SELECT database :
   âŒ Pas de support multi-database (toujours DB 0)


SOLUTIONS DE CONTOURNEMENT :
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. Hash Tags pour co-localisation forcÃ©e :
   âœ“ {user:1000}:profile
   âœ“ {user:1000}:friends
   â””â”€> Garantit mÃªme slot

2. Restructuration du modÃ¨le de donnÃ©es :
   Au lieu de : user:1000, user:2000, user:3000
   Utiliser : {shard:0}:users (hash contenant tous les users)

3. Application-level aggregation :
   ImplÃ©menter l'agrÃ©gation cÃ´tÃ© client si multi-slots requis

4. Accepter la cohÃ©rence Ã©ventuelle :
   Plusieurs opÃ©rations sÃ©parÃ©es au lieu d'une transaction atomique
```

### 3. Overhead du rÃ©seau

```
Comparaison des latences :
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Single Instance (In-Memory) :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GET key â†’ Latence : ~0.1 ms (access RAM local)


Redis Cluster (mÃªme datacenter) :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GET key â†’ Latence : ~0.5-1 ms
â”œâ”€ Calcul du slot : ~0.01 ms
â”œâ”€ Routing vers bon nÅ“ud : 0 ms (si client intelligent)
â”œâ”€ Network RTT : ~0.3-0.5 ms
â””â”€ Access RAM : ~0.1 ms

Overhead rÃ©seau : +0.4-0.9 ms (~5-10x)


Redis Cluster (inter-datacenter) :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GET key â†’ Latence : ~50-200 ms
â””â”€> Network RTT domine (gÃ©ographie)


MITIGATION :
â•â•â•â•â•â•â•â•â•â•â•â•
â€¢ Utiliser clients intelligents (pas de redirections)
â€¢ Pipelining pour amortir le RTT
â€¢ Co-localiser clients et cluster (mÃªme datacenter/VPC)
â€¢ Caching cÃ´tÃ© client pour hot keys
```

### 4. Pas de cohÃ©rence forte garantie

```
FenÃªtre de vulnÃ©rabilitÃ© :
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Timeline d'une Ã©criture avec rÃ©plication asynchrone :

T0 : Client Ã©crit sur Master A
     SET user:1000 "John Doe"  âœ“
     â”‚
     â”‚ Acknowledgement immÃ©diat au client
     â–¼

T0+1ms : RÃ©plication vers Replica A1 (asynchrone)
         â”‚
         â”‚ DonnÃ©e en transit sur le rÃ©seau
         â”‚
         â–¼

T0+2ms : DonnÃ©e arrive sur Replica A1  âœ“

Mais si Master A crash entre T0 et T0+2ms :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â””â”€> DonnÃ©e perdue (pas encore rÃ©pliquÃ©e)
    â””â”€> Failover vers A1 â†’ DonnÃ©e absente


GARANTIE DE COHÃ‰RENCE :
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Redis Cluster offre : "At-most-once delivery"
â”œâ”€> Chaque Ã©criture confirmÃ©e au client...
â”œâ”€> ...PEUT Ãªtre perdue lors d'un failover
â””â”€> CohÃ©rence Ã‰VENTUELLE, pas FORTE


MITIGATION (pour Ã©critures critiques) :
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
WAIT numreplicas timeout
â””â”€> Force attente de la rÃ©plication avant ACK

Exemple :
â”€â”€â”€â”€â”€â”€â”€â”€â”€
SET critical:data "important"
WAIT 1 5000  â† Attendre au moins 1 replica (timeout 5s)
â””â”€> RÃ©duit la fenÃªtre de vulnÃ©rabilitÃ©
    â””â”€> Mais augmente la latence d'Ã©criture
```

## Gestion de la mÃ©moire distribuÃ©e

### StratÃ©gies d'allocation par nÅ“ud

```
Chaque nÅ“ud gÃ¨re sa mÃ©moire indÃ©pendamment :
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Configuration par nÅ“ud (redis.conf) :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
maxmemory 60gb                    # Limite RAM
maxmemory-policy allkeys-lru      # Politique d'Ã©viction

ScÃ©nario avec 3 nÅ“uds (64 GB RAM each) :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Node A : maxmemory 60gb
â”œâ”€ UtilisÃ© : 58 GB (97%)  â† Proche de la limite
â”œâ”€ Politique : allkeys-lru activÃ©e
â””â”€> Ã‰victions locales sur Node A uniquement

Node B : maxmemory 60gb
â”œâ”€ UtilisÃ© : 45 GB (75%)  â† Beaucoup d'espace libre
â”œâ”€ Politique : allkeys-lru en attente
â””â”€> Pas d'Ã©victions

Node C : maxmemory 60gb
â”œâ”€ UtilisÃ© : 52 GB (87%)
â”œâ”€ Politique : allkeys-lru en attente
â””â”€> Pas d'Ã©victions


ProblÃ©matique : Hot Spots
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Si un slot contient beaucoup plus de donnÃ©es :
â””â”€> Le nÅ“ud responsable atteint maxmemory avant les autres
    â””â”€> Ã‰victions asymÃ©triques
        â””â”€> Distribution inÃ©gale de la capacitÃ© utilisÃ©e
```

### Monitoring de la mÃ©moire distribuÃ©e

```bash
# Script de monitoring agrÃ©gÃ© pour le cluster

#!/bin/bash
# cluster-memory-monitor.sh

NODES=(
    "192.168.1.10:6379"
    "192.168.1.11:6379"
    "192.168.1.12:6379"
)

echo "=== Cluster Memory Report ==="
echo ""

total_used=0
total_max=0

for node in "${NODES[@]}"; do
    IFS=':' read -r host port <<< "$node"

    used=$(redis-cli -h $host -p $port INFO memory | grep "used_memory:" | cut -d: -f2 | tr -d '\r')
    max=$(redis-cli -h $host -p $port CONFIG GET maxmemory | tail -1)

    used_gb=$(echo "scale=2; $used / 1024 / 1024 / 1024" | bc)
    max_gb=$(echo "scale=2; $max / 1024 / 1024 / 1024" | bc)
    usage=$(echo "scale=1; $used * 100 / $max" | bc)

    echo "Node $node:"
    echo "  Used: ${used_gb} GB / ${max_gb} GB (${usage}%)"

    total_used=$(echo "$total_used + $used" | bc)
    total_max=$(echo "$total_max + $max" | bc)
done

total_used_gb=$(echo "scale=2; $total_used / 1024 / 1024 / 1024" | bc)
total_max_gb=$(echo "scale=2; $total_max / 1024 / 1024 / 1024" | bc)
total_usage=$(echo "scale=1; $total_used * 100 / $total_max" | bc)

echo ""
echo "=== Cluster Totals ==="
echo "Total Used: ${total_used_gb} GB / ${total_max_gb} GB (${total_usage}%)"
```

### RÃ©Ã©quilibrage de la charge mÃ©moire

ProcÃ©dure pour corriger une distribution inÃ©gale :

```bash
# 1. Identifier les nÅ“uds surchargÃ©s
redis-cli --cluster check 192.168.1.10:6379

# Sortie typique :
# Node A: 58GB (97%) â† SurchargÃ©
# Node B: 45GB (75%)
# Node C: 52GB (87%)

# 2. Analyser la distribution des clÃ©s par slot
for node in 192.168.1.10 192.168.1.11 192.168.1.12; do
    echo "=== $node ==="
    redis-cli -h $node DBSIZE
    redis-cli -h $node INFO keyspace
done

# 3. DÃ©cision de resharding
# Si Node A est surchargÃ©, dÃ©placer des slots vers Node B

redis-cli --cluster reshard 192.168.1.10:6379 \
    --cluster-from <node-a-id> \
    --cluster-to <node-b-id> \
    --cluster-slots 500 \
    --cluster-yes

# 4. Monitoring post-resharding
watch -n 10 './cluster-memory-monitor.sh'

# 5. Ajuster maxmemory si nÃ©cessaire
redis-cli -h 192.168.1.10 CONFIG SET maxmemory 65gb
```

## ProcÃ©dures de maintenance avancÃ©es

### 1. Ajout d'un nÅ“ud avec rÃ©Ã©quilibrage automatique

```bash
# ProcÃ©dure complÃ¨te d'ajout de nÅ“ud

# Ã‰tape 0 : VÃ©rifier l'Ã©tat actuel du cluster
redis-cli --cluster check 192.168.1.10:6379
redis-cli --cluster info 192.168.1.10:6379

# Sortie :
# 192.168.1.10:6379 -> 10240 keys | 5461 slots
# 192.168.1.11:6379 -> 10280 keys | 5462 slots
# 192.168.1.12:6379 -> 10244 keys | 5461 slots

# Ã‰tape 1 : DÃ©marrer le nouveau nÅ“ud (Node D)
ssh 192.168.1.13
sudo systemctl start redis@6379

# VÃ©rifier qu'il est seul (pas encore dans le cluster)
redis-cli -h 192.168.1.13 -p 6379 CLUSTER INFO
# cluster_state:fail (normal, pas encore de cluster)
# cluster_known_nodes:1 (lui-mÃªme seulement)

# Ã‰tape 2 : Ajouter le nÅ“ud au cluster (MEET)
redis-cli --cluster add-node 192.168.1.13:6379 192.168.1.10:6379

# VÃ©rification : Node D connaÃ®t maintenant tous les autres
redis-cli -h 192.168.1.13 CLUSTER NODES
# Devrait lister les 4 nÅ“uds (A, B, C, D)

# Ã‰tat : D est dans le cluster mais possÃ¨de 0 slots

# Ã‰tape 3 : RÃ©Ã©quilibrage automatique
redis-cli --cluster rebalance 192.168.1.10:6379 \
    --cluster-threshold 1 \
    --cluster-use-empty-masters

# Explications des paramÃ¨tres :
# --cluster-threshold 1 : Accepter dÃ©sÃ©quilibre de 1% max
# --cluster-use-empty-masters : Utiliser Node D (qui a 0 slots)

# RÃ©sultat attendu :
# 192.168.1.10:6379 -> 4096 slots (25%)
# 192.168.1.11:6379 -> 4096 slots (25%)
# 192.168.1.12:6379 -> 4096 slots (25%)
# 192.168.1.13:6379 -> 4096 slots (25%) â† Nouveau

# Ã‰tape 4 : Ajouter une replica pour Node D
redis-cli --cluster add-node 192.168.1.14:6379 192.168.1.10:6379 \
    --cluster-slave \
    --cluster-master-id <node-d-id>

# Ã‰tape 5 : VÃ©rification finale
redis-cli --cluster check 192.168.1.10:6379
# [OK] All 16384 slots covered.
# 4 masters, 4 replicas

# Ã‰tape 6 : Monitoring post-ajout (24-48h)
# VÃ©rifier :
# - Distribution uniforme de la charge
# - Pas de redirections excessives
# - RÃ©plication stable
# - Gossip protocol healthy
```

### 2. Retrait d'un nÅ“ud avec redistribution

```bash
# ProcÃ©dure sÃ©curisÃ©e de retrait de nÅ“ud

# Ã‰tape 0 : Identifier le nÅ“ud Ã  retirer
redis-cli --cluster check 192.168.1.10:6379

# DÃ©cision : Retirer Node C (192.168.1.12) et sa replica

# Ã‰tape 1 : Si le nÅ“ud a une replica, la supprimer d'abord
NODE_C_REPLICA_ID=$(redis-cli -h 192.168.1.15 CLUSTER MYID)
redis-cli --cluster del-node 192.168.1.10:6379 $NODE_C_REPLICA_ID

# Ã‰tape 2 : DÃ©placer TOUS les slots de Node C vers les autres nÅ“uds
NODE_C_ID=$(redis-cli -h 192.168.1.12 CLUSTER MYID)

# Option A : RÃ©partir Ã©quitablement sur tous les autres nÅ“uds
redis-cli --cluster reshard 192.168.1.10:6379 \
    --cluster-from $NODE_C_ID \
    --cluster-to all \
    --cluster-slots 5461 \
    --cluster-yes \
    --cluster-timeout 60000 \
    --cluster-pipeline 10

# Option B : Cibler un nÅ“ud spÃ©cifique
# redis-cli --cluster reshard 192.168.1.10:6379 \
#     --cluster-from $NODE_C_ID \
#     --cluster-to <node-a-id> \
#     --cluster-slots 5461 \
#     --cluster-yes

# Ã‰tape 3 : VÃ©rifier que Node C ne possÃ¨de plus aucun slot
redis-cli -h 192.168.1.12 CLUSTER INFO | grep cluster_slots_assigned
# Doit retourner : cluster_slots_assigned:0

redis-cli --cluster check 192.168.1.10:6379
# 192.168.1.12:6379 -> 0 keys | 0 slots â† PrÃªt Ã  Ãªtre retirÃ©

# Ã‰tape 4 : Retirer Node C du cluster
redis-cli --cluster del-node 192.168.1.10:6379 $NODE_C_ID

# Ã‰tape 5 : ArrÃªter le service Redis sur Node C
ssh 192.168.1.12
sudo systemctl stop redis@6379
sudo systemctl disable redis@6379

# Optionnel : Nettoyer les donnÃ©es
# sudo rm -rf /var/lib/redis/*

# Ã‰tape 6 : VÃ©rification finale
redis-cli --cluster check 192.168.1.10:6379
# [OK] All 16384 slots covered.
# 3 masters, 3 replicas (Node C absent)

# Ã‰tape 7 : Mettre Ã  jour la configuration cÃ´tÃ© client
# Retirer 192.168.1.12:6379 des seeds nodes
# Les clients intelligents dÃ©couvriront automatiquement la nouvelle topologie
```

### 3. Maintenance avec zÃ©ro downtime

```bash
# ScÃ©nario : Mise Ã  jour d'un nÅ“ud (OS patch, upgrade Redis, etc.)

# Principe : Basculer sur replica avant maintenance du master

# Ã‰tape 1 : Identifier le master Ã  maintenir
TARGET_MASTER="192.168.1.11:6379"

# Ã‰tape 2 : VÃ©rifier que le master a bien une replica
redis-cli -h 192.168.1.11 -p 6379 INFO replication
# role:master
# connected_slaves:1
# slave0:ip=192.168.1.14,port=6379,state=online

# Ã‰tape 3 : DÃ©clencher un failover PLANIFIÃ‰ sur la replica
# (Pas d'attente de timeout, basculement immÃ©diat)
redis-cli -h 192.168.1.14 -p 6379 CLUSTER FAILOVER TAKEOVER

# La replica devient master SANS attendre le timeout
# L'ancien master devient replica automatiquement

# Ã‰tape 4 : VÃ©rifier le basculement
redis-cli -h 192.168.1.14 CLUSTER NODES | grep myself
# Devrait afficher : myself,master

redis-cli -h 192.168.1.11 CLUSTER NODES | grep myself
# Devrait afficher : myself,slave

# Ã‰tape 5 : Maintenance sur l'ancien master (maintenant replica)
ssh 192.168.1.11

# ArrÃªter Redis
sudo systemctl stop redis@6379

# Effectuer la maintenance (OS patch, upgrade, etc.)
sudo apt update && sudo apt upgrade -y
# ou
sudo yum update -y
# ou upgrade Redis binary

# RedÃ©marrer Redis
sudo systemctl start redis@6379

# VÃ©rifier que la rÃ©plication reprend
redis-cli -h 192.168.1.11 INFO replication
# role:slave
# master_host:192.168.1.14
# master_link_status:up

# Ã‰tape 6 : Optionnel - Re-basculer pour restaurer la topologie initiale
# Si souhaitÃ©, faire l'inverse pour remettre 192.168.1.11 en master
redis-cli -h 192.168.1.11 CLUSTER FAILOVER TAKEOVER

# Ã‰tape 7 : RÃ©pÃ©ter pour tous les autres nÅ“uds
# En parallÃ¨le ou sÃ©quentiellement selon politique de maintenance

# Downtime total : 0 seconde (basculement instantanÃ©)
```

### 4. RÃ©cupÃ©ration aprÃ¨s corruption du cluster

```bash
# ScÃ©nario catastrophe : Configuration cluster corrompue

# SymptÃ´mes :
# - cluster_state:fail persistant
# - Slots non assignÃ©s ou dupliquÃ©s
# - NÅ“uds ne se reconnaissent plus

# ATTENTION : ProcÃ©dure de dernier recours

# Ã‰tape 1 : Backup complet AVANT toute intervention
for node in 192.168.1.10 192.168.1.11 192.168.1.12; do
    ssh $node "sudo cp /var/lib/redis/nodes.conf /var/lib/redis/nodes.conf.backup"
    ssh $node "sudo cp /var/lib/redis/dump.rdb /var/lib/redis/dump.rdb.backup"
done

# Ã‰tape 2 : Tentative de rÃ©paration automatique
redis-cli --cluster fix 192.168.1.10:6379 \
    --cluster-fix-with-unreachable-masters

# Si Ã©chec, passer aux Ã©tapes suivantes

# Ã‰tape 3 : Reset SOFT du cluster (prÃ©serve les donnÃ©es)
# Ã€ faire sur TOUS les nÅ“uds
for node in 192.168.1.10 192.168.1.11 192.168.1.12; do
    redis-cli -h $node CLUSTER RESET SOFT
done

# Ã‰tape 4 : RecrÃ©er le cluster
redis-cli --cluster create \
    192.168.1.10:6379 \
    192.168.1.11:6379 \
    192.168.1.12:6379 \
    --cluster-replicas 0 \
    --cluster-yes

# Les donnÃ©es sont prÃ©servÃ©es car RESET SOFT n'a pas vidÃ© le dataset

# Ã‰tape 5 : RÃ©-ajouter les replicas
redis-cli --cluster add-node 192.168.1.13:6379 192.168.1.10:6379 \
    --cluster-slave --cluster-master-id <node-a-id>

redis-cli --cluster add-node 192.168.1.14:6379 192.168.1.10:6379 \
    --cluster-slave --cluster-master-id <node-b-id>

redis-cli --cluster add-node 192.168.1.15:6379 192.168.1.10:6379 \
    --cluster-slave --cluster-master-id <node-c-id>

# Ã‰tape 6 : VÃ©rification exhaustive
redis-cli --cluster check 192.168.1.10:6379
redis-cli --cluster info 192.168.1.10:6379

# Comparer le nombre de clÃ©s avant/aprÃ¨s
# VÃ©rifier qu'aucune donnÃ©e n'a Ã©tÃ© perdue
```

## ConsidÃ©rations pour la production

### Dimensionnement optimal

```
RÃ¨gles de dimensionnement Shared-Nothing :
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. NOMBRE DE NÅ’UDS
   â”œâ”€ Minimum : 3 masters (majoritÃ© = 2)
   â”œâ”€ RecommandÃ© production : 6 nÅ“uds (3M + 3R)
   â”œâ”€ Large scale : 10-50 nÅ“uds
   â””â”€ Maximum pratique : ~1000 nÅ“uds
      (au-delÃ , overhead Gossip devient significatif)

2. MÃ‰MOIRE PAR NÅ’UD
   â”œâ”€ Minimum : 8 GB
   â”œâ”€ Sweet spot : 16-64 GB
   â”œâ”€ Maximum : 256 GB
   â””â”€> Au-delÃ  de 256GB :
       â”œâ”€ Fork pour RDB/AOF devient lent
       â”œâ”€ Failover plus long (rÃ©plication volumineuse)
       â””â”€ ConsidÃ©rer split en plusieurs nÅ“uds

3. CPU PAR NÅ’UD
   â”œâ”€ Single-threaded (1 core pour Redis)
   â”œâ”€ RecommandÃ© : 4-8 cores
   â””â”€> Cores additionnels utiles pour :
       â”œâ”€ I/O threads (Redis 6+)
       â”œâ”€ Background saving (fork)
       â””â”€ Monitoring / Logs

4. RÃ‰SEAU
   â”œâ”€ Minimum : 1 Gbps
   â”œâ”€ RecommandÃ© : 10 Gbps
   â”œâ”€ Latence inter-nÅ“uds : < 1 ms (LAN) ou < 10 ms (WAN)
   â””â”€> Bande passante critique pour :
       â”œâ”€ Gossip protocol
       â”œâ”€ RÃ©plication masterâ†’replica
       â””â”€ Resharding (migration de slots)

5. STOCKAGE
   â”œâ”€ Type : SSD recommandÃ© (si persistence activÃ©e)
   â”œâ”€ CapacitÃ© : 2x la RAM (pour RDB + AOF)
   â”œâ”€ IOPS : > 10,000 IOPS
   â””â”€> Redis est in-memory mais :
       â”œâ”€ RDB dump = burst write
       â”œâ”€ AOF = sÃ©quentiel mais continu
```

### Topologies recommandÃ©es

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Topologies Shared-Nothing Courantes            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. PETITE SCALE (< 100 GB, < 100k ops/sec)
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

   3 Masters + 3 Replicas (6 nÅ“uds total)

   [M1] â”€â”€â”€ [R1]
   [M2] â”€â”€â”€ [R2]
   [M3] â”€â”€â”€ [R3]

   CapacitÃ© : 3 Ã— RAM_par_nÅ“ud
   HA : TolÃ¨re 1 panne de master


2. MOYENNE SCALE (100-500 GB, 100k-1M ops/sec)
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

   6 Masters + 6 Replicas (12 nÅ“uds total)

   [M1]â”€[R1]   [M2]â”€[R2]   [M3]â”€[R3]
   [M4]â”€[R4]   [M5]â”€[R5]   [M6]â”€[R6]

   CapacitÃ© : 6 Ã— RAM_par_nÅ“ud
   HA : TolÃ¨re 3 pannes simultanÃ©es (1 par master)


3. LARGE SCALE (> 500 GB, > 1M ops/sec)
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

   12+ Masters avec replicas multiples

   ConsidÃ©rations :
   â”œâ”€ Gossip overhead : monitoring requis
   â”œâ”€ Latence rÃ©seau : critique
   â””â”€ ComplÃ©xitÃ© opÃ©rationnelle Ã©levÃ©e


4. MULTI-DC (Disaster Recovery)
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

   DC1 (Primary):        DC2 (DR):
   [M1]â”€[R1]             [R1']
   [M2]â”€[R2]      âŸ·     [R2']
   [M3]â”€[R3]             [R3']

   RÃ©plication asynchrone inter-DC
   âš ï¸  Latence Ã©levÃ©e (10-100+ ms)
   âš ï¸  Pas de failover automatique inter-DC
```

### Checklist de dÃ©ploiement production

```
âœ… AVANT LE DÃ‰PLOIEMENT
   â”œâ”€ Architecture validÃ©e (nombre de nÅ“uds, RAM, rÃ©seau)
   â”œâ”€ Cluster configurÃ© et testÃ© en staging
   â”œâ”€ ProcÃ©dures de failover testÃ©es et documentÃ©es
   â”œâ”€ ProcÃ©dures de resharding testÃ©es
   â”œâ”€ Plan de reprise d'activitÃ© (DRP) rÃ©digÃ©
   â””â”€ Formation Ã©quipe ops sur Redis Cluster

âœ… CONFIGURATION CLUSTER
   â”œâ”€ cluster-enabled yes
   â”œâ”€ cluster-node-timeout 15000 (ajuster selon rÃ©seau)
   â”œâ”€ cluster-replica-validity-factor 10
   â”œâ”€ cluster-require-full-coverage yes (ou no selon cas)
   â””â”€ cluster-migration-barrier 1

âœ… CONFIGURATION SYSTÃˆME
   â”œâ”€ DÃ©sactiver THP (Transparent Huge Pages)
   â”œâ”€ Configurer overcommit_memory = 1
   â”œâ”€ Ajuster somaxconn et tcp-backlog
   â”œâ”€ Configurer firewall (ports 6379 + 16379)
   â””â”€ NTP synchronisÃ© sur tous les nÅ“uds

âœ… MONITORING
   â”œâ”€ Dashboards : cluster_state, slots coverage, memory
   â”œâ”€ Alertes : nÅ“ud down, slot non couvert, mÃ©moire haute
   â”œâ”€ MÃ©triques Gossip : latency, message rate
   â”œâ”€ Logs centralisÃ©s (ELK, Splunk, etc.)
   â””â”€ Health checks automatisÃ©s

âœ… HAUTE DISPONIBILITÃ‰
   â”œâ”€ Au moins 1 replica par master
   â”œâ”€ Replicas sur diffÃ©rents racks/AZ
   â”œâ”€ Persistence configurÃ©e (RDB + AOF)
   â”œâ”€ Backups automatisÃ©s (quotidien minimum)
   â””â”€ ProcÃ©dure de restauration testÃ©e

âœ… SÃ‰CURITÃ‰
   â”œâ”€ ACLs configurÃ©es (requirepass dÃ©prÃ©ciÃ©e)
   â”œâ”€ TLS/SSL activÃ© si sensible
   â”œâ”€ Bind sur interfaces privÃ©es uniquement
   â”œâ”€ Firewall : whitelist IP sources
   â””â”€ Audit logging activÃ©

âœ… OPÃ‰RATIONS
   â”œâ”€ Runbooks pour scÃ©narios d'incident
   â”œâ”€ Contacts on-call dÃ©finis
   â”œâ”€ FenÃªtres de maintenance planifiÃ©es
   â”œâ”€ Processus de rollback documentÃ©
   â””â”€ Post-mortems aprÃ¨s chaque incident
```

## Conclusion

L'architecture Shared-Nothing de Redis Cluster reprÃ©sente un choix architectural fondamental qui privilÃ©gie :

1. **Scaling horizontal** illimitÃ© sur la simplicitÃ© opÃ©rationnelle
2. **Performance distribuÃ©e** sur la cohÃ©rence forte
3. **RÃ©silience par isolation** sur la coordination globale
4. **CoÃ»t linÃ©aire** (commodity hardware) sur l'infrastructure spÃ©cialisÃ©e

Cette architecture impose des contraintes (opÃ©rations multi-clÃ©s limitÃ©es, cohÃ©rence Ã©ventuelle) mais offre en contrepartie des capacitÃ©s de scaling et de rÃ©silience impossibles Ã  atteindre avec des architectures traditionnelles.

La comprÃ©hension profonde de ce paradigme Shared-Nothing est essentielle pour :
- Concevoir des modÃ¨les de donnÃ©es adaptÃ©s (hash tags, co-localisation)
- Dimensionner correctement le cluster (nombre de nÅ“uds, RAM par nÅ“ud)
- OpÃ©rer le cluster en production (resharding, maintenance, monitoring)
- Anticiper et gÃ©rer les cas limites (hot spots, pannes multiples)

---

**Points clÃ©s Ã  retenir :**

- **Autonomie complÃ¨te** : Chaque nÅ“ud = processus + RAM + disque indÃ©pendants
- **Pas de ressource partagÃ©e** : Communication uniquement via rÃ©seau
- **Scaling linÃ©aire** : CapacitÃ© totale = Somme des capacitÃ©s individuelles
- **Isolation des pannes** : DÃ©faillance localisÃ©e (1/N des donnÃ©es)
- **ComplexitÃ© opÃ©rationnelle** : Trade-off inÃ©vitable du Shared-Nothing
- **Pas de cohÃ©rence forte** : Accepter l'Ã©ventuelle cohÃ©rence
- **Maintenance par nÅ“ud** : Chaque nÅ“ud est une unitÃ© opÃ©rationnelle
- **Dimensionnement critique** : 16-64 GB RAM / nÅ“ud = sweet spot

La section suivante (11.3) dÃ©taillera les mÃ©canismes de distribution des donnÃ©es via les hash slots.

â­ï¸ [Distribution des donnÃ©es : Hash Slots (0-16383)](/11-architecture-distribuee-scaling/03-distribution-donnees-hash-slots.md)
