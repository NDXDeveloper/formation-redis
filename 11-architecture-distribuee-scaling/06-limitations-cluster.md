ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 11.6 Limitations du Cluster (multi-key operations, transactions)

## Introduction

Redis Cluster, malgrÃ© ses nombreux avantages en termes de scaling horizontal et de haute disponibilitÃ©, impose des contraintes architecturales significatives qui dÃ©coulent directement de son modÃ¨le distribuÃ© Shared-Nothing. Ces limitations ne sont pas des dÃ©fauts de conception, mais plutÃ´t des compromis inÃ©vitables inhÃ©rents Ã  tout systÃ¨me distribuÃ© qui cherche Ã  maintenir performance, disponibilitÃ© et tolÃ©rance aux pannes.

Cette section explore en dÃ©tail les contraintes imposÃ©es par Redis Cluster, leurs origines techniques, leurs implications pratiques, et les stratÃ©gies pour les contourner ou s'adapter Ã  ces limitations lors de la conception d'applications.

## Origine des limitations : Le problÃ¨me fondamental

### Le thÃ©orÃ¨me CAP appliquÃ© Ã  Redis Cluster

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ThÃ©orÃ¨me CAP                             â”‚
â”‚              (Consistency, Availability, Partition)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Un systÃ¨me distribuÃ© ne peut garantir simultanÃ©ment que 2 des 3 propriÃ©tÃ©s :

    [C] Consistency          [A] Availability       [P] Partition Tolerance
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•          â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•       â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    Tous les nÅ“uds           Le systÃ¨me rÃ©pond      Le systÃ¨me continue
    voient les mÃªmes         toujours (mÃªme         de fonctionner malgrÃ©
    donnÃ©es au mÃªme          partiellement)         les pannes rÃ©seau
    instant


Redis Cluster choisit : AP (Availability + Partition Tolerance)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    âœ“ Availability : Le cluster continue de servir les requÃªtes
                     mÃªme si certains nÅ“uds sont down

    âœ“ Partition Tolerance : Le cluster fonctionne malgrÃ©
                            les partitions rÃ©seau

    âœ— Strong Consistency : Pas de garantie que tous les nÅ“uds
                           voient la mÃªme valeur simultanÃ©ment


ConsÃ©quences directes :
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. Pas de transactions distribuÃ©es (2PC impossible)
   â””â”€> Les transactions sont limitÃ©es Ã  un seul nÅ“ud

2. CohÃ©rence Ã©ventuelle (eventual consistency)
   â””â”€> FenÃªtre de rÃ©plication asynchrone

3. OpÃ©rations multi-clÃ©s limitÃ©es
   â””â”€> Seulement si toutes les clÃ©s sont sur le mÃªme nÅ“ud

4. Pas de coordination globale
   â””â”€> Chaque nÅ“ud est autonome
```

### Partitionnement des donnÃ©es et indÃ©pendance des nÅ“uds

```
Le problÃ¨me fondamental des opÃ©rations distribuÃ©es :
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

OpÃ©ration atomique sur 1 nÅ“ud (Redis standalone) :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    MULTI
    SET key1 "value1"    â”
    SET key2 "value2"    â”‚ ExÃ©cutÃ©es atomiquement
    INCR counter         â”‚ sur le MÃŠME serveur
    EXEC                 â”˜

    Garantie : Tout ou rien (atomicitÃ©) âœ“


OpÃ©ration sur plusieurs nÅ“uds (Redis Cluster) :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    MULTI
    SET key1 "value1"    â†’ NÅ“ud A (slot 1234)
    SET key2 "value2"    â†’ NÅ“ud B (slot 5678)  â† Slots diffÃ©rents !
    INCR counter         â†’ NÅ“ud C (slot 9012)
    EXEC

    ProblÃ¨me :
    â”œâ”€ NÅ“ud A, B, C sont indÃ©pendants
    â”œâ”€ Pas de coordinateur centralisÃ©
    â”œâ”€ Pas de protocole 2PC (Two-Phase Commit)
    â””â”€> IMPOSSIBLE de garantir l'atomicitÃ©

    RÃ©sultat : âŒ CROSSSLOT Keys in request don't hash to the same slot
```

## Limitation 1 : OpÃ©rations multi-clÃ©s

### Commandes multi-clÃ©s affectÃ©es

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Commandes multi-clÃ©s avec limitations cluster        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚ COMMANDES GET/SET MULTIPLES                                 â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                 â”‚
â”‚                                                             â”‚
â”‚ MGET key1 key2 key3                                         â”‚
â”‚ â”œâ”€ Fonctionne SI : hash(key1) = hash(key2) = hash(key3)     â”‚
â”‚ â””â”€ Ã‰choue SI : clÃ©s sur slots/nÅ“uds diffÃ©rents              â”‚
â”‚                                                             â”‚
â”‚ MSET key1 val1 key2 val2 key3 val3                          â”‚
â”‚ â”œâ”€ MÃªme contrainte que MGET                                 â”‚
â”‚ â””â”€ Erreur : CROSSSLOT si slots diffÃ©rents                   â”‚
â”‚                                                             â”‚
â”‚ DEL key1 key2 key3                                          â”‚
â”‚ â”œâ”€ Peut Ã©chouer partiellement si multi-slots                â”‚
â”‚ â””â”€ Retourne nombre de clÃ©s supprimÃ©es (peut Ãªtre < 3)       â”‚
â”‚                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚ OPÃ‰RATIONS ENSEMBLISTES (SETS)                              â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                              â”‚
â”‚                                                             â”‚
â”‚ SUNION set1 set2 set3                                       â”‚
â”‚ SINTER set1 set2 set3                                       â”‚
â”‚ SDIFF set1 set2                                             â”‚
â”‚ â”œâ”€ NÃ©cessite que tous les sets soient sur le mÃªme slot      â”‚
â”‚ â””â”€ Solution : Hash tags ou restructuration                  â”‚
â”‚                                                             â”‚
â”‚ SUNIONSTORE dest set1 set2                                  â”‚
â”‚ SINTERSTORE dest set1 set2                                  â”‚
â”‚ SDIFFSTORE dest set1 set2                                   â”‚
â”‚ â”œâ”€ dest + tous les sets doivent Ãªtre sur mÃªme slot          â”‚
â”‚ â””â”€ Sinon : CROSSSLOT                                        â”‚
â”‚                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚ OPÃ‰RATIONS SUR SORTED SETS                                  â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                  â”‚
â”‚                                                             â”‚
â”‚ ZUNIONSTORE dest numkeys key1 key2                          â”‚
â”‚ ZINTERSTORE dest numkeys key1 key2                          â”‚
â”‚ â”œâ”€ Toutes les clÃ©s doivent Ãªtre co-localisÃ©es               â”‚
â”‚ â””â”€ Erreur CROSSSLOT si non respectÃ©                         â”‚
â”‚                                                             â”‚
â”‚ ZDIFF numkeys key1 key2 (Redis 6.2+)                        â”‚
â”‚ ZDIFFSTORE dest numkeys key1 key2                           â”‚
â”‚ â”œâ”€ MÃªme contrainte                                          â”‚
â”‚ â””â”€ Co-localisation obligatoire                              â”‚
â”‚                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚ OPÃ‰RATIONS BITWISE                                          â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                              â”‚
â”‚                                                             â”‚
â”‚ BITOP AND/OR/XOR dest key1 key2                             â”‚
â”‚ â”œâ”€ dest + toutes les clÃ©s sources â†’ mÃªme slot               â”‚
â”‚ â””â”€ UtilisÃ© pour opÃ©rations sur bitmaps distribuÃ©s           â”‚
â”‚                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚ AUTRES COMMANDES AFFECTÃ‰ES                                  â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                  â”‚
â”‚                                                             â”‚
â”‚ RENAME oldkey newkey                                        â”‚
â”‚ â”œâ”€ Fonctionne SI hash(oldkey) = hash(newkey)                â”‚
â”‚ â””â”€ Sinon : ERR No such key (impossible de dÃ©placer)         â”‚
â”‚                                                             â”‚
â”‚ RPOPLPUSH source dest                                       â”‚
â”‚ BRPOPLPUSH source dest timeout                              â”‚
â”‚ â”œâ”€ source et dest doivent Ãªtre sur mÃªme slot                â”‚
â”‚ â””â”€ Alternative Redis 6.2+ : LMOVE avec hash tags            â”‚
â”‚                                                             â”‚
â”‚ PFCOUNT key1 key2 key3 (HyperLogLog)                        â”‚
â”‚ PFMERGE dest key1 key2                                      â”‚
â”‚ â”œâ”€ Toutes les clÃ©s co-localisÃ©es                            â”‚
â”‚ â””â”€ HyperLogLog distribuÃ© nÃ©cessite merge cÃ´tÃ© client        â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Exemples d'Ã©checs et de succÃ¨s

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DÃ‰MONSTRATION DES LIMITATIONS MULTI-CLÃ‰S
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Se connecter au cluster
redis-cli -c -h 192.168.1.10 -p 6379


# EXEMPLE 1 : MGET avec clÃ©s sur slots diffÃ©rents (Ã‰CHEC)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

127.0.0.1:6379> SET user:1000 "Alice"
OK

127.0.0.1:6379> SET user:2000 "Bob"
-> Redirected to slot [8834] located at 192.168.1.11:6379
OK

127.0.0.1:6379> SET user:3000 "Charlie"
-> Redirected to slot [12456] located at 192.168.1.12:6379
OK

# Tentative de MGET sur les 3 clÃ©s
127.0.0.1:6379> MGET user:1000 user:2000 user:3000
(error) CROSSSLOT Keys in request don't hash to the same slot

# Calcul des slots (pour comprendre)
127.0.0.1:6379> CLUSTER KEYSLOT user:1000
(integer) 5798

127.0.0.1:6379> CLUSTER KEYSLOT user:2000
(integer) 8834

127.0.0.1:6379> CLUSTER KEYSLOT user:3000
(integer) 12456

# Slots diffÃ©rents â†’ MGET impossible


# EXEMPLE 2 : MGET avec hash tags (SUCCÃˆS)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Utiliser hash tags pour forcer mÃªme slot
127.0.0.1:6379> SET {users}:1000 "Alice"
OK

127.0.0.1:6379> SET {users}:2000 "Bob"
OK

127.0.0.1:6379> SET {users}:3000 "Charlie"
OK

# VÃ©rifier les slots
127.0.0.1:6379> CLUSTER KEYSLOT {users}:1000
(integer) 4576

127.0.0.1:6379> CLUSTER KEYSLOT {users}:2000
(integer) 4576  â† MÃªme slot !

127.0.0.1:6379> CLUSTER KEYSLOT {users}:3000
(integer) 4576  â† MÃªme slot !

# MGET fonctionne maintenant
127.0.0.1:6379> MGET {users}:1000 {users}:2000 {users}:3000
1) "Alice"
2) "Bob"
3) "Charlie"


# EXEMPLE 3 : OpÃ©rations ensemblistes (Ã‰CHEC sans hash tag)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

127.0.0.1:6379> SADD users:active "alice" "bob"
(integer) 2

127.0.0.1:6379> SADD users:premium "bob" "charlie"
-> Redirected to slot [9876] located at 192.168.1.11:6379
(integer) 2

# Tentative d'intersection
127.0.0.1:6379> SINTER users:active users:premium
(error) CROSSSLOT Keys in request don't hash to the same slot


# EXEMPLE 4 : OpÃ©rations ensemblistes (SUCCÃˆS avec hash tag)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

127.0.0.1:6379> SADD {usersets}:active "alice" "bob"
(integer) 2

127.0.0.1:6379> SADD {usersets}:premium "bob" "charlie"
(integer) 2

# Intersection fonctionne
127.0.0.1:6379> SINTER {usersets}:active {usersets}:premium
1) "bob"

# Store fonctionne aussi
127.0.0.1:6379> SINTERSTORE {usersets}:active_premium {usersets}:active {usersets}:premium
(integer) 1


# EXEMPLE 5 : RENAME entre slots diffÃ©rents (Ã‰CHEC)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

127.0.0.1:6379> SET old:key "value"
OK

# Calcul des slots
127.0.0.1:6379> CLUSTER KEYSLOT old:key
(integer) 1234

127.0.0.1:6379> CLUSTER KEYSLOT new:key
(integer) 5678  â† Slot diffÃ©rent

# RENAME Ã©choue
127.0.0.1:6379> RENAME old:key new:key
(error) ERR No such key

# Solution : hash tags
127.0.0.1:6379> SET {keys}:old "value"
OK

127.0.0.1:6379> RENAME {keys}:old {keys}:new
OK  âœ“


# EXEMPLE 6 : DEL multi-clÃ©s (succÃ¨s partiel)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

127.0.0.1:6379> SET key1 "a"
OK

127.0.0.1:6379> SET key2 "b"
-> Redirected to slot [4998] located at 192.168.1.10:6379
OK

127.0.0.1:6379> SET key3 "c"
-> Redirected to slot [9189] located at 192.168.1.11:6379
OK

# DEL ne retourne pas d'erreur mais peut ne supprimer qu'une partie
127.0.0.1:6379> DEL key1 key2 key3
(integer) 1  â† Seulement 1 clÃ© supprimÃ©e (celle du nÅ“ud local)

# Pour supprimer toutes les clÃ©s, le client doit router vers chaque nÅ“ud
```

### SchÃ©ma explicatif des limitations multi-clÃ©s

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Pourquoi MGET Ã©choue sur multi-slots               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Client envoie : MGET user:1000 user:2000 user:3000
                      â”‚         â”‚         â”‚
                      â”‚         â”‚         â”‚
        Calcul CRC16 sur chaque clÃ©
                      â”‚         â”‚         â”‚
                      â–¼         â–¼         â–¼
                  Slot 5798  Slot 8834  Slot 12456
                      â”‚         â”‚         â”‚
        Mapping slot â†’ nÅ“ud
                      â”‚         â”‚         â”‚
                      â–¼         â–¼         â–¼
                  Node A     Node B     Node C
                      â”‚         â”‚         â”‚

ProblÃ¨me : Les 3 clÃ©s sont sur 3 nÅ“uds diffÃ©rents
          â†“
Redis Cluster ne peut pas :
â”œâ”€ Faire un broadcast vers tous les nÅ“uds
â”œâ”€ Coordonner une transaction multi-nÅ“uds
â”œâ”€ Garantir l'atomicitÃ© de la lecture
â””â”€> ERREUR : CROSSSLOT


Solution : Hash Tags
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

MGET {users}:1000 {users}:2000 {users}:3000
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         Hash uniquement "users"
                â”‚
                â–¼
            Slot 4576
                â”‚
                â–¼
            Node A (unique)
                â”‚
                â–¼
          ExÃ©cution atomique âœ“
```

## Limitation 2 : Transactions (MULTI/EXEC)

### Scope des transactions limitÃ© Ã  un nÅ“ud

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Transactions dans Redis Cluster                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Redis Standalone (pas de limitation) :
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    MULTI
    SET account:1:balance 1000      â”
    SET account:2:balance 500       â”‚
    INCRBY account:1:balance -100   â”‚ Transaction atomique
    INCRBY account:2:balance +100   â”‚ sur toutes les clÃ©s
    EXEC                            â”˜

    Toutes les commandes s'exÃ©cutent sur le mÃªme serveur
    â†’ AtomicitÃ© garantie âœ“


Redis Cluster (limitation stricte) :
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    MULTI
    SET account:1:balance 1000      â†’ Slot 1234 (Node A)
    SET account:2:balance 500       â†’ Slot 5678 (Node B)  â† DiffÃ©rent !
    EXEC

    RÃ©sultat : âŒ CROSSSLOT

    Les transactions ne peuvent affecter que des clÃ©s du MÃŠME slot


Transaction valide dans un cluster :
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    MULTI
    SET {account:1}:balance 1000       â”
    SET {account:1}:email "a@x.com"    â”‚ Toutes les clÃ©s
    INCR {account:1}:login_count       â”‚ hash "account:1"
    EXEC                               â”˜ â†’ MÃªme slot

    Toutes les clÃ©s forcÃ©es sur le mÃªme slot via hash tag
    â†’ Transaction exÃ©cutÃ©e atomiquement sur Node A âœ“
```

### Exemples de transactions

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TRANSACTIONS DANS REDIS CLUSTER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


# EXEMPLE 1 : Transaction cross-slot (Ã‰CHEC)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

127.0.0.1:6379> MULTI
OK

127.0.0.1:6379> SET user:1000:name "Alice"
QUEUED

127.0.0.1:6379> SET user:2000:name "Bob"
QUEUED

127.0.0.1:6379> EXEC
(error) CROSSSLOT Keys in request don't hash to the same slot

# Les commandes ont Ã©tÃ© QUEUED mais pas exÃ©cutÃ©es


# EXEMPLE 2 : Transaction avec hash tags (SUCCÃˆS)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

127.0.0.1:6379> MULTI
OK

127.0.0.1:6379> SET {user:1000}:name "Alice"
QUEUED

127.0.0.1:6379> SET {user:1000}:email "alice@example.com"
QUEUED

127.0.0.1:6379> INCR {user:1000}:visit_count
QUEUED

127.0.0.1:6379> SADD {user:1000}:tags "premium" "active"
QUEUED

127.0.0.1:6379> EXEC
1) OK
2) OK
3) (integer) 1
4) (integer) 2

# Toutes les opÃ©rations exÃ©cutÃ©es atomiquement âœ“


# EXEMPLE 3 : Transaction avec WATCH (Optimistic Locking)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# TransfÃ©rer de l'argent entre deux comptes (mÃªme utilisateur)

127.0.0.1:6379> SET {user:1000}:checking 1000
OK

127.0.0.1:6379> SET {user:1000}:savings 500
OK

# Observer les comptes pour optimistic locking
127.0.0.1:6379> WATCH {user:1000}:checking {user:1000}:savings
OK

# VÃ©rifier les soldes
127.0.0.1:6379> GET {user:1000}:checking
"1000"

127.0.0.1:6379> GET {user:1000}:savings
"500"

# DÃ©marrer la transaction
127.0.0.1:6379> MULTI
OK

127.0.0.1:6379> DECRBY {user:1000}:checking 100
QUEUED

127.0.0.1:6379> INCRBY {user:1000}:savings 100
QUEUED

127.0.0.1:6379> EXEC
1) (integer) 900
2) (integer) 600

# Transaction exÃ©cutÃ©e avec succÃ¨s âœ“


# EXEMPLE 4 : Transaction cross-users (IMPOSSIBLE)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# TransfÃ©rer de l'argent entre deux utilisateurs diffÃ©rents
# IMPOSSIBLE avec une transaction atomique dans cluster

127.0.0.1:6379> SET {user:1000}:balance 1000
OK

127.0.0.1:6379> SET {user:2000}:balance 500
OK

# VÃ©rifier les slots
127.0.0.1:6379> CLUSTER KEYSLOT {user:1000}:balance
(integer) 5649

127.0.0.1:6379> CLUSTER KEYSLOT {user:2000}:balance
(integer) 7598  â† Slots diffÃ©rents !

# Transaction impossible
127.0.0.1:6379> MULTI
OK

127.0.0.1:6379> DECRBY {user:1000}:balance 100
QUEUED

127.0.0.1:6379> INCRBY {user:2000}:balance 100
QUEUED

127.0.0.1:6379> EXEC
(error) CROSSSLOT Keys in request don't hash to the same slot


# WORKAROUND : Deux transactions sÃ©parÃ©es (SANS atomicitÃ© globale)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Transaction 1 : DÃ©bit
127.0.0.1:6379> DECRBY {user:1000}:balance 100
(integer) 900

# Transaction 2 : CrÃ©dit
127.0.0.1:6379> INCRBY {user:2000}:balance 100
(integer) 600

# ATTENTION : Pas d'atomicitÃ© entre les deux opÃ©rations !
# Si le serveur crash entre les deux, incohÃ©rence possible
```

### Limitations du WATCH en cluster

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# WATCH ET OPTIMISTIC LOCKING EN CLUSTER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


# WATCH fonctionne uniquement pour clÃ©s sur le mÃªme nÅ“ud
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Cas 1 : WATCH multi-slot (FONCTIONNE mais avec limitations)
127.0.0.1:6379> WATCH user:1000 user:2000
OK

# WATCH accepte les clÃ©s multi-slots MAIS :
# 1. Chaque WATCH est local au nÅ“ud
# 2. Pas de coordination entre nÅ“uds
# 3. Race conditions possibles si accÃ¨s concurrent sur diffÃ©rents nÅ“uds


# Cas 2 : WATCH avec hash tags (RECOMMANDÃ‰)
127.0.0.1:6379> WATCH {user:1000}:balance {user:1000}:credit_limit
OK

# Garantit que WATCH surveille le mÃªme nÅ“ud
# â†’ Optimistic locking fiable âœ“


# WATCH cross-slot : Danger de race condition
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Thread 1 (Node A)                 # Thread 2 (Node B)
WATCH {user:1000}:balance           WATCH {user:2000}:balance
GET {user:1000}:balance             GET {user:2000}:balance
# balance = 1000                    # balance = 500

MULTI                                MULTI
DECRBY {user:1000}:balance 100      INCRBY {user:2000}:balance 100
EXEC âœ“                              EXEC âœ“

# Les deux transactions rÃ©ussissent indÃ©pendamment
# Mais pas d'atomicitÃ© globale entre user:1000 et user:2000
# Possible : dÃ©bit rÃ©ussi mais crÃ©dit Ã©choue (ou inversement)
```

## Limitation 3 : Scripts Lua

### Restrictions sur les scripts Lua

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Scripts Lua dans Redis Cluster                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚ RESTRICTION 1 : Toutes les clÃ©s sur le mÃªme nÅ“ud            â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•               â”‚
â”‚                                                             â”‚
â”‚ â€¢ Toutes les clÃ©s accessibles par le script doivent Ãªtre    â”‚
â”‚   sur le mÃªme nÅ“ud/slot                                     â”‚
â”‚ â€¢ Les clÃ©s doivent Ãªtre passÃ©es explicitement en argument   â”‚
â”‚   (KEYS array)                                              â”‚
â”‚ â€¢ Redis vÃ©rifie les hash slots avant d'exÃ©cuter le script   â”‚
â”‚                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚ RESTRICTION 2 : Pas de clÃ©s dynamiques                      â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                      â”‚
â”‚                                                             â”‚
â”‚ Interdit :                                                  â”‚
â”‚ ```lua                                                      â”‚
â”‚ local key = "user:" .. ARGV[1]                              â”‚
â”‚ return redis.call("GET", key)  â† ClÃ© construite dynamiquement
â”‚ ```                                                         â”‚
â”‚                                                             â”‚
â”‚ Redis ne peut pas vÃ©rifier le slot au prÃ©alable             â”‚
â”‚                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚ RESTRICTION 3 : KEYS doit contenir toutes les clÃ©s          â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•             â”‚
â”‚                                                             â”‚
â”‚ Correct :                                                   â”‚
â”‚ ```lua                                                      â”‚
â”‚ local balance = redis.call("GET", KEYS[1])                  â”‚
â”‚ redis.call("SET", KEYS[2], balance * 2)                     â”‚
â”‚ ```                                                         â”‚
â”‚                                                             â”‚
â”‚ Appel : EVAL script 2 {user:1000}:balance {user:1000}:doubleâ”‚
â”‚                      â†‘                                      â”‚
â”‚                      Nombre de clÃ©s (KEYS)                  â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Exemples de scripts Lua

```lua
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- SCRIPTS LUA DANS REDIS CLUSTER
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


-- EXEMPLE 1 : Script valide avec hash tags
-- â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

-- Script : Transfert intra-utilisateur
local checking = redis.call("GET", KEYS[1])
local savings = redis.call("GET", KEYS[2])
local amount = tonumber(ARGV[1])

if tonumber(checking) < amount then
    return {err = "Insufficient funds"}
end

redis.call("DECRBY", KEYS[1], amount)
redis.call("INCRBY", KEYS[2], amount)

return {ok = "Transfer successful"}


-- Appel depuis redis-cli
```

```bash
redis-cli -c -h 192.168.1.10 --eval transfer.lua \
    {user:1000}:checking {user:1000}:savings , 100
#   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”˜
#   KEYS (mÃªme hash tag)                     ARGV

# RÃ©sultat : {ok = "Transfer successful"} âœ“
```

```lua
-- EXEMPLE 2 : Script invalide (clÃ©s non co-localisÃ©es)
-- â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

-- Script : Transfert inter-utilisateurs (IMPOSSIBLE)
local balance1 = redis.call("GET", KEYS[1])
local balance2 = redis.call("GET", KEYS[2])
local amount = tonumber(ARGV[1])

redis.call("DECRBY", KEYS[1], amount)
redis.call("INCRBY", KEYS[2], amount)

return "OK"
```

```bash
# Appel
redis-cli --eval transfer_users.lua \
    {user:1000}:balance {user:2000}:balance , 100

# RÃ©sultat : (error) CROSSSLOT Keys in request don't hash to the same slot
```

```lua
-- EXEMPLE 3 : Script avec clÃ©s dynamiques (DANGEREUX)
-- â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

-- Ce script peut Ã©chouer en cluster
local user_id = ARGV[1]
local key = "user:" .. user_id .. ":balance"
return redis.call("GET", key)  -- âŒ ClÃ© construite dynamiquement
```

```bash
# Appel
redis-cli EVAL "..." 0 1000

# ProblÃ¨me : Redis ne peut pas vÃ©rifier le slot avant exÃ©cution
# Peut fonctionner ou Ã©chouer selon oÃ¹ le script est exÃ©cutÃ©
```

```lua
-- EXEMPLE 4 : Script correct avec toutes les clÃ©s en KEYS
-- â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

-- IncrÃ©menter un compteur avec limite
local current = tonumber(redis.call("GET", KEYS[1]) or "0")
local limit = tonumber(ARGV[1])

if current >= limit then
    return {err = "Limit reached"}
end

local new_value = redis.call("INCR", KEYS[1])
redis.call("EXPIRE", KEYS[1], ARGV[2])  -- TTL
return new_value
```

```bash
# Appel correct
redis-cli EVAL "..." 1 {ratelimit}:api:user:1000 100 3600
#                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#                     KEYS[1] avec hash tag

# Fonctionne âœ“ car une seule clÃ©, sur un seul slot
```

## Limitation 4 : Autres contraintes importantes

### SELECT database (toujours DB 0)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Bases de donnÃ©es (DB) en cluster               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Redis Standalone :
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â€¢ 16 databases (DB 0 Ã  DB 15) par dÃ©faut
â€¢ SELECT permet de basculer entre DB
â€¢ Isolation des donnÃ©es par DB

    127.0.0.1:6379> SELECT 1
    OK
    127.0.0.1:6379[1]> SET key "value in DB 1"
    OK


Redis Cluster :
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â€¢ Une seule database : DB 0 (toujours)
â€¢ SELECT est ignorÃ© ou retourne erreur

    127.0.0.1:6379> SELECT 1
    (error) ERR SELECT is not allowed in cluster mode

â€¢ Toutes les clÃ©s sont dans DB 0


Impact :
â•â•â•â•â•â•â•â•
Applications multi-tenant utilisant SELECT doivent :
â”œâ”€ PrÃ©fixer les clÃ©s avec tenant ID
â”œâ”€ Utiliser hash tags pour isolation
â””â”€ Exemple : {tenant:acme}:users vs {tenant:corp}:users


Alternative :
â•â•â•â•â•â•â•â•â•â•â•â•â•
â€¢ Utiliser plusieurs clusters Redis sÃ©parÃ©s
â€¢ Un cluster par tenant ou environnement
```

### Pub/Sub classique (broadcast Ã  tous les nÅ“uds)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Pub/Sub dans Redis Cluster                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ProblÃ¨me avec Pub/Sub classique :
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â€¢ Les messages Pub/Sub sont BROADCAST Ã  TOUS les nÅ“uds
â€¢ Pas de sharding des channels
â€¢ Overhead rÃ©seau Ã©levÃ© si nombreux messages


ScÃ©nario :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    Publisher                      Subscribers
       â”‚                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚  PUBLISH news "hello"   â”‚ Client A   â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚ (Node A)   â”‚
                â”‚                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚ Client B   â”‚
                â”‚                â”‚ (Node B)   â”‚
                â”‚                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚ Client C   â”‚
                                 â”‚ (Node C)   â”‚
                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â€¢ Le message est propagÃ© Ã  tous les nÅ“uds du cluster
â€¢ MÃªme si seul Client B a souscrit au channel "news"
â€¢ Overhead : 2 messages inutiles (vers Node A et C)


Solution Redis 7+ : Sharded Pub/Sub
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â€¢ SSUBSCRIBE / SPUBLISH
â€¢ Les channels sont shardÃ©s (comme les clÃ©s)
â€¢ Messages envoyÃ©s uniquement au nÅ“ud responsable du channel

    SPUBLISH {news}:tech "hello"
              â””â”€â”€â”€â”€â”€â”€â”˜
              Hash tag â†’ Slot â†’ NÅ“ud spÃ©cifique

â€¢ RÃ©duit l'overhead rÃ©seau
â€¢ ScalabilitÃ© amÃ©liorÃ©e


Recommandation :
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â€¢ Pub/Sub classique : OK pour faible volume
â€¢ Sharded Pub/Sub : RecommandÃ© pour forte charge
â€¢ Kafka/RabbitMQ : Si besoins avancÃ©s (persistence, ordering, etc.)
```

### Scan vs Keys (comportement diffÃ©rent)

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SCAN DANS REDIS CLUSTER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


# Redis Standalone : SCAN global
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

127.0.0.1:6379> SCAN 0 MATCH user:* COUNT 100
1) "128"     # Cursor pour prochaine itÃ©ration
2) 1) "user:1000"
   2) "user:1001"
   3) "user:1002"
   ...


# Redis Cluster : SCAN par nÅ“ud
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# SCAN ne scanne que le nÅ“ud LOCAL
127.0.0.1:6379> SCAN 0 MATCH user:* COUNT 100
1) "0"       # Fin du scan pour CE nÅ“ud
2) 1) "user:1234"
   2) "user:5678"

# Pour scanner tout le cluster, faire SCAN sur CHAQUE nÅ“ud

# Script pour scanner tout le cluster
#!/bin/bash

NODES=(
    "192.168.1.10:6379"
    "192.168.1.11:6379"
    "192.168.1.12:6379"
)

for node in "${NODES[@]}"; do
    echo "Scanning node $node..."
    redis-cli -h ${node%:*} -p ${node#*:} --scan --pattern "user:*"
done


# Alternative : Client intelligent
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Clients comme redis-py-cluster gÃ¨rent automatiquement
# le scan de tous les nÅ“uds

from rediscluster import RedisCluster

cluster = RedisCluster(startup_nodes=[{"host": "192.168.1.10", "port": 6379}])

# Scan automatique de tous les nÅ“uds
for key in cluster.scan_iter(match="user:*", count=100):
    print(key)
```

### INFO et commandes de monitoring

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# COMMANDES D'ADMINISTRATION EN CLUSTER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


# INFO : Par nÅ“ud uniquement
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# INFO retourne stats du nÅ“ud LOCAL uniquement
redis-cli -h 192.168.1.10 INFO memory
# used_memory:1234567890  â† Seulement ce nÅ“ud

# Pour stats globales du cluster, agrÃ©ger manuellement
total_memory=0
for node in 192.168.1.10 192.168.1.11 192.168.1.12; do
    mem=$(redis-cli -h $node INFO memory | grep "used_memory:" | cut -d: -f2)
    total_memory=$((total_memory + mem))
done
echo "Total cluster memory: $total_memory bytes"


# DBSIZE : Par nÅ“ud
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

redis-cli -h 192.168.1.10 DBSIZE
(integer) 10000  â† ClÃ©s sur ce nÅ“ud seulement

# Total cluster = somme de DBSIZE de tous les nÅ“uds


# CLIENT LIST : Par nÅ“ud
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

redis-cli -h 192.168.1.10 CLIENT LIST
# Liste uniquement les clients connectÃ©s Ã  ce nÅ“ud


# SLOWLOG : Par nÅ“ud
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

redis-cli -h 192.168.1.10 SLOWLOG GET 10
# Slow queries sur ce nÅ“ud uniquement

# Pour analyse globale, collecter de tous les nÅ“uds
```

### FLUSHDB / FLUSHALL

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FLUSH EN CLUSTER (DANGER !)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


# FLUSHDB / FLUSHALL : Par nÅ“ud uniquement
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# FLUSHALL sur un nÅ“ud vide SEULEMENT ce nÅ“ud
redis-cli -h 192.168.1.10 FLUSHALL
OK

# Les donnÃ©es des autres nÅ“uds (B, C) restent intactes !


# Pour vider tout le cluster : ExÃ©cuter sur TOUS les nÅ“uds
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

for node in 192.168.1.10 192.168.1.11 192.168.1.12; do
    redis-cli -h $node FLUSHALL
done

# ATTENTION : Aucune confirmation, donnÃ©es perdues immÃ©diatement


# Bonne pratique : Renommer FLUSHALL
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Dans redis.conf
rename-command FLUSHALL ""
rename-command FLUSHDB ""

# EmpÃªche exÃ©cution accidentelle
```

## Workarounds et stratÃ©gies d'adaptation

### StratÃ©gie 1 : Hash Tags systÃ©matiques

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Patterns de Hash Tags recommandÃ©s                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚ 1. PAR ENTITÃ‰ MÃ‰TIER                                        â”‚
â”‚    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                      â”‚
â”‚    {user:1000}:profile                                      â”‚
â”‚    {user:1000}:settings                                     â”‚
â”‚    {user:1000}:sessions                                     â”‚
â”‚    {user:1000}:friends:list                                 â”‚
â”‚                                                             â”‚
â”‚    Avantage : Toutes les donnÃ©es d'un user sur mÃªme nÅ“ud    â”‚
â”‚    InconvÃ©nient : Hotspots si certains users trÃ¨s actifs    â”‚
â”‚                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚ 2. PAR FEATURE/MODULE                                       â”‚
â”‚    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                   â”‚
â”‚    {cart}:user:1000                                         â”‚
â”‚    {cart}:user:1001                                         â”‚
â”‚    {sessions}:user:1000                                     â”‚
â”‚    {sessions}:user:1001                                     â”‚
â”‚                                                             â”‚
â”‚    Avantage : Isolation par fonctionnalitÃ©                  â”‚
â”‚    InconvÃ©nient : DonnÃ©es d'un user dispersÃ©es              â”‚
â”‚                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚ 3. PAR TENANT (Multi-tenancy)                               â”‚
â”‚    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                              â”‚
â”‚    {tenant:acme}:users                                      â”‚
â”‚    {tenant:acme}:products                                   â”‚
â”‚    {tenant:corp}:users                                      â”‚
â”‚    {tenant:corp}:products                                   â”‚
â”‚                                                             â”‚
â”‚    Avantage : Isolation complÃ¨te par tenant                 â”‚
â”‚    InconvÃ©nient : Distribution inÃ©gale si tenants de tailles variÃ©es
â”‚                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚ 4. PAR FENÃŠTRE TEMPORELLE                                   â”‚
â”‚    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                               â”‚
â”‚    {analytics:2024-12-11}:pageviews                         â”‚
â”‚    {analytics:2024-12-11}:clicks                            â”‚
â”‚    {analytics:2024-12-11}:conversions                       â”‚
â”‚                                                             â”‚
â”‚    Avantage : AgrÃ©gations temporelles efficaces             â”‚
â”‚    InconvÃ©nient : Hotspots sur pÃ©riode courante             â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### StratÃ©gie 2 : Denormalization et duplication

```
Accepter la duplication pour Ã©viter opÃ©rations multi-clÃ©s
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Exemple : Compteurs d'amis
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Approche normalisÃ©e (problÃ©matique en cluster) :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SET user:1000:name "Alice"
SADD friendships:1000 "2000" "3000" "4000"
    â†“
Pour afficher profil avec nombre d'amis :
    GET user:1000:name
    SCARD friendships:1000
    â†“ Deux clÃ©s potentiellement sur slots diffÃ©rents


Approche dÃ©normalisÃ©e (adaptÃ©e au cluster) :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

HSET {user:1000}:profile \
    name "Alice" \
    friend_count 3 \
    email "alice@example.com"

SADD {user:1000}:friends "2000" "3000" "4000"
    â†“
Toutes les donnÃ©es sur le mÃªme slot
Une transaction peut atomiquement :
    MULTI
    HSET {user:1000}:profile friend_count 4
    SADD {user:1000}:friends "5000"
    EXEC


Trade-off :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ“ Performance : Lecture en une requÃªte
âœ“ AtomicitÃ© : Transaction possible
âœ— Duplication : friend_count stockÃ© 2 fois
âœ— CohÃ©rence : Doit maintenir manuellement
```

### StratÃ©gie 3 : Application-Level Transactions

```javascript
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// TRANSACTIONS MULTI-NÅ’UDS CÃ”TÃ‰ APPLICATION
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// Contexte : Transfert d'argent entre deux utilisateurs
// Impossible avec MULTI/EXEC (slots diffÃ©rents)


// Pattern 1 : Two-Phase sans rollback automatique
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async function transferMoney(fromUser, toUser, amount) {
    const fromKey = `{user:${fromUser}}:balance`;
    const toKey = `{user:${toUser}}:balance`;

    try {
        // Phase 1 : DÃ©bit
        const balanceFrom = await redis.get(fromKey);
        if (parseInt(balanceFrom) < amount) {
            throw new Error("Insufficient funds");
        }

        await redis.decrby(fromKey, amount);
        console.log(`Debited ${amount} from ${fromUser}`);

        // Phase 2 : CrÃ©dit
        await redis.incrby(toKey, amount);
        console.log(`Credited ${amount} to ${toUser}`);

        return { success: true };

    } catch (error) {
        // PROBLÃˆME : Pas de rollback automatique
        // Si crÃ©dit Ã©choue, le dÃ©bit est dÃ©jÃ  effectuÃ© !
        console.error("Transfer failed:", error);

        // Compensation manuelle nÃ©cessaire
        // await redis.incrby(fromKey, amount); // Rollback manuel

        return { success: false, error: error.message };
    }
}


// Pattern 2 : Avec log de transaction (pour recovery)
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async function transferMoneyWithLog(fromUser, toUser, amount) {
    const txId = generateTransactionId();
    const txKey = `transaction:${txId}`;

    try {
        // CrÃ©er log de transaction
        await redis.hset(txKey,
            'status', 'PENDING',
            'from', fromUser,
            'to', toUser,
            'amount', amount,
            'timestamp', Date.now()
        );

        // Phase 1 : DÃ©bit
        await redis.decrby(`{user:${fromUser}}:balance`, amount);
        await redis.hset(txKey, 'status', 'DEBITED');

        // Phase 2 : CrÃ©dit
        await redis.incrby(`{user:${toUser}}:balance`, amount);
        await redis.hset(txKey, 'status', 'COMPLETED');

        // Supprimer le log (transaction rÃ©ussie)
        await redis.del(txKey);

        return { success: true, txId };

    } catch (error) {
        // Marquer comme Ã©chouÃ©e
        await redis.hset(txKey, 'status', 'FAILED', 'error', error.message);

        // Un job background peut nettoyer les transactions Ã©chouÃ©es
        return { success: false, txId, error: error.message };
    }
}


// Pattern 3 : Optimistic Locking multi-clÃ©s
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async function transferMoneyOptimistic(fromUser, toUser, amount) {
    const fromKey = `{user:${fromUser}}:balance`;
    const toKey = `{user:${toUser}}:balance`;

    // Lecture initiale
    const balanceFrom = await redis.get(fromKey);
    const balanceTo = await redis.get(toKey);

    if (parseInt(balanceFrom) < amount) {
        throw new Error("Insufficient funds");
    }

    // Version-based optimistic locking
    const versionFrom = await redis.get(`${fromKey}:version`);
    const versionTo = await redis.get(`${toKey}:version`);

    // Tenter mise Ã  jour avec vÃ©rification de version
    const pipeline = redis.pipeline();

    // VÃ©rifier + modifier source
    pipeline.watch(`${fromKey}:version`);
    pipeline.get(`${fromKey}:version`);

    const result = await pipeline.exec();

    if (result[1][1] !== versionFrom) {
        throw new Error("Concurrent modification detected on source");
    }

    // Similaire pour destination...
    // (Code simplifiÃ© pour illustration)

    return { success: true };
}
```

### StratÃ©gie 4 : Utiliser Redis Streams pour coordination

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# COORDINATION MULTI-NÅ’UDS VIA REDIS STREAMS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


# Pattern : Event Sourcing pour opÃ©rations cross-slot
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Au lieu d'une transaction atomique, Ã©mettre des events

# Event 1 : Demande de transfert
XADD transfer:requests * \
    from user:1000 \
    to user:2000 \
    amount 100 \
    status PENDING

# Consumer 1 traite le dÃ©bit
XREADGROUP GROUP processors consumer1 COUNT 1 STREAMS transfer:requests >
# Process: DECRBY {user:1000}:balance 100
# Emit: XADD transfer:events * tx_id 12345 status DEBITED

# Consumer 2 traite le crÃ©dit
XREADGROUP GROUP processors consumer2 COUNT 1 STREAMS transfer:events >
# Process: INCRBY {user:2000}:balance 100
# Emit: XADD transfer:events * tx_id 12345 status COMPLETED


# Avantages :
# â”œâ”€ Pas de blocage entre nÅ“uds
# â”œâ”€ TraÃ§abilitÃ© complÃ¨te (audit log)
# â”œâ”€ Retry automatique en cas d'Ã©chec
# â””â”€ ScalabilitÃ© (consumer groups)

# InconvÃ©nients :
# â”œâ”€ CohÃ©rence Ã©ventuelle (pas immÃ©diate)
# â”œâ”€ ComplexitÃ© accrue
# â””â”€ NÃ©cessite gestion des compensations
```

## Comparaison avec d'autres systÃ¨mes

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Redis Cluster vs Autres SystÃ¨mes DistribuÃ©s            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚ REDIS CLUSTER                                               â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•                                               â”‚
â”‚ âœ“ Performance : Ultra-rapide (in-memory)                    â”‚
â”‚ âœ“ DisponibilitÃ© : Haute (failover automatique)              â”‚
â”‚ âœ— Transactions : LimitÃ©es Ã  un nÅ“ud                         â”‚
â”‚ âœ— CohÃ©rence : Ã‰ventuelle uniquement                         â”‚
â”‚ âœ— Operations multi-clÃ©s : Hash tags requis                  â”‚
â”‚                                                             â”‚
â”‚ Use case : Cache distribuÃ©, session store, leaderboards     â”‚
â”‚                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚ MONGODB (SHARDED)                                           â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                          â”‚
â”‚ âœ“ Transactions multi-documents : Oui (depuis 4.0)           â”‚
â”‚ âœ“ OpÃ©rations multi-clÃ©s : Sans limitation                   â”‚
â”‚ âœ“ RequÃªtes complexes : AgrÃ©gations puissantes               â”‚
â”‚ âœ— Performance : Plus lent que Redis (sur disque)            â”‚
â”‚ âœ— ComplexitÃ© : Sharding plus complexe                       â”‚
â”‚                                                             â”‚
â”‚ Use case : Base de donnÃ©es documentaire avec transactions   â”‚
â”‚                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚ CASSANDRA                                                   â”‚
â”‚ â•â•â•â•â•â•â•â•â•                                                   â”‚
â”‚ âœ“ ScalabilitÃ© : Excellente (linear scaling)                 â”‚
â”‚ âœ“ DisponibilitÃ© : TrÃ¨s haute (no SPOF)                      â”‚
â”‚ âœ— Transactions : TrÃ¨s limitÃ©es (LWT seulement)              â”‚
â”‚ âœ— CohÃ©rence : Tunable mais souvent Ã©ventuelle               â”‚
â”‚ âœ— Latence : Plus Ã©levÃ©e que Redis                           â”‚
â”‚                                                             â”‚
â”‚ Use case : Time-series, IoT, analytics Ã  trÃ¨s grande Ã©chelleâ”‚
â”‚                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚ POSTGRESQL (avec Citus)                                     â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                     â”‚
â”‚ âœ“ Transactions : ACID complÃ¨tes                             â”‚
â”‚ âœ“ OpÃ©rations multi-clÃ©s : Toutes supportÃ©es                 â”‚
â”‚ âœ“ RequÃªtes : SQL standard                                   â”‚
â”‚ âœ— Performance : Pas in-memory                               â”‚
â”‚ âœ— ScalabilitÃ© : Plus limitÃ©e que Redis Cluster              â”‚
â”‚                                                             â”‚
â”‚ Use case : Base transactionnelle avec sharding              â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Checklist de dÃ©cision architecturale

```
Avant d'utiliser Redis Cluster, se poser ces questions :
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â˜ Mon application nÃ©cessite-t-elle des transactions ACID strictes ?
  â””â”€> Si OUI : ConsidÃ©rer PostgreSQL ou MongoDB

â˜ Ai-je besoin d'opÃ©rations multi-clÃ©s frÃ©quentes sur clÃ©s non liÃ©es ?
  â””â”€> Si OUI : Restructurer donnÃ©es ou considÃ©rer autre systÃ¨me

â˜ Puis-je modÃ©liser mes donnÃ©es avec hash tags de co-localisation ?
  â””â”€> Si OUI : Redis Cluster viable

â˜ Puis-je accepter la cohÃ©rence Ã©ventuelle ?
  â””â”€> Si NON : ConsidÃ©rer base traditionnelle avec ACID

â˜ La performance sub-milliseconde est-elle critique ?
  â””â”€> Si OUI : Redis Cluster excellent choix

â˜ Ai-je besoin de SELECT multi-database ?
  â””â”€> Si OUI : Redis standalone ou refactorer avec prÃ©fixes

â˜ Mon cas d'usage est-il principalement lecture ?
  â””â”€> Si OUI : Redis Cluster avec replicas idÃ©al

â˜ Ai-je besoin de Pub/Sub Ã  haute frÃ©quence ?
  â””â”€> Si OUI : Utiliser Sharded Pub/Sub (Redis 7+) ou Kafka

â˜ Puis-je tolÃ©rer une fenÃªtre de perte de donnÃ©es (~secondes) ?
  â””â”€> Si NON : Configurer persistence stricte + WAIT
```

## Conclusion

Les limitations de Redis Cluster ne sont pas des bugs, mais des consÃ©quences inÃ©vitables de choix architecturaux privilÃ©giant performance, disponibilitÃ© et tolÃ©rance aux pannes. Comprendre ces limitations est essentiel pour :

1. **Concevoir correctement** : ModÃ©liser donnÃ©es avec hash tags appropriÃ©s
2. **Ã‰viter les piÃ¨ges** : Ne pas tenter d'opÃ©rations impossibles
3. **Choisir judicieusement** : Utiliser Redis Cluster pour cas d'usage adaptÃ©s
4. **ImplÃ©menter proprement** : Utiliser workarounds quand nÃ©cessaire

Redis Cluster excelle pour :
- Cache distribuÃ© haute performance
- Session stores
- Leaderboards et compteurs
- Rate limiting
- Cas d'usage oÃ¹ donnÃ©es sont naturellement partitionnables

Redis Cluster n'est PAS adaptÃ© pour :
- Transactions complexes multi-entitÃ©s
- Applications nÃ©cessitant ACID strict
- OpÃ©rations analytiques complexes nÃ©cessitant JOINs

---

**Points clÃ©s Ã  retenir :**

- **CROSSSLOT** : Erreur quand clÃ©s sur slots diffÃ©rents
- **Hash tags `{...}`** : Solution pour co-localiser donnÃ©es liÃ©es
- **Transactions** : MULTI/EXEC limitÃ© Ã  un seul nÅ“ud
- **Scripts Lua** : Toutes clÃ©s doivent Ãªtre co-localisÃ©es
- **SELECT DB** : Pas supportÃ©, toujours DB 0
- **Pub/Sub** : Broadcast Ã  tous nÅ“uds (utiliser Sharded Pub/Sub si possible)
- **SCAN** : Par nÅ“ud uniquement, scanner tous pour vue complÃ¨te
- **Workarounds** : Hash tags, dÃ©normalisation, transactions app-level

La prochaine section (11.7) explorera le routing des requÃªtes (client-side vs proxy-based).

â­ï¸ [Client-side routing vs Proxy-based routing](/11-architecture-distribuee-scaling/07-client-side-vs-proxy-routing.md)
