ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 11.9 Cross-datacenter replication (Active-Active vs Active-Passive)

## Introduction

La rÃ©plication cross-datacenter (XDR - Cross-Datacenter Replication) constitue l'ultime niveau de rÃ©silience et de disponibilitÃ© pour Redis Cluster. Alors que la rÃ©plication intra-cluster assure la haute disponibilitÃ© face aux dÃ©faillances individuelles de nÅ“uds, la rÃ©plication gÃ©ographique protÃ¨ge contre des pannes de datacenters entiers (catastrophes naturelles, pannes rÃ©gionales cloud, coupures rÃ©seau majeures) tout en permettant de servir les utilisateurs depuis des emplacements gÃ©ographiquement proches pour rÃ©duire la latence.

Cette section explore les architectures Active-Passive et Active-Active, leurs implÃ©mentations techniques, leurs compromis en termes de cohÃ©rence et de disponibilitÃ©, ainsi que les dÃ©fis spÃ©cifiques Ã  la rÃ©plication gÃ©ographique distribuÃ©e.

## Contexte et motivations

### Pourquoi la rÃ©plication cross-datacenter ?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Motivations pour XDR (Cross-Datacenter Replication) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. DISASTER RECOVERY (DR)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Protection contre perte complÃ¨te d'un datacenter

ScÃ©narios :
â”œâ”€ Catastrophe naturelle (ouragan, tremblement de terre)
â”œâ”€ Panne Ã©lectrique rÃ©gionale prolongÃ©e
â”œâ”€ Incendie datacenter
â”œâ”€ Attaque physique / terrorisme
â””â”€ Erreur opÃ©rationnelle majeure (suppression zone AWS)

Objectifs :
â”œâ”€ RPO (Recovery Point Objective) : < 1 seconde de perte de donnÃ©es
â””â”€ RTO (Recovery Time Objective) : < 5 minutes de downtime


2. RÃ‰DUCTION DE LATENCE GÃ‰OGRAPHIQUE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Servir utilisateurs depuis DC proche

Sans XDR (tous utilisateurs â†’ DC unique) :
    User US East â†’ DC Europe â†’ 150ms latency
    User Asia â†’ DC Europe â†’ 300ms latency

Avec XDR (utilisateurs â†’ DC proche) :
    User US East â†’ DC US East â†’ 10ms latency
    User Europe â†’ DC Europe â†’ 10ms latency
    User Asia â†’ DC Asia â†’ 10ms latency

AmÃ©lioration : 10-30x rÃ©duction de latence


3. CONFORMITÃ‰ RÃ‰GLEMENTAIRE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Respect des lois de rÃ©sidence des donnÃ©es

RGPD (Europe) : DonnÃ©es citoyens EU doivent rester en EU
Lois chinoises : DonnÃ©es citoyens chinois en Chine
CLOUD Act (US) : Implications juridiques donnÃ©es hors US

Solution : DonnÃ©es rÃ©pliquÃ©es localement selon gÃ©olocalisation


4. HAUTE DISPONIBILITÃ‰ GÃ‰OGRAPHIQUE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
TolÃ©rance aux pannes rÃ©gionales

Single DC :
    Availability = 99.9% (8.76h downtime/an)

Multi-DC avec failover :
    Availability = 99.99% (52.6min downtime/an)

Multi-DC Active-Active :
    Availability = 99.999% (5.26min downtime/an)


5. SCALABILITÃ‰ GÃ‰OGRAPHIQUE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
CapacitÃ© lecture distribuÃ©e globalement

Single DC : 100k req/sec maximum
Multi-DC : N Ã— 100k req/sec (N datacenters)

Exemple : 3 DCs â†’ 300k req/sec capacitÃ© totale


6. ISOLATION DES PANNES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Blast radius limitÃ©

Panne dans DC Asia :
â”œâ”€ Sans XDR : Service global down
â””â”€ Avec XDR : Seulement users Asia affectÃ©s temporairement
              Users US/Europe continuent normalement
```

### Challenges spÃ©cifiques Ã  Redis Cluster XDR

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              DÃ©fis de la rÃ©plication gÃ©ographique           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. LATENCE WAN
â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Liaison inter-DC : 50-300ms (vs <1ms LAN)

Impact sur rÃ©plication synchrone :
    WRITE â†’ RÃ©plication vers DC distant â†’ 300ms
    â””â”€> Chaque Ã©criture prend 300ms minimum
        â””â”€> Throughput limitÃ©, latence inacceptable

Solution : RÃ©plication ASYNCHRONE obligatoire


2. THÃ‰ORÃˆME CAP Ã€ GRANDE Ã‰CHELLE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Partition rÃ©seau WAN plus frÃ©quente qu'en LAN

Network partition entre DCs :
â”œâ”€ RÃ©plication impossible temporairement
â”œâ”€ Divergence des donnÃ©es entre DCs
â””â”€ NÃ©cessitÃ© de gestion de conflits

Redis Cluster XDR choisit : AP (Availability + Partition tolerance)
    â””â”€> CohÃ©rence Ã©ventuelle (eventual consistency)


3. BANDE PASSANTE LIMITÃ‰E
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
WAN moins performant que LAN

Bande passante typique :
â”œâ”€ LAN : 10-100 Gbps
â””â”€ WAN : 1-10 Gbps

Impact :
â”œâ”€ CoÃ»t Ã©levÃ© du transfert (facturÃ© au volume)
â”œâ”€ Saturation possible lors de pics
â””â”€> Compression et optimisation nÃ©cessaires


4. ARCHITECTURE REDIS CLUSTER NON CONÃ‡UE POUR XDR
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Redis Cluster open-source :
â”œâ”€ Pas de rÃ©plication cross-cluster native
â”œâ”€ Chaque cluster est indÃ©pendant
â””â”€> Solutions tierces ou Redis Enterprise nÃ©cessaires


5. GESTION DES CONFLITS D'Ã‰CRITURE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Active-Active : Ã‰critures concurrentes sur mÃªme clÃ©

DC US : SET user:1000:name "Alice"  (t=0)
DC EU : SET user:1000:name "Alicia" (t=0)

AprÃ¨s rÃ©plication :
    DC US : name = "Alicia" (derniÃ¨re Ã©criture)
    DC EU : name = "Alice"  (derniÃ¨re Ã©criture)

    âœ— Inconsistance !

Solution : CRDT (Conflict-free Replicated Data Types)
           ou Last-Write-Wins (LWW) avec timestamps


6. COÃ›T FINANCIER
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Infrastructure multi-DC trÃ¨s coÃ»teuse

CoÃ»ts :
â”œâ”€ N Ã— Infrastructure (chaque DC = cluster complet)
â”œâ”€ Bande passante WAN facturÃ©e au volume
â”œâ”€ Licences Redis Enterprise (si utilisÃ©)
â””â”€> 3-5x le coÃ»t d'un dÃ©ploiement single-DC
```

## Architecture Active-Passive

### Principe et topologie

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Active-Passive (Master-Slave)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Principe : Un DC actif (Ã©critures + lectures), un DC passif (standby)


    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                   Datacenter ACTIF                   â”‚
    â”‚                    (Primary)                         â”‚
    â”‚                                                      â”‚
    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
    â”‚   â”‚  Node A  â”‚   â”‚  Node B  â”‚   â”‚  Node C  â”‚         â”‚
    â”‚   â”‚ Master   â”‚   â”‚ Master   â”‚   â”‚ Master   â”‚         â”‚
    â”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜         â”‚
    â”‚        â”‚              â”‚              â”‚               â”‚
    â”‚        â”‚    Redis Cluster (Primary)  â”‚               â”‚
    â”‚        â”‚              â”‚              â”‚               â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚              â”‚              â”‚
             â”‚              â”‚              â”‚
             â”‚   RÃ©plication WAN (async)   â”‚
             â”‚              â”‚              â”‚
             â–¼              â–¼              â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                 Datacenter PASSIF                      â”‚
    â”‚                   (Secondary/Standby)                  â”‚
    â”‚                                                        â”‚
    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
    â”‚   â”‚  Node D  â”‚   â”‚  Node E  â”‚   â”‚  Node F  â”‚           â”‚
    â”‚   â”‚  Replica â”‚   â”‚  Replica â”‚   â”‚  Replica â”‚           â”‚
    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
    â”‚                                                        â”‚
    â”‚             Redis Cluster (Secondary)                  â”‚
    â”‚             Receive only (pas d'Ã©critures)             â”‚
    â”‚                                                        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


Flux de donnÃ©es :
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. Toutes les Ã©critures â†’ DC Actif
2. DC Actif rÃ©plique vers DC Passif (asynchrone)
3. DC Passif applique les changements
4. DC Passif ne sert PAS les lectures (option : peut servir lectures)


Avantages :
â•â•â•â•â•â•â•â•â•â•â•
âœ“ Simple Ã  comprendre et implÃ©menter
âœ“ Pas de conflits d'Ã©criture (un seul master)
âœ“ CohÃ©rence plus forte (single source of truth)
âœ“ CoÃ»t modÃ©rÃ© (DC passif peut Ãªtre plus petit)
âœ“ Compatible avec Redis Cluster open-source (via outils tiers)


InconvÃ©nients :
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âœ— DC passif sous-utilisÃ© (gaspillage ressources)
âœ— Pas de rÃ©duction de latence pour utilisateurs distants
âœ— Failover manuel ou automatique nÃ©cessaire
âœ— RPO > 0 (perte possible de donnÃ©es rÃ©centes)
âœ— CapacitÃ© lecture limitÃ©e au DC actif
```

### ImplÃ©mentation avec Redis Cluster

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# IMPLÃ‰MENTATION ACTIVE-PASSIVE AVEC REDIS CLUSTER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ARCHITECTURE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# DC Primary (US-EAST) : Cluster 3 nodes (active)
# DC Secondary (EU-WEST) : Cluster 3 nodes (passive)


# MÃ‰THODE 1 : RÃ©plication au niveau RDB/AOF
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Sur chaque nÅ“ud du DC Primary, activer persistence
# redis.conf
appendonly yes
save 900 1
save 300 10

# Script de synchronisation continue
#!/bin/bash
# sync-to-secondary.sh

PRIMARY_NODES=("us-east-1a" "us-east-1b" "us-east-1c")
SECONDARY_NODES=("eu-west-1a" "eu-west-1b" "eu-west-1c")

while true; do
    for i in "${!PRIMARY_NODES[@]}"; do
        primary="${PRIMARY_NODES[$i]}"
        secondary="${SECONDARY_NODES[$i]}"

        # RDB sync
        scp "$primary:/var/lib/redis/dump.rdb" "/tmp/dump-$i.rdb"
        scp "/tmp/dump-$i.rdb" "$secondary:/var/lib/redis/dump.rdb"

        # RedÃ©marrer secondary pour charger nouveau RDB
        ssh "$secondary" "redis-cli shutdown nosave && systemctl start redis"
    done

    sleep 300  # Toutes les 5 minutes
done

# âš ï¸ Cette mÃ©thode est basique et a un RPO de ~5 minutes


# MÃ‰THODE 2 : Redis REPLICAOF avec tunneling
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# CrÃ©er un tunnel SSH/VPN entre DCs

# Sur chaque nÅ“ud Secondary, configurer comme replica du Primary

# Secondary Node D (EU-WEST-1A)
redis-cli -h eu-west-1a -p 6379 REPLICAOF us-east-1a 6379

# ProblÃ¨me : Redis Cluster n'est pas conÃ§u pour rÃ©plication cross-cluster
# Chaque nÅ“ud se rÃ©plique individuellement, pas de cohÃ©rence cluster


# MÃ‰THODE 3 : RIOT (Redis Input/Output Tools)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# RIOT : Outil open-source pour rÃ©plication Redis
# https://github.com/redis-developer/riot

# Installation
wget https://github.com/redis-developer/riot/releases/download/v3.1.0/riot-3.1.0.zip
unzip riot-3.1.0.zip

# RÃ©plication live Primary â†’ Secondary
./riot replicate \
    redis://us-east-1a:6379,us-east-1b:6379,us-east-1c:6379 \
    redis://eu-west-1a:6379,eu-west-1b:6379,eu-west-1c:6379 \
    --mode live \
    --batch-size 500

# Options :
# --mode live : RÃ©plication continue (vs snapshot)
# --mode snapshot : RÃ©plication ponctuelle
# --batch-size : Nombre de commandes par batch


# Avantages RIOT :
âœ“ Fonctionne avec Redis Cluster
âœ“ RÃ©plication continue (streaming)
âœ“ Open-source et gratuit
âœ“ Support RDB + AOF + Live streaming

# InconvÃ©nients RIOT :
âœ— Point de dÃ©faillance (si RIOT crash, rÃ©plication stop)
âœ— Latence additionnelle
âœ— Pas de gestion de failover automatique


# MÃ‰THODE 4 : RedisRaft (expÃ©rimental)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# RedisRaft : Module Redis implÃ©mentant Raft consensus
# Permet rÃ©plication multi-DC avec consensus

# âš ï¸ ExpÃ©rimental, non recommandÃ© pour production


# MÃ‰THODE 5 : Redis Enterprise (solution commerciale)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Redis Enterprise supporte nativement Active-Passive XDR

# Configuration via UI ou API
curl -X POST https://redis-enterprise-api/v1/crdb \
    -d '{
        "name": "my-database",
        "replication": "active-passive",
        "participating_clusters": [
            {"url": "redis://us-east-cluster"},
            {"url": "redis://eu-west-cluster", "passive": true}
        ]
    }'

# Avantages Redis Enterprise :
âœ“ RÃ©plication native et optimisÃ©e
âœ“ Failover automatique
âœ“ Compression WAN intÃ©grÃ©e
âœ“ Monitoring et observabilitÃ©
âœ“ Support commercial

# InconvÃ©nients :
âœ— CoÃ»t Ã©levÃ© (licence)
âœ— Vendor lock-in
```

### ProcÃ©dure de failover

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PROCÃ‰DURE DE FAILOVER : ACTIF â†’ PASSIF
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# SCÃ‰NARIO : DC Primary (US-EAST) est tombÃ©
#            Basculer vers DC Secondary (EU-WEST)


# Ã‰TAPE 1 : DÃ‰TECTER LA PANNE (automatique ou manuel)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Health check automatique
#!/bin/bash
# health-check.sh

PRIMARY_ENDPOINT="us-east-cluster.example.com:6379"

while true; do
    if ! redis-cli -h $PRIMARY_ENDPOINT PING > /dev/null 2>&1; then
        echo "PRIMARY DOWN - Initiating failover"
        /usr/local/bin/trigger-failover.sh
        break
    fi
    sleep 10
done


# Ã‰TAPE 2 : ARRÃŠTER LA RÃ‰PLICATION (Ã©viter corruption)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Si Primary est partiellement accessible, arrÃªter rÃ©plication
# Pour Ã©viter rÃ©plication de donnÃ©es corrompues

# Sur Secondary nodes
for node in eu-west-1a eu-west-1b eu-west-1c; do
    redis-cli -h $node REPLICAOF NO ONE
done


# Ã‰TAPE 3 : PROMOUVOIR SECONDARY EN PRIMARY
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Activer Ã©critures sur Secondary cluster
# Si Redis Enterprise, via API :
curl -X POST https://redis-api/v1/crdb/promote \
    -d '{"cluster": "eu-west-cluster"}'

# Si setup manuel, simplement autoriser Ã©critures :
# (pas de changement nÃ©cessaire si Secondary lisait dÃ©jÃ )


# Ã‰TAPE 4 : BASCULER LE TRAFIC (DNS ou Load Balancer)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Option A : DNS failover
# Changer enregistrement DNS pour pointer vers Secondary

aws route53 change-resource-record-sets \
    --hosted-zone-id Z1234567890ABC \
    --change-batch '{
        "Changes": [{
            "Action": "UPSERT",
            "ResourceRecordSet": {
                "Name": "redis.example.com",
                "Type": "CNAME",
                "TTL": 60,
                "ResourceRecords": [{"Value": "eu-west-cluster.example.com"}]
            }
        }]
    }'

# Attention : TTL DNS peut retarder basculement (60s ici)


# Option B : Global Load Balancer (ex: AWS Global Accelerator)
# DÃ©sactiver endpoint Primary, activer endpoint Secondary

aws globalaccelerator update-endpoint-group \
    --endpoint-group-arn arn:aws:... \
    --endpoint-configurations '[
        {
            "EndpointId": "eu-west-cluster",
            "Weight": 100
        }
    ]'

# Basculement instantanÃ© (pas de TTL DNS)


# Ã‰TAPE 5 : VÃ‰RIFIER LE FAILOVER
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Test Ã©criture/lecture depuis nouveau Primary
redis-cli -h eu-west-cluster.example.com SET test:failover "success"
redis-cli -h eu-west-cluster.example.com GET test:failover

# VÃ©rifier mÃ©triques
redis-cli -h eu-west-1a INFO replication
# role:master (devrait Ãªtre master maintenant)

redis-cli -h eu-west-1a INFO stats
# total_commands_processed (devrait augmenter)


# Ã‰TAPE 6 : Ã‰VALUER LA PERTE DE DONNÃ‰ES
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# RPO dÃ©pend de la latence de rÃ©plication avant panne

# VÃ©rifier AOF/RDB timestamp du Secondary
ls -lh /var/lib/redis/dump.rdb
# Si rÃ©plication Ã©tait Ã  10s prÃ¨s, RPO â‰ˆ 10 secondes


# Ã‰TAPE 7 : COMMUNICATION
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Informer stakeholders du failover

cat <<EOF
INCIDENT: Primary datacenter failure - Failover executed

Timeline:
- 14:00 UTC: Primary DC (US-EAST) unresponsive
- 14:02 UTC: Automatic failover triggered
- 14:05 UTC: Traffic routed to Secondary DC (EU-WEST)
- 14:06 UTC: Service restored

Impact:
- Downtime: 6 minutes
- Data loss: ~10 seconds (RPO met)
- Geographic shift: Users now served from EU-WEST

Next steps:
- Root cause analysis of Primary failure
- Plan reconstruction of Primary DC
- Monitor Secondary for stability
EOF


# Ã‰TAPE 8 : POST-FAILOVER (aprÃ¨s incident)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Une fois Primary DC rÃ©cupÃ©rÃ© :

# Option A : Reconstruire Primary comme nouveau Secondary
# 1. Configurer Primary nodes en replica de EU-WEST
# 2. Laisser sync complet
# 3. Une fois Ã  jour, dÃ©cider si re-failover vers Primary

# Option B : Garder EU-WEST comme nouveau Primary permanent
# (si latence acceptable pour utilisateurs US)
```

### Monitoring et mÃ©triques

```yaml
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MONITORING ACTIVE-PASSIVE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# MÃ©triques Prometheus
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# prometheus-rules-xdr.yml

groups:
  - name: redis_xdr_active_passive
    interval: 30s
    rules:

      # Replication lag (Ã©cart entre Primary et Secondary)
      - record: redis:replication:lag_seconds
        expr: |
          (redis_replication_last_io_seconds_ago{role="slave"})

      # Alerte : Lag Ã©levÃ©
      - alert: RedisXDRHighLag
        expr: redis:replication:lag_seconds > 60
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High replication lag between datacenters"
          description: "{{ $labels.instance }} is {{ $value }}s behind"

      # Alerte : RÃ©plication cassÃ©e
      - alert: RedisXDRReplicationDown
        expr: redis_master_link_status{role="slave"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "XDR replication link down"
          description: "{{ $labels.instance }} lost connection to master"

      # Alerte : Primary datacenter down
      - alert: RedisXDRPrimaryDown
        expr: up{job="redis-primary"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Primary datacenter unreachable"
          description: "Failover may be required"

      # Bande passante WAN
      - record: redis:xdr:bandwidth_mbps
        expr: |
          rate(redis_net_output_bytes_total{role="master"}[1m]) * 8 / 1000000

      # Alerte : Bande passante saturÃ©e
      - alert: RedisXDRBandwidthSaturated
        expr: redis:xdr:bandwidth_mbps > 800  # 80% de 1 Gbps
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "WAN bandwidth near saturation"


# Dashboard Grafana
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# grafana-xdr-dashboard.json (extrait)

{
  "panels": [
    {
      "title": "Replication Lag (Primary â†’ Secondary)",
      "targets": [{
        "expr": "redis:replication:lag_seconds"
      }],
      "yaxis": {
        "label": "Seconds"
      }
    },
    {
      "title": "WAN Bandwidth Usage",
      "targets": [{
        "expr": "redis:xdr:bandwidth_mbps"
      }],
      "yaxis": {
        "label": "Mbps"
      }
    },
    {
      "title": "Commands/sec by Datacenter",
      "targets": [
        {
          "expr": "rate(redis_commands_processed_total{dc='us-east'}[1m])",
          "legendFormat": "Primary (US-EAST)"
        },
        {
          "expr": "rate(redis_commands_processed_total{dc='eu-west'}[1m])",
          "legendFormat": "Secondary (EU-WEST)"
        }
      ]
    },
    {
      "title": "Replication Status",
      "targets": [{
        "expr": "redis_master_link_status{role='slave'}"
      }],
      "valueMaps": [
        {"value": "0", "text": "DOWN"},
        {"value": "1", "text": "UP"}
      ]
    }
  ]
}


# VÃ©rifications manuelles
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# VÃ©rifier rÃ©plication lag
redis-cli -h eu-west-1a INFO replication | grep master_repl_offset

# Sur Primary :
# master_repl_offset:123456789

# Sur Secondary :
# master_repl_offset:123456500

# Lag = 123456789 - 123456500 = 289 bytes de retard


# VÃ©rifier vitesse de rÃ©plication
redis-cli -h eu-west-1a INFO stats | grep instantaneous_input_kbps
# instantaneous_input_kbps:850.42  # RÃ©plication Ã  850 KB/s
```

## Architecture Active-Active

### Principe et dÃ©fis

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Active-Active (Multi-Master)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Principe : Chaque DC accepte lectures ET Ã©critures
          RÃ©plication bidirectionnelle entre tous les DCs


    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Datacenter US-EAST   â”‚       â”‚   Datacenter EU-WEST   â”‚
    â”‚       (Active)         â”‚       â”‚       (Active)         â”‚
    â”‚                        â”‚       â”‚                        â”‚
    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚       â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
    â”‚   â”‚  Cluster A   â”‚     â”‚       â”‚     â”‚  Cluster B   â”‚   â”‚
    â”‚   â”‚  (R/W)       â”‚â—„â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â–ºâ”‚  (R/W)       â”‚   â”‚
    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚       â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
    â”‚                        â”‚       â”‚                        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚                               â”‚
                 â”‚         RÃ©plication           â”‚
                 â”‚        bidirectionnelle       â”‚
                 â”‚                               â”‚
                 â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                 â”‚       â”‚  Datacenter ASIA   â”‚  â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”¤     (Active)       â”œâ”€â”€â”˜
                         â”‚                    â”‚
                         â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                         â”‚   â”‚  Cluster C   â”‚ â”‚
                         â”‚   â”‚  (R/W)       â”‚ â”‚
                         â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                         â”‚                    â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


RÃ©plication mesh (tous vers tous) :
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

US-EAST â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚                  â”‚
   â”‚                  â–¼
   â”‚              EU-WEST
   â”‚                  â”‚
   â”‚                  â”‚
   â–¼                  â–¼
 ASIA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> EU-WEST

Chaque changement est rÃ©pliquÃ© vers tous les autres DCs


Avantages :
â•â•â•â•â•â•â•â•â•â•â•
âœ“ Latence minimale (users â†’ DC proche)
âœ“ Haute disponibilitÃ© (perte 1 DC = service continue)
âœ“ CapacitÃ© lecture ET Ã©criture distribuÃ©e globalement
âœ“ Pas de failover nÃ©cessaire (tous actifs)
âœ“ ScalabilitÃ© gÃ©ographique maximale


InconvÃ©nients :
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âœ— COMPLEXITÃ‰ TRÃˆS Ã‰LEVÃ‰E
âœ— Gestion des conflits d'Ã©criture
âœ— CohÃ©rence Ã©ventuelle (eventual consistency)
âœ— CoÃ»t maximum (N Ã— infrastructure complÃ¨te)
âœ— Debugging difficile (Ã©tat distribuÃ©)
âœ— NÃ©cessite CRDT ou rÃ©solution de conflits


PROBLÃˆME CENTRAL : CONFLITS D'Ã‰CRITURE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Temps T=0 :
    US-EAST : SET user:1000:score 100
    EU-WEST : SET user:1000:score 200

Temps T=1 (aprÃ¨s rÃ©plication) :
    US-EAST reÃ§oit : user:1000:score = 200 (de EU-WEST)
    EU-WEST reÃ§oit : user:1000:score = 100 (de US-EAST)

Quelle valeur garder ? 100 ou 200 ?
    â””â”€> StratÃ©gie de rÃ©solution nÃ©cessaire !
```

### StratÃ©gies de rÃ©solution de conflits

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              StratÃ©gies de rÃ©solution de conflits           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


STRATÃ‰GIE 1 : LAST-WRITE-WINS (LWW)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Principe : L'Ã©criture avec le timestamp le plus rÃ©cent gagne


DC1 (t=100) : SET key "value1"
DC2 (t=105) : SET key "value2"

AprÃ¨s rÃ©plication :
    Les deux DCs convergent vers "value2" (t=105 > t=100)


ImplÃ©mentation :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Chaque Ã©criture inclut timestamp :

    SET key "value"
    â””â”€> StockÃ© comme : {value: "value", timestamp: 1638360000.123}

Lors de rÃ©plication :
    IF incoming_timestamp > local_timestamp:
        Accepter nouvelle valeur
    ELSE:
        Ignorer (garder valeur locale)


Avantages :
â”œâ”€ Simple Ã  implÃ©menter
â”œâ”€ Convergence garantie
â””â”€ Pas d'intervention manuelle

InconvÃ©nients :
â”œâ”€ Perte de donnÃ©es possible (Ã©criture "perdante" ignorÃ©e)
â”œâ”€ DÃ©pend de la synchronisation des horloges
â””â”€> DÃ©rive d'horloge = incohÃ©rence


STRATÃ‰GIE 2 : CRDT (Conflict-free Replicated Data Types)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Principe : Types de donnÃ©es mathÃ©matiquement conÃ§us pour
          converger sans conflits


Types CRDT courants :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. COUNTER (G-Counter, PN-Counter)
   â”œâ”€ Chaque DC maintient son propre compteur
   â””â”€ Total = somme de tous les compteurs

   DC1 : counter_dc1 = +10
   DC2 : counter_dc2 = +5
   Total : 10 + 5 = 15 âœ“

   MÃªme aprÃ¨s rÃ©plication dans n'importe quel ordre !


2. SET (OR-Set)
   â”œâ”€ Observe-Remove Set
   â””â”€ Ã‰lÃ©ments ont des IDs uniques

   DC1 : ADD("alice", uid=1)
   DC2 : ADD("bob", uid=2)

   RÃ©sultat convergent : {"alice", "bob"}


3. REGISTER (LWW-Register)
   â”œâ”€ Last-Write-Wins avec timestamp
   â””â”€> Similaire Ã  stratÃ©gie 1


4. MAP (OR-Map)
   â”œâ”€ Map avec rÃ©solution par champ
   â””â”€ Chaque champ suit sa propre stratÃ©gie CRDT


Exemple Redis Enterprise CRDB :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Redis Enterprise implÃ©mente CRDT nativement

SET key "value"  â†’  LWW-Register automatiquement
INCR counter     â†’  PN-Counter automatiquement
SADD set "item"  â†’  OR-Set automatiquement


Avantages :
â”œâ”€ Pas de perte de donnÃ©es
â”œâ”€ Convergence mathÃ©matiquement garantie
â”œâ”€ Pas de synchronisation d'horloge stricte nÃ©cessaire
â””â”€ IdÃ©al pour Active-Active

InconvÃ©nients :
â”œâ”€ ComplexitÃ© d'implÃ©mentation Ã©levÃ©e
â”œâ”€ Overhead mÃ©moire (mÃ©tadonnÃ©es CRDT)
â”œâ”€ Pas tous les types de donnÃ©es supportÃ©s
â””â”€> Redis Enterprise requis (ou implÃ©mentation custom)


STRATÃ‰GIE 3 : VECTOR CLOCKS / VERSION VECTORS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Principe : Chaque DC maintient un vecteur de versions


DC1 : [DC1:1, DC2:0, DC3:0]
DC2 : [DC1:0, DC2:1, DC3:0]

Ã‰criture sur DC1 â†’ [DC1:2, DC2:0, DC3:0]
Ã‰criture sur DC2 â†’ [DC1:0, DC2:2, DC3:0]

AprÃ¨s rÃ©plication :
    DÃ©tection de conflit si vecteurs incomparables
    â””â”€> RÃ©solution manuelle ou automatique nÃ©cessaire


Avantages :
â”œâ”€ DÃ©tection prÃ©cise des conflits
â””â”€ Pas de dÃ©pendance horloge

InconvÃ©nients :
â”œâ”€ Overhead mÃ©moire Ã©levÃ©
â”œâ”€ ComplexitÃ© de rÃ©solution
â””â”€> Rarement utilisÃ© avec Redis


STRATÃ‰GIE 4 : APPLICATION-LEVEL CONFLICT RESOLUTION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Principe : L'application dÃ©cide de la rÃ©solution


Exemple : Score de jeu
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

DC1 : user:1000:score = 100 (t=1)
DC2 : user:1000:score = 150 (t=2)

StratÃ©gie mÃ©tier : Toujours prendre le score le plus Ã©levÃ©

RÃ©solution custom :
    IF incoming_score > local_score:
        Accept
    ELSE:
        Reject

RÃ©sultat : score = 150 sur tous les DCs âœ“


Avantages :
â”œâ”€ Logique mÃ©tier prÃ©cise
â””â”€ Pas de perte de donnÃ©es mÃ©tier importantes

InconvÃ©nients :
â”œâ”€ ComplexitÃ© applicative
â”œâ”€ Chaque type de donnÃ©es nÃ©cessite sa logique
â””â”€> Maintenance difficile


STRATÃ‰GIE 5 : PARTITIONING (Ã‰viter conflits)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Principe : Chaque DC possÃ¨de des clÃ©s exclusives


Shard par gÃ©ographie :
    US users â†’ ClÃ©s US-EAST uniquement
    EU users â†’ ClÃ©s EU-WEST uniquement

    user:US:1000 â†’ US-EAST (jamais Ã©crit depuis EU)
    user:EU:2000 â†’ EU-WEST (jamais Ã©crit depuis US)

Pas de conflits car pas d'Ã©critures concurrentes !


Avantages :
â”œâ”€ Pas de conflits = pas de rÃ©solution
â”œâ”€ Simple
â””â”€ ConformitÃ© RGPD facilitÃ©

InconvÃ©nients :
â”œâ”€ Utilisateur "bloquÃ©" Ã  son DC d'origine
â”œâ”€ Pas de vraie Active-Active
â””â”€> Plus proche d'Active-Passive gÃ©ographiquement partitionnÃ©
```

### ImplÃ©mentation avec Redis Enterprise

```yaml
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# REDIS ENTERPRISE : ACTIVE-ACTIVE AVEC CRDB
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Redis Enterprise supporte nativement Active-Active via CRDB
# (Conflict-free Replicated DataBase)


# Configuration CRDB via API
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

curl -X POST https://redis-api.example.com/v1/crdbs \
    -H "Content-Type: application/json" \
    -d '{
        "name": "global-database",
        "memory_size": 10737418240,
        "replication": true,
        "participating_clusters": [
            {
                "cluster": {
                    "url": "https://us-east.redis.example.com",
                    "name": "us-east-cluster"
                }
            },
            {
                "cluster": {
                    "url": "https://eu-west.redis.example.com",
                    "name": "eu-west-cluster"
                }
            },
            {
                "cluster": {
                    "url": "https://asia-pacific.redis.example.com",
                    "name": "asia-cluster"
                }
            }
        ],
        "crdt_config": {
            "causal_consistency": true,
            "sync_sources": ["all"],
            "featureset_version": 1
        },
        "data_persistence": "aof-every-1-sec",
        "eviction_policy": "allkeys-lru"
    }'


# Comportement CRDT automatique
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Les types Redis standard deviennent CRDT transparently :

# STRING â†’ LWW-Register
redis-cli -h us-east SET user:1000:name "Alice"
redis-cli -h eu-west SET user:1000:name "Alicia"
# RÃ©solution automatique avec timestamp


# COUNTER â†’ PN-Counter (Positive-Negative Counter)
redis-cli -h us-east INCRBY global:views 100
redis-cli -h eu-west INCRBY global:views 50
# AprÃ¨s rÃ©plication : global:views = 150 (somme)


# SET â†’ OR-Set
redis-cli -h us-east SADD tags "redis" "cluster"
redis-cli -h eu-west SADD tags "active-active" "crdt"
# AprÃ¨s rÃ©plication : tags = {"redis", "cluster", "active-active", "crdt"}


# SORTED SET â†’ LWW-Sorted-Set avec scores
redis-cli -h us-east ZADD leaderboard 100 "player1"
redis-cli -h eu-west ZADD leaderboard 150 "player1"
# RÃ©solution : score le plus rÃ©cent (150)


# Monitoring CRDB
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# MÃ©triques spÃ©cifiques CRDB

# Lag de rÃ©plication entre DCs
curl https://redis-api.example.com/v1/crdbs/global-database/stats | jq '.replication_lag'

# Output :
{
  "us-east â†’ eu-west": "120ms",
  "us-east â†’ asia": "250ms",
  "eu-west â†’ us-east": "125ms",
  "eu-west â†’ asia": "200ms",
  "asia â†’ us-east": "245ms",
  "asia â†’ eu-west": "195ms"
}


# Bandwidth entre DCs
curl https://redis-api.example.com/v1/crdbs/global-database/stats | jq '.bandwidth'

# Output :
{
  "us-east â†’ eu-west": "45 Mbps",
  "us-east â†’ asia": "38 Mbps",
  ...
}


# Conflits dÃ©tectÃ©s et rÃ©solus
curl https://redis-api.example.com/v1/crdbs/global-database/stats | jq '.conflicts'

# Output :
{
  "total_conflicts": 12456,
  "conflicts_resolved": 12456,
  "conflicts_pending": 0
}


# Configuration avancÃ©e
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Activer causal consistency (cohÃ©rence causale)
# Garantit que les lectures respectent l'ordre causal des Ã©critures

curl -X PUT https://redis-api.example.com/v1/crdbs/global-database \
    -d '{"causal_consistency": true}'

# Avec causal consistency :
#   DC1: SET x "1"; SET y "2"  (y dÃ©pend de x)
#   DC2: GET y â†’ "2"  THEN GET x â†’ "1"  âœ“
#
# Sans causal consistency :
#   DC2: GET y â†’ "2"  THEN GET x â†’ null (pas encore rÃ©pliquÃ©) âœ—


# Compression WAN
curl -X PUT https://redis-api.example.com/v1/crdbs/global-database \
    -d '{"compression": 6}'  # Niveau 0-9 (6 = bon compromis)

# RÃ©duit bandwidth de 50-70% typiquement
```

### Patterns de dÃ©ploiement gÃ©ographique

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Patterns de dÃ©ploiement multi-DC               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


PATTERN 1 : GLOBAL MESH (3+ DCs, tous Active)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Topologie complÃ¨te : Chaque DC connectÃ© Ã  tous les autres


       US-EAST â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ EU-WEST
          â†•                         â†•
          â†•                         â†•
          â†•                         â†•
       ASIA-PAC â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


Latence rÃ©plication :
    US-EAST â†” EU-WEST : 80ms
    US-EAST â†” ASIA-PAC : 200ms
    EU-WEST â†” ASIA-PAC : 150ms

Cas d'usage :
â”œâ”€ Application globale (gaming, social network)
â”œâ”€ Utilisateurs rÃ©partis mondialement
â””â”€ Besoin latence minimale partout

CoÃ»t : Ã‰LEVÃ‰ (3 Ã— infra complÃ¨te + bandwidth mesh)


PATTERN 2 : HUB-AND-SPOKE (1 hub central, N spokes)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Hub central rÃ©plique vers spokes
Spokes rÃ©pliquent vers hub (pas entre eux)


    SPOKE-1 â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚
    SPOKE-2 â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â†’ HUB (central)
                    â”‚
    SPOKE-3 â”€â”€â”€â”€â”€â”€â”€â”€â”˜


Avantages :
â”œâ”€ RÃ©duit connexions (N au lieu de NÃ—(N-1)/2)
â”œâ”€ Simplifie monitoring
â””â”€ CoÃ»t bandwidth rÃ©duit

InconvÃ©nients :
â”œâ”€ Latence 2Ã— entre spokes (via hub)
â””â”€ Hub = point de contention

Cas d'usage :
â”œâ”€ Hub = DC principal (US)
â”œâ”€ Spokes = DC rÃ©gionaux (EU, ASIA)
â””â”€ Trafic inter-rÃ©gional faible


PATTERN 3 : REGIONAL CLUSTERS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Chaque rÃ©gion = cluster indÃ©pendant
RÃ©plication sÃ©lective de certaines clÃ©s


    US-CLUSTER          EU-CLUSTER          ASIA-CLUSTER
    â”œâ”€ user:US:*       â”œâ”€ user:EU:*        â”œâ”€ user:ASIA:*
    â”œâ”€ config:global   â”œâ”€ config:global    â”œâ”€ config:global
    â””â”€ (no EU data)    â””â”€ (no ASIA data)   â””â”€ (no US data)


RÃ©plication :
    config:global â†’ tous les clusters
    user:* â†’ seulement cluster d'origine


Avantages :
â”œâ”€ Isolation gÃ©ographique (RGPD)
â”œâ”€ Bandwidth optimisÃ©
â””â”€ ScalabilitÃ© indÃ©pendante par rÃ©gion

InconvÃ©nients :
â”œâ”€ ComplexitÃ© applicative (routing par rÃ©gion)
â””â”€ Utilisateur fixÃ© Ã  sa rÃ©gion

Cas d'usage :
â”œâ”€ SaaS multi-tenant
â”œâ”€ ConformitÃ© rÃ©glementaire stricte
â””â”€ DonnÃ©es rÃ©gionalisÃ©es


PATTERN 4 : CASCADING REPLICATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

RÃ©plication en cascade pour rÃ©duire charge hub


    PRIMARY â”€â”€â†’ SECONDARY-1 â”€â”€â†’ SECONDARY-2
                     â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ SECONDARY-3


Avantages :
â”œâ”€ RÃ©duit charge rÃ©seau sur Primary
â””â”€ ScalabilitÃ© rÃ©plication

InconvÃ©nients :
â”œâ”€ Latence accrue (multi-hop)
â”œâ”€ RPO augmentÃ©
â””â”€ ComplexitÃ©

Cas d'usage :
â”œâ”€ 5+ DCs
â””â”€ Primary avec bandwidth limitÃ©
```

## Latence et performance

### Analyse de latence WAN

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Latence inter-datacenter                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Latences typiques (round-trip) :
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

MÃªme rÃ©gion (ex: US-EAST-1A â†” US-EAST-1B)
    Latence : 1-3ms
    Bande passante : 100 Gbps+

RÃ©gions proches (ex: US-EAST â†” US-WEST)
    Latence : 60-80ms
    Bande passante : 10-100 Gbps

Continents (ex: US â†” Europe)
    Latence : 80-120ms
    Bande passante : 1-10 Gbps

Antipodes (ex: US â†” Australie)
    Latence : 180-250ms
    Bande passante : 1-5 Gbps


Impact sur rÃ©plication :
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

RÃ©plication asynchrone :
    â””â”€> Latence WAN n'impacte PAS les Ã©critures locales âœ“
    â””â”€> Mais crÃ©e lag de rÃ©plication

Exemple :
    Write sur US-EAST Ã  t=0
    â””â”€> RÃ©pliquÃ© vers EU-WEST Ã  t=100ms
        â””â”€> FenÃªtre de 100ms oÃ¹ EU-WEST a ancienne valeur


Calcul du lag :
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Lag = Latency_WAN + Processing_time + Queue_time

Latency_WAN : 100ms (US â†” EU)
Processing : 5ms (sÃ©rialisation/dÃ©sÃ©rialisation)
Queue : 10ms (buffer rÃ©plication)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total lag : 115ms


Sous forte charge :
    Queue_time peut atteindre plusieurs secondes
    â””â”€> Lag total = 100ms + 5ms + 5000ms = 5.1 secondes !


Optimisations :
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. COMPRESSION
   â”œâ”€ RÃ©duire volume â†’ moins de temps transfert
   â””â”€> Gain 50-70% bandwidth

2. BATCHING
   â”œâ”€ Regrouper plusieurs ops en un batch
   â””â”€> Amortir overhead rÃ©seau

3. PIPELINING
   â”œâ”€ Envoyer batch suivant avant ACK du prÃ©cÃ©dent
   â””â”€> Utiliser pleine bande passante

4. DEDICATED LINKS (VPN, Direct Connect)
   â”œâ”€ Ã‰viter internet public
   â””â”€> Latence plus stable, bandwidth garanti
```

### Optimisation de la bande passante

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# OPTIMISATIONS BANDWIDTH POUR XDR
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


# TECHNIQUE 1 : COMPRESSION
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Redis Enterprise : Compression native
# Niveau 6 (dÃ©faut) : Bon compromis CPU/compression

# Comparaison :
Sans compression : 1 GB/heure â†’ 1 GB transfert
Avec compression (ratio 70%) : 1 GB/heure â†’ 300 MB transfert
Ã‰conomie : 700 MB/heure = 16.8 GB/jour


# TECHNIQUE 2 : DELTA SYNC
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# N'envoyer que les changements, pas les valeurs complÃ¨tes

# Sans delta :
SET large_object "{...5MB...}"  â†’ RÃ©plication 5 MB

# Avec delta (Redis Enterprise) :
SET large_object "{...5MB...}"
HSET large_object field1 "new"  â†’ RÃ©plication 10 bytes seulement


# TECHNIQUE 3 : SELECTIVE REPLICATION
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# RÃ©pliquer seulement certaines clÃ©s vers certains DCs

# Configuration Redis Enterprise :
curl -X POST .../crdbs/config \
    -d '{
        "replication_rules": [
            {
                "pattern": "user:EU:*",
                "replicate_to": ["eu-west"],
                "exclude_from": ["us-east", "asia"]
            },
            {
                "pattern": "config:*",
                "replicate_to": ["all"]
            }
        ]
    }'


# TECHNIQUE 4 : THROTTLING / RATE LIMITING
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Limiter dÃ©bit rÃ©plication pour ne pas saturer WAN

# Redis Enterprise :
curl -X PUT .../crdbs/config \
    -d '{
        "replication_bandwidth_limit": "100MB/s"
    }'

# Utile si :
â”œâ”€ Bandwidth partagÃ© avec autres applications
â”œâ”€ CoÃ»t facturÃ© au volume
â””â”€ Ã‰viter saturation lors de catch-up aprÃ¨s panne


# TECHNIQUE 5 : ASYNC WRITES (Write-Behind)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Buffer Ã©critures localement, flush par batch

# Sans buffering :
1000 writes/sec Ã— 1 KB/write = 1 MB/sec bandwidth

# Avec buffering (flush toutes les 100ms) :
100 writes buffered Ã— 10 flushes/sec = mÃªme throughput
Mais moins de packets, moins d'overhead TCP


# TECHNIQUE 6 : COALESCING
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Fusionner Ã©critures multiples sur mÃªme clÃ©

# Sans coalescing :
SET key "v1"  â†’ RÃ©plication
SET key "v2"  â†’ RÃ©plication
SET key "v3"  â†’ RÃ©plication
Total : 3 rÃ©plications

# Avec coalescing :
SET key "v1"
SET key "v2"
SET key "v3"
â””â”€> RÃ©plication : SET key "v3" seulement
Total : 1 rÃ©plication


# MONITORING BANDWIDTH
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Prometheus query pour bandwidth WAN
rate(redis_net_output_bytes_total{role="master"}[1m]) * 8 / 1000000

# Alerte si > 80% capacitÃ© lien
```

## Disaster Recovery et procÃ©dures

### ProcÃ©dure DR pour Active-Active

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DISASTER RECOVERY : PERTE COMPLÃˆTE D'UN DC (ACTIVE-ACTIVE)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# SCÃ‰NARIO : DC ASIA-PACIFIC complÃ¨tement dÃ©truit
#            DCs US-EAST et EU-WEST toujours actifs


# Ã‰TAPE 1 : DÃ‰TECTION (automatique)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Health checks dÃ©tectent perte de connectivitÃ© vers ASIA

# Logs :
[2024-01-15 14:00:00] ERROR: ASIA-PAC unreachable
[2024-01-15 14:00:05] ERROR: Replication to ASIA-PAC failed
[2024-01-15 14:00:10] CRITICAL: ASIA-PAC datacenter DOWN


# Ã‰TAPE 2 : Ã‰VALUATION IMMÃ‰DIATE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Impact sur service :
â”œâ”€ Utilisateurs ASIA : AffectÃ©s (DC local down)
â”œâ”€ Utilisateurs US/EU : Non affectÃ©s (DCs actifs)

# Ã‰tat des donnÃ©es :
â”œâ”€ US-EAST : OK, contient toutes donnÃ©es rÃ©pliquÃ©es avant panne
â”œâ”€ EU-WEST : OK, contient toutes donnÃ©es rÃ©pliquÃ©es avant panne
â””â”€ ASIA-PAC : PERDU (assume hardware destroyed)


# Ã‰TAPE 3 : ROUTER TRAFIC ASIA VERS AUTRE DC
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Option A : Route vers DC gÃ©ographiquement proche

# Via DNS (GeoDNS)
aws route53 change-resource-record-sets \
    --hosted-zone-id Z1234567890ABC \
    --change-batch '{
        "Changes": [{
            "Action": "UPSERT",
            "ResourceRecordSet": {
                "Name": "redis-asia.example.com",
                "Type": "CNAME",
                "TTL": 60,
                "GeoLocation": {
                    "ContinentCode": "AS"
                },
                "ResourceRecords": [
                    {"Value": "redis-eu-west.example.com"}
                ]
            }
        }]
    }'

# Utilisateurs ASIA maintenant routÃ©s vers EU-WEST
# Latence augmentÃ©e (300ms au lieu de 10ms) mais service maintenu


# Option B : Global Load Balancer avec health checks

# AWS Global Accelerator / Cloudflare Load Balancer
# DÃ©tecte automatiquement ASIA down, route vers backup


# Ã‰TAPE 4 : DÃ‰SACTIVER RÃ‰PLICATION VERS ASIA
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Ã‰viter tentatives de rÃ©plication vers DC mort

# Redis Enterprise API :
curl -X DELETE https://redis-api/v1/crdbs/global-db/participants/asia-cluster


# Ã‰TAPE 5 : Ã‰VALUER PERTE DE DONNÃ‰ES
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# RPO dÃ©pend du lag de rÃ©plication avant panne

# VÃ©rifier derniÃ¨re rÃ©plication rÃ©ussie
curl https://redis-api/v1/crdbs/global-db/replication_log | jq

# Output :
{
  "last_successful_sync": {
    "asia_to_us": "2024-01-15 13:59:58",
    "asia_to_eu": "2024-01-15 13:59:59"
  },
  "failure_detected": "2024-01-15 14:00:00"
}

# Perte estimÃ©e : 1-2 secondes de donnÃ©es Ã©crites sur ASIA
# avant destruction


# Ã‰TAPE 6 : RECONSTRUIRE DC ASIA (aprÃ¨s incident)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Une fois nouveau hardware disponible :

# 1. Provisionner nouveau cluster Redis sur ASIA-NEW
#    (machines, rÃ©seau, Redis Enterprise)

# 2. RÃ©intÃ©grer ASIA-NEW dans CRDB
curl -X POST https://redis-api/v1/crdbs/global-db/participants \
    -d '{
        "cluster": {
            "url": "https://asia-new.redis.example.com",
            "name": "asia-new-cluster"
        }
    }'

# 3. Initial sync (peut prendre plusieurs heures)
#    CRDB copie Ã©tat complet de US ou EU vers ASIA-NEW

# 4. Une fois sync terminÃ©, ASIA-NEW devient actif
#    Trafic automatiquement routÃ© vers ASIA-NEW

# 5. Nettoyer l'ancien ASIA (si nÃ©cessaire)


# Ã‰TAPE 7 : POST-MORTEM
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

cat <<EOF > /docs/postmortem-asia-dc-loss.md
# Post-Mortem : Perte DC ASIA-PACIFIC

## Incident Summary
Date: 2024-01-15 14:00 UTC
Duration: 6 hours (until traffic fully restored)
Impact: 10% users (ASIA region) experienced degraded service

## Timeline
- 13:59:58 : Last successful replication from ASIA
- 14:00:00 : ASIA DC unresponsive (power failure confirmed)
- 14:02:30 : Traffic rerouted to EU-WEST for ASIA users
- 14:05:00 : Replication to ASIA disabled in CRDB
- 20:00:00 : New ASIA-NEW cluster provisioned
- 23:30:00 : Initial sync completed, ASIA-NEW active

## Data Loss
Estimated: 1-2 seconds of writes on ASIA before failure
RPO met: < 5 seconds (SLA: < 60 seconds)

## What Went Well
âœ“ Active-Active design prevented global outage
âœ“ Automatic health checks detected failure quickly
âœ“ Runbook followed successfully
âœ“ 90% of users unaffected

## What Could Be Improved
âœ— Manual traffic rerouting took 2.5 minutes
  â†’ Action: Implement automatic GeoDNS failover
âœ— Initial sync to ASIA-NEW took 3.5 hours
  â†’ Action: Pre-warm standby DC with periodic snapshots

## Action Items
1. [P0] Implement automated failover (owner: SRE team, due: 2024-02-01)
2. [P1] Setup standby ASIA DC with daily snapshots (owner: Ops, due: 2024-02-15)
3. [P2] Improve monitoring alerts for WAN replication lag (owner: DevOps, due: 2024-02-28)
EOF
```

### Tests de DR (Chaos Engineering)

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TESTS DISASTER RECOVERY (GAME DAYS)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Objectif : Valider que les procÃ©dures DR fonctionnent
#            EntraÃ®ner l'Ã©quipe
#            Identifier faiblesses avant incident rÃ©el


# TEST 1 : FAILOVER SIMPLE (ACTIVE-PASSIVE)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

#!/bin/bash
# test-failover-active-passive.sh

echo "=== DR Test : Active-Passive Failover ==="

# 1. VÃ©rifier Ã©tat initial
echo "Step 1: Verify initial state"
redis-cli -h primary.example.com PING || exit 1
redis-cli -h secondary.example.com PING || exit 1

# 2. Injecter panne sur Primary (simulÃ©)
echo "Step 2: Simulate Primary failure"
ssh primary.example.com "sudo systemctl stop redis"

# 3. VÃ©rifier dÃ©tection panne
echo "Step 3: Wait for failure detection (30s)"
sleep 30

# 4. DÃ©clencher failover
echo "Step 4: Trigger failover to Secondary"
/usr/local/bin/failover-to-secondary.sh

# 5. VÃ©rifier traffic sur Secondary
echo "Step 5: Verify traffic on Secondary"
for i in {1..10}; do
    redis-cli -h redis.example.com INCR test:failover:counter
done

count=$(redis-cli -h secondary.example.com GET test:failover:counter)
if [ "$count" -eq 10 ]; then
    echo "âœ“ Failover successful"
else
    echo "âœ— Failover failed"
    exit 1
fi

# 6. Restaurer Primary
echo "Step 6: Restore Primary"
ssh primary.example.com "sudo systemctl start redis"

# 7. Rapport
echo "=== Test Complete ==="
echo "Downtime: ~60 seconds"
echo "Data loss: 0 keys"


# TEST 2 : PERTE D'UN DC (ACTIVE-ACTIVE)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

#!/bin/bash
# test-dc-loss-active-active.sh

echo "=== DR Test : Active-Active DC Loss ==="

TARGET_DC="asia"

# 1. Baseline metrics
echo "Collecting baseline metrics..."
BASELINE_QPS=$(curl -s http://prometheus:9090/api/v1/query?query=redis_commands_per_sec | jq '.data.result[0].value[1]')

# 2. Simuler perte DC ASIA (network partition)
echo "Simulating loss of DC: $TARGET_DC"
ssh asia-firewall "iptables -A OUTPUT -j DROP"

# 3. Attendre routing automatique (ou manuel)
echo "Waiting for traffic rerouting..."
sleep 60

# 4. VÃ©rifier service toujours up
echo "Verifying service availability..."
for dc in us-east eu-west; do
    redis-cli -h $dc.redis.example.com PING || echo "âœ— $dc unreachable"
done

# 5. VÃ©rifier QPS total
CURRENT_QPS=$(curl -s http://prometheus:9090/api/v1/query?query=redis_commands_per_sec | jq '.data.result[0].value[1]')
echo "QPS: Baseline=$BASELINE_QPS, Current=$CURRENT_QPS"

# 6. Restaurer ASIA
echo "Restoring $TARGET_DC..."
ssh asia-firewall "iptables -F"

# 7. Attendre resync
echo "Waiting for resync..."
sleep 300

# 8. Rapport
echo "=== Test Complete ==="
echo "Service remained available: âœ“"
echo "Estimated downtime for ASIA users: 60s"


# TEST 3 : SPLIT-BRAIN (cas critique)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

#!/bin/bash
# test-split-brain.sh

echo "=== DR Test : Split-Brain Scenario ==="

# Simuler partition rÃ©seau isolant EU-WEST
echo "Creating network partition..."
ssh eu-west-firewall "iptables -A INPUT -s us-east-subnet -j DROP"
ssh eu-west-firewall "iptables -A INPUT -s asia-subnet -j DROP"

# EU-WEST ne peut plus communiquer avec US/ASIA
# Mais EU-WEST continue de servir clients locaux

# Ã‰criture concurrente sur mÃªme clÃ©
redis-cli -h us-east SET critical:key "value_from_us"
redis-cli -h eu-west SET critical:key "value_from_eu"

echo "Split-brain created. Monitoring for 60s..."
sleep 60

# Restaurer rÃ©seau
ssh eu-west-firewall "iptables -F"

echo "Network restored. Checking conflict resolution..."
sleep 30

# VÃ©rifier convergence
us_value=$(redis-cli -h us-east GET critical:key)
eu_value=$(redis-cli -h eu-west GET critical:key)

if [ "$us_value" == "$eu_value" ]; then
    echo "âœ“ Conflict resolved, values converged: $us_value"
else
    echo "âœ— Split-brain persists! US=$us_value, EU=$eu_value"
fi
```

## Best Practices

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Best Practices : Cross-Datacenter Replication       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚ ARCHITECTURE                                                â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•                                                â”‚
â”‚                                                             â”‚
â”‚ âœ“ Choisir pattern adaptÃ© au use case                        â”‚
â”‚   â”œâ”€ Active-Passive : DR simple, pas de conflits            â”‚
â”‚   â””â”€ Active-Active : Latence minimale, HA maximale          â”‚
â”‚                                                             â”‚
â”‚ âœ“ Dimensionner bande passante WAN gÃ©nÃ©reusement             â”‚
â”‚   â””â”€> 2-3x charge moyenne pour absorber pics                â”‚
â”‚                                                             â”‚
â”‚ âœ“ Utiliser liens dÃ©diÃ©s (VPN, Direct Connect)               â”‚
â”‚   â””â”€> Ã‰viter internet public pour latence stable            â”‚
â”‚                                                             â”‚
â”‚ âœ“ PrÃ©voir compression (50-70% gain bandwidth)               â”‚
â”‚                                                             â”‚
â”‚ âœ“ Tester rÃ©guliÃ¨rement failover (game days)                 â”‚
â”‚   â””â”€> Au moins trimestriellement                            â”‚
â”‚                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚ RÃ‰PLICATION                                                 â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•                                                 â”‚
â”‚                                                             â”‚
â”‚ âœ“ Toujours asynchrone pour WAN (latence inacceptable si sync)
â”‚                                                             â”‚
â”‚ âœ“ Monitorer lag de rÃ©plication en continu                   â”‚
â”‚   â””â”€> Alertes si lag > seuil (ex: 5s)                       â”‚
â”‚                                                             â”‚
â”‚ âœ“ Buffer/Batch pour optimiser bandwidth                     â”‚
â”‚                                                             â”‚
â”‚ âœ“ RÃ©plication sÃ©lective si possible                         â”‚
â”‚   â””â”€> Ne rÃ©pliquer que donnÃ©es nÃ©cessaires par rÃ©gion       â”‚
â”‚                                                             â”‚
â”‚ âœ“ PrÃ©voir retry automatique en cas d'Ã©chec rÃ©seau           â”‚
â”‚                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚ COHÃ‰RENCE                                                   â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•                                                  â”‚
â”‚                                                             â”‚
â”‚ âœ“ Comprendre et accepter eventual consistency               â”‚
â”‚                                                             â”‚
â”‚ âœ“ Utiliser CRDT si Active-Active (Redis Enterprise)         â”‚
â”‚                                                             â”‚
â”‚ âœ“ ImplÃ©menter idempotence dans l'application                â”‚
â”‚   â””â”€> GÃ©rer rÃ©ception multiple de mÃªme event                â”‚
â”‚                                                             â”‚
â”‚ âœ“ Timestamping prÃ©cis (NTP synchronisÃ©)                     â”‚
â”‚   â””â”€> Essentiel pour LWW                                    â”‚
â”‚                                                             â”‚
â”‚ âœ“ Ã‰viter opÃ©rations non-commutatives sur Active-Active      â”‚
â”‚   â””â”€> Ex: SET OK, APPEND risquÃ©                             â”‚
â”‚                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚ MONITORING                                                  â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•                                                  â”‚
â”‚                                                             â”‚
â”‚ âœ“ MÃ©triques par DC ET globales                              â”‚
â”‚                                                             â”‚
â”‚ âœ“ Dashboards sÃ©parÃ©s par rÃ©gion                             â”‚
â”‚                                                             â”‚
â”‚ âœ“ Alertes multi-niveaux :                                   â”‚
â”‚   â”œâ”€ P1: DC complÃ¨tement down                               â”‚
â”‚   â”œâ”€ P2: Lag > 10s                                          â”‚
â”‚   â””â”€ P3: Bandwidth > 80%                                    â”‚
â”‚                                                             â”‚
â”‚ âœ“ Distributed tracing (OpenTelemetry)                       â”‚
â”‚   â””â”€> Suivre requÃªte cross-DC                               â”‚
â”‚                                                             â”‚
â”‚ âœ“ Runbooks Ã  jour et testÃ©s                                 â”‚
â”‚                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚ COÃ›TS                                                       â”‚
â”‚ â•â•â•â•â•                                                       â”‚
â”‚                                                             â”‚
â”‚ âœ“ Budgeter correctement :                                   â”‚
â”‚   â”œâ”€ Infrastructure : N Ã— coÃ»t single-DC                    â”‚
â”‚   â”œâ”€ Bandwidth : FacturÃ© au volume                          â”‚
â”‚   â””â”€ Licences : Redis Enterprise si Active-Active           â”‚
â”‚                                                             â”‚
â”‚ âœ“ Optimiser bandwidth pour rÃ©duire coÃ»ts                    â”‚
â”‚   â””â”€> Compression, batching, selective replication          â”‚
â”‚                                                             â”‚
â”‚ âœ“ Dimensionner DCs selon charge rÃ©gionale                   â”‚
â”‚   â””â”€> Ex: US 50%, EU 30%, ASIA 20%                          â”‚
â”‚                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚ SÃ‰CURITÃ‰                                                    â”‚
â”‚ â•â•â•â•â•â•â•â•                                                    â”‚
â”‚                                                             â”‚
â”‚ âœ“ Chiffrer rÃ©plication WAN (TLS)                            â”‚
â”‚                                                             â”‚
â”‚ âœ“ Authentification mutuelle entre DCs                       â”‚
â”‚                                                             â”‚
â”‚ âœ“ Firewall : Whitelist IPs uniquement                       â”‚
â”‚                                                             â”‚
â”‚ âœ“ ConformitÃ© rÃ©glementaire (RGPD, etc.)                     â”‚
â”‚   â””â”€> Residence des donnÃ©es par rÃ©gion                      â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Conclusion

La rÃ©plication cross-datacenter reprÃ©sente le sommet de la complexitÃ© architecturale pour Redis Cluster, offrant une rÃ©silience maximale au prix d'une complexitÃ© opÃ©rationnelle significative. Le choix entre Active-Passive et Active-Active doit Ãªtre guidÃ© par les besoins rÃ©els :

**Active-Passive** convient pour :
- Disaster Recovery simple
- Applications tolÃ©rant latence gÃ©ographique
- Budgets limitÃ©s
- Ã‰quipes de taille modeste

**Active-Active** convient pour :
- Applications globales nÃ©cessitant latence minimale
- Haute disponibilitÃ© critique (99.999%+)
- CapacitÃ© Ã  investir dans infrastructure et licences
- Ã‰quipes matures avec expertise systÃ¨mes distribuÃ©s

Dans tous les cas, une approche rigoureuse de testing (game days), monitoring continu, et documentation exhaustive sont essentiels pour maintenir un systÃ¨me XDR stable et performant.

---

**Points clÃ©s Ã  retenir :**

- **Active-Passive** : Simple, un seul master, pas de conflits
- **Active-Active** : Latence minimale, HA maximale, gestion conflits nÃ©cessaire
- **RÃ©plication WAN** : Toujours asynchrone (latence inacceptable si sync)
- **CRDT** : Solution mathÃ©matique pour convergence sans conflits (Redis Enterprise)
- **LWW** : Last-Write-Wins, simple mais peut perdre donnÃ©es
- **Lag** : Latency + Processing + Queue, monitorer en continu
- **Compression** : 50-70% gain bandwidth, essentiel pour XDR
- **Tests DR** : Game days rÃ©guliers, chaos engineering
- **CoÃ»t** : 3-5x infrastructure single-DC, budgÃ©ter appropriÃ©ment

Le module 11 "Architecture DistribuÃ©e et Scaling Horizontal" est maintenant complet avec cette section finale sur la rÃ©plication gÃ©ographique.

â­ï¸ [Redis en Production et SÃ©curitÃ©](/12-redis-production-securite/README.md)
