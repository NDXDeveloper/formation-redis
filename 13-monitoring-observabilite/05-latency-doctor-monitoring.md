üîù Retour au [Sommaire](/SOMMAIRE.md)

# 13.5 Latency Doctor et Latency Monitoring

## Introduction

La latence est le KPI le plus critique pour Redis. En tant que syst√®me in-memory con√ßu pour des r√©ponses sub-millisecondes, toute d√©gradation de latence a un impact imm√©diat sur l'exp√©rience utilisateur. Cette section explore les outils natifs Redis et les strat√©gies de monitoring avanc√©es pour d√©tecter, diagnostiquer et r√©soudre les probl√®mes de latence.

### Pourquoi la latence est critique

**Redis = Speed** :
```
Latence attendue : 0.1 - 1ms (P99)
Latence probl√©matique : > 10ms
Latence catastrophique : > 100ms
```

**Impact business** :
```
+100ms latence = -1% conversion (Amazon)
+1s latence = -7% conversion (Walmart)
Redis timeout (2s) = Incident P1
```

### Anatomie de la latence Redis

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                Total Latency                        ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Network  ‚îÇ‚Üí‚îÇ Redis    ‚îÇ‚Üí‚îÇ Command  ‚îÇ‚Üí‚îÇNetwork ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ (client  ‚îÇ ‚îÇ Queue    ‚îÇ ‚îÇ Execution‚îÇ ‚îÇ(reply) ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚Üíserver) ‚îÇ ‚îÇ Wait     ‚îÇ ‚îÇ          ‚îÇ ‚îÇ        ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ  Mesurable    Opaque      Mesurable    Mesurable    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Composantes** :
1. **Network RTT** : Round-trip r√©seau (client ‚Üí Redis)
2. **Queue Wait** : Attente dans la queue Redis (single-thread)
3. **Command Execution** : Temps d'ex√©cution de la commande
4. **Reply Network** : Envoi de la r√©ponse au client

## 1. Latency Monitoring Subsystem de Redis

### 1.1 Activation du monitoring

**Configuration** :
```conf
# redis.conf
latency-monitor-threshold 100  # En millisecondes

# Si latency > 100ms ‚Üí Enregistr√© dans le subsystem
```

**Activation √† chaud** :
```bash
# Activer (seuil 100ms)
redis-cli CONFIG SET latency-monitor-threshold 100

# V√©rifier
redis-cli CONFIG GET latency-monitor-threshold

# D√©sactiver
redis-cli CONFIG SET latency-monitor-threshold 0
```

**Recommandations par environnement** :

| Environnement | Seuil | Rationale |
|---------------|-------|-----------|
| **Production** | 50-100ms | D√©tecter anomalies sans bruit |
| **Staging** | 25-50ms | Testing plus strict |
| **Dev** | 10-25ms | Maximum sensibilit√© |
| **Cache critique** | 10ms | SLA strict |

### 1.2 √âv√©nements monitor√©s

**Types d'√©v√©nements** :
```
command          : Ex√©cution de commande lente
fast-command     : Commande "rapide" anormalement lente
fork             : Fork pour BGSAVE/AOF rewrite
aof-write        : √âcriture AOF bloqu√©e
aof-fsync-always : fsync AOF synchrone
aof-fstat        : fstat() sur fichier AOF
aof-rename       : Renommage fichier AOF
aof-rewrite-diff-write : √âcriture diff√©rentiel AOF
active-defrag-cycle : Cycle de d√©fragmentation active
expire-cycle     : Cycle d'expiration de cl√©s
eviction-cycle   : Cycle d'√©viction
```

### 1.3 Commandes LATENCY

#### LATENCY LATEST

**Objectif** : Voir les derni√®res latences enregistr√©es

```bash
redis-cli LATENCY LATEST
```

**Output** :
```
1) 1) "command"
   2) (integer) 1702310000    # Timestamp Unix
   3) (integer) 156            # Latence (ms)
   4) (integer) 1247           # Latence max depuis reset

2) 1) "fork"
   2) (integer) 1702309500
   3) (integer) 2345
   4) (integer) 3456
```

**Interpr√©tation** :
- √âv√©nement `command` : Derni√®re commande lente 156ms
- √âv√©nement `fork` : Dernier fork a pris 2.3 secondes

#### LATENCY HISTORY

**Objectif** : Historique d'un √©v√©nement sp√©cifique

```bash
redis-cli LATENCY HISTORY command
```

**Output** :
```
1) 1) (integer) 1702310000    # Timestamp
   2) (integer) 156            # Latence (ms)

2) 1) (integer) 1702309800
   2) (integer) 234

3) 1) (integer) 1702309600
   2) (integer) 189
```

**Limite** : Garde maximum 160 entr√©es par √©v√©nement

#### LATENCY GRAPH

**Objectif** : Graphique ASCII de la latence

```bash
redis-cli LATENCY GRAPH command
```

**Output** :
```
command - high 234 ms, low 45 ms (all time high 456 ms)
--------------------------------------------------------------------------------
   #_
  _|
 _|
_|
||
||    _
||   ||
||   ||    #
||   ||   _|
```

**Usage** : Diagnostic rapide en SSH sans Grafana

#### LATENCY DOCTOR

**üî• Commande la plus puissante üî•**

```bash
redis-cli LATENCY DOCTOR
```

**Output exemple** :
```
Dave, I have observed latency spikes in this Redis instance.
You don't mind talking about it, do you Dave?

1. command: 12 latency spikes (average 156ms, mean deviation 45ms, period 120.00 sec).
   Worst all time event 234ms.

2. fork: 3 latency spikes (average 2345ms, mean deviation 567ms, period 3600.00 sec).
   Worst all time event 3456ms.

I have a few advices for you:

- Your current Transparent Huge Pages (THP) support seems to be enabled.
  Latency due to forks can be reduced disabling THP.

- The fork() system call took 2345 milliseconds in the last BGSAVE.
  The data set is 8GB, you may want to increase the machine RAM.

- I detected a slow command taking 234ms: KEYS *
  Slow commands can block Redis. Use SCAN instead of KEYS.

- AOF fsync is taking 156ms on average.
  Your disk may be too slow. Consider using faster SSD.
```

**Analyse automatique** :
- Corr√©lation des √©v√©nements
- Identification des patterns
- Recommandations actionnables
- R√©f√©rences aux m√©triques syst√®me

#### LATENCY RESET

**Objectif** : R√©initialiser les donn√©es de latence

```bash
# Reset tous les √©v√©nements
redis-cli LATENCY RESET

# Reset un √©v√©nement sp√©cifique
redis-cli LATENCY RESET command

# Retour
(integer) 1  # Nombre d'√©v√©nements reset
```

**Use case** : Apr√®s correction d'un probl√®me, reset pour confirmer

#### LATENCY HELP

```bash
redis-cli LATENCY HELP
```

**Output** :
```
LATENCY DOCTOR                     -- Return a human-readable latency analysis report.
LATENCY GRAPH <event>              -- Return an ASCII-art graph of event latency.
LATENCY HISTORY <event>            -- Return time-latency samples for event.
LATENCY LATEST                     -- Return latest latency samples for all events.
LATENCY RESET [event]              -- Reset latency data of one or all events.
LATENCY HELP                       -- This help.
```

## 2. Causes de Latence et Diagnostic

### 2.1 Fork Latency (BGSAVE/AOF Rewrite)

**Sympt√¥me** :
```bash
redis-cli LATENCY LATEST
# fork: 2500ms
```

**Cause** : Le fork() duplique l'espace m√©moire ‚Üí latence proportionnelle √† la RAM utilis√©e

**Facteurs aggravants** :
1. **Dataset large** : Plus de RAM = fork plus long
2. **THP activ√©** : Transparent Huge Pages augmente la latence de fork
3. **Fragmentation √©lev√©e** : Plus de pages √† copier
4. **Swap actif** : Fork extr√™mement lent

**Diagnostic** :
```bash
# 1. V√©rifier le temps de fork r√©cent
redis-cli INFO stats | grep latest_fork_usec
# latest_fork_usec:2500000  (2.5 secondes)

# 2. V√©rifier THP
cat /sys/kernel/mm/transparent_hugepage/enabled
# [always] madvise never  ‚Üê Probl√®me si [always]

# 3. V√©rifier la RAM et fragmentation
redis-cli INFO memory | grep -E "used_memory:|mem_fragmentation"
# used_memory:8589934592  (8GB)
# mem_fragmentation_ratio:1.75
```

**Solutions** :

#### Solution 1 : D√©sactiver THP
```bash
# Temporaire
echo never > /sys/kernel/mm/transparent_hugepage/enabled

# Permanent (systemd)
sudo tee /etc/systemd/system/disable-thp.service > /dev/null <<EOF
[Unit]
Description=Disable Transparent Huge Pages (THP)
Before=redis.service

[Service]
Type=oneshot
ExecStart=/bin/sh -c "echo never > /sys/kernel/mm/transparent_hugepage/enabled && echo never > /sys/kernel/mm/transparent_hugepage/defrag"

[Install]
WantedBy=multi-user.target
EOF

sudo systemctl daemon-reload
sudo systemctl enable disable-thp
sudo systemctl start disable-thp
```

**Gain attendu** : Fork latency r√©duite de 50-70%

#### Solution 2 : Augmenter la RAM
```
Dataset : 8GB
Fork : 2.5s
‚Üí Passer √† 16GB RAM
‚Üí Fork estim√© : 1.2s
```

#### Solution 3 : R√©duire la fr√©quence de BGSAVE
```conf
# redis.conf - Moins fr√©quent
save 900 1       # ‚Üê D√©faut
save 3600 1      # ‚Üê Moins fr√©quent (1h)

# Ou d√©sactiver si RDB non critique
save ""
```

#### Solution 4 : D√©fragmentation
```conf
# R√©duire la fragmentation
activedefrag yes
active-defrag-threshold-lower 10
```

### 2.2 Slow Commands

**Sympt√¥me** :
```bash
redis-cli LATENCY LATEST
# command: 234ms
```

**Cause** : Commandes O(N) sur grandes collections

**Commandes dangereuses** :

| Commande | Complexit√© | Danger | Alternative |
|----------|-----------|--------|-------------|
| `KEYS *` | O(N) | üî¥ Extr√™me | `SCAN` |
| `HGETALL` | O(N) | üü† √âlev√© | `HSCAN` |
| `SMEMBERS` | O(N) | üü† √âlev√© | `SSCAN` |
| `ZRANGE 0 -1` | O(N) | üü† √âlev√© | Limiter range |
| `SORT` | O(N log N) | üî¥ √âlev√© | √âviter ou LIMIT |
| `SUNIONSTORE` | O(N) | üü† Moyen | D√©pend de N |
| `FLUSHDB` | O(N) | üü† Moyen | `FLUSHDB ASYNC` |

**Diagnostic** :

#### SLOWLOG

**Configuration** :
```conf
# redis.conf
slowlog-log-slower-than 10000  # 10ms en microsecondes
slowlog-max-len 128            # Garder 128 entr√©es
```

**Consultation** :
```bash
# Voir les commandes lentes
redis-cli SLOWLOG GET 10
```

**Output** :
```
1) 1) (integer) 127              # ID unique
   2) (integer) 1702310000       # Timestamp Unix
   3) (integer) 234567           # Dur√©e (microsecondes = 234ms)
   4) 1) "KEYS"                  # Commande
      2) "*"                     # Arguments
   5) "10.0.1.50:54321"          # IP:Port client
   6) ""                         # Client name

2) 1) (integer) 126
   2) (integer) 1702309800
   3) (integer) 156789           # 156ms
   4) 1) "HGETALL"
      2) "user:12345"
   5) "10.0.1.51:54322"
   6) "app-worker-3"
```

**Analyse** :
```bash
# Nombre d'entr√©es dans le slowlog
redis-cli SLOWLOG LEN
# (integer) 45

# Reset du slowlog
redis-cli SLOWLOG RESET
```

**Monitoring avec script** :
```bash
#!/bin/bash
# slowlog-monitor.sh

while true; do
  redis-cli SLOWLOG GET 10 | grep -A 5 "KEYS\|HGETALL\|SMEMBERS" | \
  logger -t redis-slowlog
  sleep 60
done
```

**Solutions** :

#### Solution 1 : Remplacer KEYS par SCAN
```python
# ‚ùå Mauvais : Bloquant
keys = redis.keys('user:*')

# ‚úÖ Bon : Non-bloquant
cursor = 0
keys = []
while True:
    cursor, partial_keys = redis.scan(cursor, match='user:*', count=100)
    keys.extend(partial_keys)
    if cursor == 0:
        break
```

#### Solution 2 : Limiter HGETALL
```python
# ‚ùå Mauvais : Tout le hash
data = redis.hgetall('large_hash')

# ‚úÖ Bon : Champs sp√©cifiques
data = redis.hmget('large_hash', 'field1', 'field2', 'field3')

# ‚úÖ Ou HSCAN si besoin de tout
cursor = 0
data = {}
while True:
    cursor, items = redis.hscan('large_hash', cursor, count=100)
    data.update(dict(zip(items[::2], items[1::2])))
    if cursor == 0:
        break
```

#### Solution 3 : Pagination ZRANGE
```python
# ‚ùå Mauvais : Tout le sorted set
items = redis.zrange('leaderboard', 0, -1)

# ‚úÖ Bon : Top N seulement
items = redis.zrange('leaderboard', 0, 99)  # Top 100
```

### 2.3 AOF Latency

**Sympt√¥me** :
```bash
redis-cli LATENCY LATEST
# aof-fsync-always: 156ms
# aof-write: 89ms
```

**Cause** : I/O disque bloquant pendant fsync

**Diagnostic** :
```bash
# 1. V√©rifier la politique AOF
redis-cli CONFIG GET appendfsync
# appendfsync: everysec  (ou always, no)

# 2. V√©rifier AOF delayed fsync
redis-cli INFO persistence | grep aof_delayed_fsync
# aof_delayed_fsync: 12  ‚Üê Probl√®me si > 0

# 3. Tester I/O disque
sudo iostat -x 1 10
# Surveiller %util, await, svctm
```

**Impact des politiques** :

| appendfsync | Durabilit√© | Latence | Use Case |
|-------------|-----------|---------|----------|
| `always` | Maximale | √âlev√©e (100-500ms) | Finance, critique |
| `everysec` | Haute | Faible (< 1ms) | **Recommand√©** |
| `no` | Minimale | Minimale | Cache √©ph√©m√®re |

**Solutions** :

#### Solution 1 : Changer la politique
```bash
# Si currently "always" et acceptable de perdre 1s
redis-cli CONFIG SET appendfsync everysec
```

**Trade-off** :
- `always` ‚Üí `everysec` : Perte max 1s de donn√©es, gain latence √©norme
- `everysec` ‚Üí `no` : Perte possible plusieurs secondes, gain latence marginal

#### Solution 2 : Upgrade disque (HDD ‚Üí SSD)
```
HDD (7200 RPM)  : 100-200 IOPS, latence 10-20ms
SATA SSD        : 50k-100k IOPS, latence 0.1-1ms
NVMe SSD        : 500k-1M IOPS, latence 0.01-0.1ms

Impact sur AOF fsync:
HDD ‚Üí SSD : Latence √∑ 100
SATA SSD ‚Üí NVMe : Latence √∑ 10
```

#### Solution 3 : no-appendfsync-on-rewrite
```conf
# redis.conf
no-appendfsync-on-rewrite yes
```

**Effet** : Pendant AOF rewrite, d√©sactive fsync (moins de durabilit√©, mais pas de spike)

**Recommand√©** : `yes` pour √©viter les spikes lors des rewrites

### 2.4 Expiration/Eviction Cycles

**Sympt√¥me** :
```bash
redis-cli LATENCY LATEST
# expire-cycle: 45ms
# eviction-cycle: 78ms
```

**Cause** : Trop de cl√©s √† expirer/√©vincer en un cycle

**Diagnostic** :
```bash
# 1. Nombre de cl√©s avec TTL
redis-cli INFO keyspace
# db0:keys=10000000,expires=9500000,avg_ttl=300000

# Ratio : 95% des cl√©s ont un TTL ‚Üí Potentiellement probl√©matique

# 2. Taux d'expiration
redis-cli INFO stats | grep expired_keys
# expired_keys:8547123

# 3. √âvictions
redis-cli INFO stats | grep evicted_keys
# evicted_keys:154789  ‚Üê Probl√®me si > 0
```

**Solutions** :

#### Solution 1 : R√©partir les expirations
```python
# ‚ùå Mauvais : Tous les TTL identiques
import time
for i in range(1000000):
    redis.setex(f'key:{i}', 3600, 'value')  # Expiration √† la m√™me seconde

# ‚úÖ Bon : TTL avec jitter
import random
for i in range(1000000):
    ttl = 3600 + random.randint(-300, 300)  # ¬±5 min jitter
    redis.setex(f'key:{i}', ttl, 'value')
```

**Effet** : R√©partit les expirations dans le temps ‚Üí pas de spike

#### Solution 2 : Lazy expiration + Active
```conf
# redis.conf
# Redis expire les cl√©s en deux modes:
# 1. Lazy : √Ä l'acc√®s
# 2. Active : Cycles p√©riodiques (10x/sec par d√©faut)

# Tuning du cycle actif
hz 10  # Fr√©quence des t√¢ches de fond (d√©faut)
```

**Trade-off** :
- `hz` plus √©lev√© (50-100) : Expirations plus rapides, mais plus de CPU
- `hz` plus bas (5) : Moins de CPU, mais expirations plus lentes

#### Solution 3 : Augmenter maxmemory (√©viter √©victions)
```bash
# Si √©victions actives
redis-cli CONFIG SET maxmemory 8gb  # Augmenter
```

### 2.5 Network Latency

**Sympt√¥me** : Latence c√¥t√© client √©lev√©e mais pas dans Redis

**Diagnostic** :

#### Ping RTT
```bash
# Ping depuis le client
redis-cli -h redis-server --latency
# min: 0.12, max: 45.67, avg: 0.89 (1000 samples)

# Ping √©tendu
redis-cli -h redis-server --latency-history
# Affiche l'historique avec graph
```

**Outils r√©seau** :
```bash
# 1. Ping ICMP
ping redis-server
# rtt min/avg/max = 0.2/0.5/1.2 ms

# 2. MTR (My Traceroute)
mtr redis-server
# Identifie les hops avec perte de paquets

# 3. iperf (bandwidth test)
# Server
iperf3 -s

# Client
iperf3 -c redis-server
# 9.8 Gbits/sec ‚Üí Bon
# 100 Mbits/sec ‚Üí Goulot d'√©tranglement
```

**Solutions** :

#### Solution 1 : Co-localisation
```
M√™me datacenter : < 1ms RTT
Cross-datacenter (m√™me r√©gion) : 5-15ms RTT
Cross-datacenter (diff√©rentes r√©gions) : 50-200ms RTT
Cross-continent : 150-300ms RTT

Recommandation : Client et Redis dans le m√™me subnet/AZ
```

#### Solution 2 : Pipelining
```python
# ‚ùå Sans pipeline : N √ó RTT
for i in range(1000):
    redis.get(f'key:{i}')  # 1000 roundtrips

# ‚úÖ Avec pipeline : 1 √ó RTT
pipe = redis.pipeline()
for i in range(1000):
    pipe.get(f'key:{i}')
results = pipe.execute()  # 1 seul roundtrip
```

**Gain** :
```
Sans pipeline : 1000 √ó 0.5ms = 500ms
Avec pipeline : 1 √ó 0.5ms = 0.5ms
Gain : 1000√ó
```

#### Solution 3 : Connection pooling
```python
# ‚ùå Nouvelle connexion √† chaque requ√™te
def get_user(user_id):
    r = redis.Redis(host='redis-server')  # 3-way handshake
    return r.get(f'user:{user_id}')

# ‚úÖ Connection pool
pool = redis.ConnectionPool(
    host='redis-server',
    max_connections=50,
    decode_responses=True
)
redis_client = redis.Redis(connection_pool=pool)

def get_user(user_id):
    return redis_client.get(f'user:{user_id}')
```

### 2.6 Memory Swap

**Sympt√¥me** : Latence erratique, pics extr√™mes (> 1s)

**Diagnostic** :
```bash
# 1. V√©rifier si Redis est en swap
redis-cli INFO memory | grep used_memory_rss
# used_memory_rss:10000000000  (10GB)

redis-cli INFO memory | grep used_memory:
# used_memory:8000000000  (8GB)

# RSS > used_memory ‚Üí Pas de swap (OK)
# RSS < used_memory ‚Üí SWAP ACTIF (PROBL√àME GRAVE)

# 2. V√©rifier le swap syst√®me
free -h
#               total        used        free      shared  buff/cache   available
# Mem:           15Gi       12Gi       500Mi       100Mi        2.5Gi        2.8Gi
# Swap:          8.0Gi      3.2Gi      4.8Gi  ‚Üê 3.2GB utilis√©s (PROBL√àME)

# 3. Voir quel process utilise le swap
sudo smem -t -k
```

**Solutions** :

#### Solution 1 : D√©sactiver le swap (production)
```bash
# Temporaire
sudo swapoff -a

# Permanent
sudo sed -i '/swap/d' /etc/fstab
```

**Important** : Serveurs production d√©di√©s √† Redis ne devraient JAMAIS swapper

#### Solution 2 : Augmenter la RAM
```
Redis used_memory : 8GB
RAM syst√®me : 16GB
‚Üí Pas assez de marge

Recommandation : RAM ‚â• 2 √ó used_memory_peak
Exemple : 8GB dataset ‚Üí 16GB+ RAM
```

#### Solution 3 : vm.overcommit_memory
```bash
# V√©rifier
sysctl vm.overcommit_memory
# vm.overcommit_memory = 0  ‚Üê D√©faut (pas optimal)

# Configurer pour Redis
sudo sysctl vm.overcommit_memory=1

# Permanent
echo "vm.overcommit_memory=1" | sudo tee -a /etc/sysctl.conf
sudo sysctl -p
```

**Explication** :
- `0` : Kernel d√©cide (peut refuser fork)
- `1` : Toujours overcommit (recommand√© pour Redis)
- `2` : Ne jamais overcommit au-del√† de swap+RAM

## 3. Monitoring de Latence avec Prometheus

### 3.1 M√©triques de latence expos√©es

**Par Redis Exporter** :
```
# Latence moyenne des commandes
redis_command_call_duration_seconds_sum
redis_command_call_duration_seconds_count

# Latence par percentile (si histograms activ√©s)
redis_command_call_duration_seconds_bucket{le="0.001"}  # 1ms
redis_command_call_duration_seconds_bucket{le="0.005"}  # 5ms
redis_command_call_duration_seconds_bucket{le="0.01"}   # 10ms
redis_command_call_duration_seconds_bucket{le="0.05"}   # 50ms
redis_command_call_duration_seconds_bucket{le="0.1"}    # 100ms
redis_command_call_duration_seconds_bucket{le="0.5"}    # 500ms
redis_command_call_duration_seconds_bucket{le="1.0"}    # 1s
redis_command_call_duration_seconds_bucket{le="+Inf"}

# Temps de fork
redis_latest_fork_seconds

# Slowlog count
redis_slowlog_length
```

### 3.2 Requ√™tes PromQL pour latence

#### Latence moyenne
```promql
# Latence moyenne sur 5 minutes (en ms)
rate(redis_command_call_duration_seconds_sum[5m]) /
rate(redis_command_call_duration_seconds_count[5m]) * 1000
```

#### Latence P50, P95, P99
```promql
# P50 (m√©diane)
histogram_quantile(0.50,
  rate(redis_command_call_duration_seconds_bucket[5m])
) * 1000

# P95
histogram_quantile(0.95,
  rate(redis_command_call_duration_seconds_bucket[5m])
) * 1000

# P99
histogram_quantile(0.99,
  rate(redis_command_call_duration_seconds_bucket[5m])
) * 1000

# P99.9
histogram_quantile(0.999,
  rate(redis_command_call_duration_seconds_bucket[5m])
) * 1000
```

#### Latence par commande
```promql
# Top 10 commandes par latence moyenne
topk(10,
  rate(redis_command_call_duration_seconds_sum{cmd!=""}[5m]) /
  rate(redis_command_call_duration_seconds_count{cmd!=""}[5m])
) * 1000
```

#### Fork latency
```promql
# Derni√®re latence de fork (en secondes)
redis_latest_fork_seconds

# Taux de fork
rate(redis_rdb_changes_since_last_save[5m]) > 0
```

#### Slowlog growth
```promql
# Croissance du slowlog
rate(redis_slowlog_length[5m])
```

### 3.3 Recording Rules pour latence

```yaml
# /etc/prometheus/rules/redis_latency.yml
groups:
  - name: redis_latency
    interval: 30s
    rules:
      # Latence moyenne par instance
      - record: redis:latency_avg:ms
        expr: |
          rate(redis_command_call_duration_seconds_sum[5m]) /
          rate(redis_command_call_duration_seconds_count[5m]) * 1000

      # P50
      - record: redis:latency_p50:ms
        expr: |
          histogram_quantile(0.50,
            rate(redis_command_call_duration_seconds_bucket[5m])
          ) * 1000

      # P95
      - record: redis:latency_p95:ms
        expr: |
          histogram_quantile(0.95,
            rate(redis_command_call_duration_seconds_bucket[5m])
          ) * 1000

      # P99
      - record: redis:latency_p99:ms
        expr: |
          histogram_quantile(0.99,
            rate(redis_command_call_duration_seconds_bucket[5m])
          ) * 1000

      # Fork latency √©lev√©
      - record: redis:fork_latency:high
        expr: |
          redis_latest_fork_seconds > 1
```

### 3.4 Alertes latence

```yaml
# /etc/prometheus/rules/redis_latency_alerts.yml
groups:
  - name: redis_latency_alerts
    interval: 30s
    rules:
      # Latence P99 √©lev√©e
      - alert: RedisLatencyP99High
        expr: |
          histogram_quantile(0.99,
            rate(redis_command_call_duration_seconds_bucket[5m])
          ) * 1000 > 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Redis latency P99 √©lev√©e sur {{ $labels.instance }}"
          description: "P99: {{ $value | humanize }}ms (> 10ms)"

      # Latence P99 critique
      - alert: RedisLatencyP99Critical
        expr: |
          histogram_quantile(0.99,
            rate(redis_command_call_duration_seconds_bucket[5m])
          ) * 1000 > 50
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Redis latency P99 CRITIQUE sur {{ $labels.instance }}"
          description: "P99: {{ $value | humanize }}ms (> 50ms)"

      # Fork latency √©lev√©
      - alert: RedisForkLatencyHigh
        expr: redis_latest_fork_seconds > 1
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "Fork latency √©lev√© sur {{ $labels.instance }}"
          description: "Fork: {{ $value }}s - Consid√©rer d√©sactivation THP"

      # Slowlog croissant
      - alert: RedisSlowlogGrowing
        expr: rate(redis_slowlog_length[5m]) > 1
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Slowlog croissant sur {{ $labels.instance }}"
          description: "{{ $value }} commandes lentes/min - V√©rifier SLOWLOG"

      # Baseline deviation
      - alert: RedisLatencyAnomaly
        expr: |
          abs(
            redis:latency_p99:ms -
            avg_over_time(redis:latency_p99:ms[7d])
          ) > (3 * stddev_over_time(redis:latency_p99:ms[7d]))
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "Anomalie de latence d√©tect√©e"
          description: "Latence d√©vie de +3œÉ de la moyenne 7j"
```

## 4. Panel Grafana pour Latency

### 4.1 Panel : Latency Percentiles

```json
{
  "title": "Command Latency Percentiles",
  "type": "timeseries",
  "targets": [
    {
      "expr": "histogram_quantile(0.50, rate(redis_command_call_duration_seconds_bucket{instance=~\"$instance\"}[5m])) * 1000",
      "legendFormat": "P50 - {{instance}}",
      "refId": "A"
    },
    {
      "expr": "histogram_quantile(0.95, rate(redis_command_call_duration_seconds_bucket{instance=~\"$instance\"}[5m])) * 1000",
      "legendFormat": "P95 - {{instance}}",
      "refId": "B"
    },
    {
      "expr": "histogram_quantile(0.99, rate(redis_command_call_duration_seconds_bucket{instance=~\"$instance\"}[5m])) * 1000",
      "legendFormat": "P99 - {{instance}}",
      "refId": "C"
    },
    {
      "expr": "histogram_quantile(0.999, rate(redis_command_call_duration_seconds_bucket{instance=~\"$instance\"}[5m])) * 1000",
      "legendFormat": "P99.9 - {{instance}}",
      "refId": "D"
    }
  ],
  "fieldConfig": {
    "defaults": {
      "unit": "ms",
      "min": 0,
      "thresholds": {
        "mode": "absolute",
        "steps": [
          {"value": 0, "color": "green"},
          {"value": 5, "color": "yellow"},
          {"value": 10, "color": "orange"},
          {"value": 50, "color": "red"}
        ]
      }
    }
  }
}
```

### 4.2 Panel : Latency Heatmap

```json
{
  "title": "Latency Distribution (Heatmap)",
  "type": "heatmap",
  "targets": [
    {
      "expr": "sum(rate(redis_command_call_duration_seconds_bucket{instance=~\"$instance\"}[5m])) by (le)",
      "format": "heatmap",
      "legendFormat": "{{le}}",
      "refId": "A"
    }
  ],
  "options": {
    "calculate": false,
    "cellGap": 2,
    "color": {
      "mode": "scheme",
      "scheme": "Spectral",
      "steps": 128
    },
    "yAxis": {
      "unit": "s",
      "decimals": 0
    }
  }
}
```

### 4.3 Panel : Fork Latency

```json
{
  "title": "Fork Latency (seconds)",
  "type": "timeseries",
  "targets": [
    {
      "expr": "redis_latest_fork_seconds{instance=~\"$instance\"}",
      "legendFormat": "{{instance}}",
      "refId": "A"
    }
  ],
  "fieldConfig": {
    "defaults": {
      "unit": "s",
      "min": 0,
      "thresholds": {
        "steps": [
          {"value": 0, "color": "green"},
          {"value": 1, "color": "yellow"},
          {"value": 2, "color": "red"}
        ]
      }
    }
  }
}
```

### 4.4 Panel : Slowlog Table

```json
{
  "title": "Slow Commands",
  "type": "table",
  "targets": [
    {
      "expr": "topk(10, sum(rate(redis_command_call_duration_seconds_count{instance=~\"$instance\"}[5m])) by (cmd))",
      "format": "table",
      "instant": true,
      "refId": "A"
    },
    {
      "expr": "topk(10, rate(redis_command_call_duration_seconds_sum{instance=~\"$instance\"}[5m]) / rate(redis_command_call_duration_seconds_count{instance=~\"$instance\"}[5m])) by (cmd) * 1000",
      "format": "table",
      "instant": true,
      "refId": "B"
    }
  ],
  "transformations": [
    {
      "id": "merge"
    },
    {
      "id": "organize",
      "options": {
        "renameByName": {
          "cmd": "Command",
          "Value #A": "Calls/sec",
          "Value #B": "Avg Latency (ms)"
        }
      }
    }
  ]
}
```

## 5. Latency SLI/SLO

### 5.1 D√©finir des SLI (Service Level Indicators)

**M√©triques de latence** :
```
SLI_latency_p50 = P50 latency < 1ms
SLI_latency_p95 = P95 latency < 5ms
SLI_latency_p99 = P99 latency < 10ms
SLI_latency_p999 = P99.9 latency < 50ms
```

### 5.2 D√©finir des SLO (Service Level Objectives)

**Objectifs de disponibilit√©** :
```
SLO: 99.9% des requ√™tes P99 < 10ms

Calcul :
Total requ√™tes sur 30j : 2.592 milliards (1000 req/s √ó 30j)
Budget erreur : 0.1% = 2.592 millions de requ√™tes
‚Üí Maximum 2.592M requ√™tes > 10ms tol√©r√©
```

**Requ√™te PromQL pour SLO compliance** :
```promql
# % de temps o√π P99 < 10ms
100 * (
  count_over_time(
    (histogram_quantile(0.99,
      rate(redis_command_call_duration_seconds_bucket[5m])
    ) * 1000 < 10)[30d:]
  ) /
  count_over_time(
    histogram_quantile(0.99,
      rate(redis_command_call_duration_seconds_bucket[5m])
    )[30d:]
  )
)
```

**Alerte SLO burn rate** :
```yaml
- alert: RedisLatencySLOBurnRate
  expr: |
    (
      1 -
      (
        sum(rate(redis_command_call_duration_seconds_bucket{le="0.01"}[1h]))
        /
        sum(rate(redis_command_call_duration_seconds_count[1h]))
      )
    ) > 0.001 * 14.4  # 14.4√ó burn rate
  for: 5m
  labels:
    severity: critical
  annotations:
    summary: "SLO budget burning too fast"
    description: "√Ä ce rythme, budget SLO √©puis√© en 2 jours"
```

## 6. Troubleshooting Playbook

### 6.1 Checklist diagnostic rapide

**√âtape 1 : Identifier le type de latence** (2 min)
```bash
# 1. Latency Doctor
redis-cli --latency-doctor

# 2. Latest events
redis-cli LATENCY LATEST

# 3. Slowlog
redis-cli SLOWLOG GET 10

# 4. Fork recent
redis-cli INFO stats | grep latest_fork_usec
```

**√âtape 2 : V√©rifier les suspects habituels** (5 min)
```bash
# 1. THP
cat /sys/kernel/mm/transparent_hugepage/enabled

# 2. Swap
free -h | grep Swap

# 3. Disk I/O
iostat -x 1 5

# 4. Memory
redis-cli INFO memory | grep -E "used_memory|fragmentation"

# 5. Network
redis-cli --latency
```

**√âtape 3 : Corr√©ler avec m√©triques** (3 min)
```bash
# Grafana dashboard
# V√©rifier les panels :
# - Memory usage spike ?
# - Ops/sec spike ?
# - Clients spike ?
# - √âvictions actives ?
```

### 6.2 Arbre de d√©cision

```
Latence d√©tect√©e
    ‚îÇ
    ‚îú‚îÄ √âv√©nement "fork" ?
    ‚îÇ   ‚îú‚îÄ Oui ‚Üí THP activ√© ?
    ‚îÇ   ‚îÇ   ‚îú‚îÄ Oui ‚Üí D√©sactiver THP
    ‚îÇ   ‚îÇ   ‚îî‚îÄ Non ‚Üí RAM insuffisante, augmenter
    ‚îÇ   ‚îî‚îÄ Non ‚Üí Continuer
    ‚îÇ
    ‚îú‚îÄ √âv√©nement "command" ?
    ‚îÇ   ‚îú‚îÄ Oui ‚Üí V√©rifier SLOWLOG
    ‚îÇ   ‚îÇ   ‚îî‚îÄ KEYS, HGETALL, etc. ?
    ‚îÇ   ‚îÇ       ‚îî‚îÄ Remplacer par SCAN, limiter
    ‚îÇ   ‚îî‚îÄ Non ‚Üí Continuer
    ‚îÇ
    ‚îú‚îÄ √âv√©nement "aof-*" ?
    ‚îÇ   ‚îú‚îÄ Oui ‚Üí appendfsync = always ?
    ‚îÇ   ‚îÇ   ‚îú‚îÄ Oui ‚Üí Changer en everysec
    ‚îÇ   ‚îÇ   ‚îî‚îÄ Non ‚Üí Disque lent, upgrade SSD
    ‚îÇ   ‚îî‚îÄ Non ‚Üí Continuer
    ‚îÇ
    ‚îú‚îÄ √âv√©nement "expire/eviction" ?
    ‚îÇ   ‚îú‚îÄ Oui ‚Üí Trop de cl√©s expirant simultan√©ment ?
    ‚îÇ   ‚îÇ   ‚îî‚îÄ Ajouter jitter aux TTL
    ‚îÇ   ‚îî‚îÄ Non ‚Üí Continuer
    ‚îÇ
    ‚îî‚îÄ Network latency client ?
        ‚îî‚îÄ Oui ‚Üí Co-location, pipelining, pooling
```

### 6.3 Scripts d'automatisation

**Script de diagnostic complet** :
```bash
#!/bin/bash
# redis-latency-diagnosis.sh

echo "=== Redis Latency Diagnosis ==="
echo ""

echo "1. Latency Doctor:"
redis-cli LATENCY DOCTOR
echo ""

echo "2. Latest Latency Events:"
redis-cli LATENCY LATEST
echo ""

echo "3. Top 5 Slow Commands:"
redis-cli SLOWLOG GET 5
echo ""

echo "4. Fork Latency:"
redis-cli INFO stats | grep latest_fork_usec
echo ""

echo "5. THP Status:"
cat /sys/kernel/mm/transparent_hugepage/enabled
echo ""

echo "6. Swap Usage:"
free -h | grep -E "Mem:|Swap:"
echo ""

echo "7. Disk I/O:"
iostat -x 1 3
echo ""

echo "8. Memory Stats:"
redis-cli INFO memory | grep -E "used_memory:|fragmentation|maxmemory"
echo ""

echo "9. Client Latency (10 pings):"
redis-cli --latency-history -i 1 -c 10
echo ""

echo "=== Diagnosis Complete ==="
```

## 7. Best Practices R√©capitulatives

### 7.1 Configuration

- ‚úÖ `latency-monitor-threshold 100` (ou moins en fonction du SLA)
- ‚úÖ `slowlog-log-slower-than 10000` (10ms)
- ‚úÖ THP d√©sactiv√© (`echo never`)
- ‚úÖ `vm.overcommit_memory=1`
- ‚úÖ Pas de swap sur serveurs d√©di√©s
- ‚úÖ `appendfsync everysec` (pas `always` sauf besoin critique)
- ‚úÖ `no-appendfsync-on-rewrite yes`

### 7.2 Monitoring

- ‚úÖ Alertes sur P99 latency (> 10ms warning, > 50ms critical)
- ‚úÖ Alertes sur fork latency (> 1s)
- ‚úÖ Monitoring du slowlog growth
- ‚úÖ Dashboard avec percentiles (P50, P95, P99, P99.9)
- ‚úÖ SLI/SLO d√©finis et track√©s

### 7.3 Code applicatif

- ‚úÖ Utiliser SCAN au lieu de KEYS
- ‚úÖ Limiter HGETALL, SMEMBERS, ZRANGE
- ‚úÖ Pipelining pour requ√™tes multiples
- ‚úÖ Connection pooling
- ‚úÖ TTL avec jitter
- ‚úÖ Timeouts configur√©s (2-5s)

### 7.4 Infrastructure

- ‚úÖ SSD NVMe pour persistence
- ‚úÖ RAM ‚â• 2√ó peak memory
- ‚úÖ Client et Redis co-localis√©s (m√™me AZ/subnet)
- ‚úÖ 10 Gbps+ r√©seau si possible
- ‚úÖ CPU moderne (latence fork d√©pend du CPU)

## Conclusion

La latence Redis est un indicateur pr√©coce de probl√®mes syst√©miques. Un monitoring proactif avec LATENCY DOCTOR, SLOWLOG, et Prometheus permet de :

1. **D√©tecter** : Identifier les anomalies avant impact utilisateur
2. **Diagnostiquer** : Comprendre la cause racine (fork, slowlog, I/O, r√©seau)
3. **R√©soudre** : Appliquer les corrections appropri√©es
4. **Pr√©venir** : Ajuster configuration et code pour √©viter r√©currence

**Checklist op√©rationnelle** :
- [ ] `latency-monitor-threshold` configur√©
- [ ] THP d√©sactiv√©
- [ ] Alertes Prometheus actives
- [ ] Dashboard Grafana avec percentiles
- [ ] SLOWLOG r√©guli√®rement consult√©
- [ ] SLI/SLO d√©finis
- [ ] Playbook de troubleshooting document√©
- [ ] Tests de latence automatis√©s (CI/CD)

Une latence ma√Ætris√©e = Redis performant = Utilisateurs satisfaits.

---

**Prochaine section** : 13.6 - Alerting : Quand et comment alerter (strat√©gies avanc√©es)

‚è≠Ô∏è [Alerting : Quand et comment alerter](/13-monitoring-observabilite/06-alerting-quand-comment.md)
