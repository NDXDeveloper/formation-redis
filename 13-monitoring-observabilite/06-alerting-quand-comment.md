ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 13.6 Alerting : Quand et comment alerter

## Introduction

L'alerting est un art dÃ©licat : trop peu d'alertes = incidents non dÃ©tectÃ©s, trop d'alertes = dÃ©sensibilisation et alert fatigue. Cette section couvre les stratÃ©gies d'alerting production-ready pour Redis, de la conception des rÃ¨gles Ã  l'intÃ©gration avec l'astreinte.

### Le paradoxe de l'alerting

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Trop d'alertes                      â”‚
â”‚    (Alert Fatigue)                          â”‚
â”‚                                             â”‚
â”‚  â€¢ DÃ©sensibilisation de l'Ã©quipe            â”‚
â”‚  â€¢ Alertes ignorÃ©es                         â”‚
â”‚  â€¢ Vrais incidents manquÃ©s                  â”‚
â”‚  â€¢ Burnout on-call                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–²
                    â”‚
              Sweet Spot
                    â”‚
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Pas assez d'alertes                 â”‚
â”‚    (Blind Spots)                            â”‚
â”‚                                             â”‚
â”‚  â€¢ Incidents dÃ©couverts par les users       â”‚
â”‚  â€¢ Temps de rÃ©ponse Ã©levÃ©                   â”‚
â”‚  â€¢ Impact business majeur                   â”‚
â”‚  â€¢ Perte de confiance                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Principes fondamentaux

**RÃ¨gle d'or** : Chaque alerte doit Ãªtre **actionnaire** et **urgente**

```
Alerte = Quelque chose est cassÃ© OU va casser bientÃ´t
       + Action humaine immÃ©diate requise
       + Impact utilisateur si non traitÃ©
```

## 1. Philosophie de l'Alerting

### 1.1 Symptom-Based vs Cause-Based

#### Symptom-Based (RecommandÃ©)

**DÃ©finition** : Alerter sur l'impact utilisateur, pas sur la cause technique

**Exemple Redis** :
```yaml
# âœ… BON : Symptom-based
- alert: RedisHighLatency
  expr: redis:latency_p99:ms > 50
  annotations:
    summary: "Les utilisateurs subissent des lenteurs"

# âŒ MAUVAIS : Cause-based
- alert: RedisCPUHigh
  expr: redis_cpu_usage > 80
  annotations:
    summary: "CPU Ã©levÃ©"
```

**Pourquoi ?**
- CPU 80% peut Ãªtre normal si latence OK
- CPU 30% est problÃ©matique si latence Ã©levÃ©e
- **Alerter sur ce qui impacte les users, pas sur les mÃ©triques intermÃ©diaires**

#### Quand utiliser Cause-Based ?

**Acceptable pour** :
- PrÃ©diction (avant impact) : `memory > 90%` â†’ OOM imminent
- Debugging (combinÃ© avec symptom) : Latence Ã©levÃ©e + CPU 100%

### 1.2 Niveaux de SÃ©vÃ©ritÃ©

**Architecture Ã  3 niveaux** :

| Niveau | DÃ©finition | Action | Notification | Exemple |
|--------|-----------|--------|--------------|---------|
| **Warning** | Tendance dÃ©gradÃ©e, pas d'impact immÃ©diat | EnquÃªte en heures ouvrÃ©es | Slack | Memory > 80% |
| **Critical** | Impact utilisateur actuel ou imminent | Action immÃ©diate | PagerDuty | Latence > 100ms |
| **Info** | Ã‰vÃ©nement notable, pas d'action | Logging/dashboard | Slack (optionnel) | Deployment |

**Erreur courante** : Trop de niveaux (low, medium, high, critical, urgent, emergency)
â†’ Confusion et dÃ©sensibilisation

### 1.3 Alert Fatigue : Comment l'Ã©viter

**SymptÃ´mes d'alert fatigue** :
- Alertes ignorÃ©es systÃ©matiquement
- "Acknowledge without checking"
- Burnout on-call
- Vraies alertes manquÃ©es

**Solutions** :

#### 1. Tuning agressif des seuils
```yaml
# âŒ Trop sensible (alerte chaque nuit)
- alert: RedisMemoryHigh
  expr: redis_memory_used_pct > 70
  for: 5m

# âœ… Seuil rÃ©aliste
- alert: RedisMemoryHigh
  expr: redis_memory_used_pct > 85
  for: 10m
```

#### 2. PÃ©riode de "for" appropriÃ©e
```yaml
# âŒ Trop court (faux positifs sur spikes temporaires)
- alert: RedisLatencyHigh
  expr: redis:latency_p99:ms > 10
  for: 30s

# âœ… Filtre les spikes
- alert: RedisLatencyHigh
  expr: redis:latency_p99:ms > 10
  for: 5m
```

#### 3. Alertes basÃ©es sur tendance, pas valeur absolue
```promql
# âŒ Valeur fixe
redis_connected_clients > 900

# âœ… DÃ©viation de la baseline
abs(redis_connected_clients - avg_over_time(redis_connected_clients[7d])) >
(3 * stddev_over_time(redis_connected_clients[7d]))
```

#### 4. Grouping et deduplication
```yaml
# Grouper les alertes similaires
route:
  group_by: ['alertname', 'instance']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 4h
```

## 2. Alertes Essentielles Redis

### 2.1 Tier 1 : Alertes Critiques (PagerDuty)

Ces alertes DOIVENT rÃ©veiller quelqu'un Ã  3h du matin.

#### Alerte 1 : Redis Down

```yaml
- alert: RedisDown
  expr: redis_up == 0
  for: 1m
  labels:
    severity: critical
    tier: 1
  annotations:
    summary: "Redis instance {{ $labels.instance }} is DOWN"
    description: |
      Redis ne rÃ©pond plus depuis 1 minute.

      Impact: Tous les services dÃ©pendants sont affectÃ©s.

      Action immÃ©diate:
      1. VÃ©rifier l'Ã©tat du process: systemctl status redis
      2. VÃ©rifier les logs: journalctl -u redis -n 100
      3. Si crash: vÃ©rifier dmesg (OOM killer?)
      4. RedÃ©marrer si nÃ©cessaire

      Runbook: https://wiki.company.com/redis-down
```

**Justification** : Redis indisponible = impact immÃ©diat 100% des users

#### Alerte 2 : Latence Critique

```yaml
- alert: RedisLatencyCritical
  expr: |
    histogram_quantile(0.99,
      rate(redis_command_call_duration_seconds_bucket[5m])
    ) * 1000 > 100
  for: 3m
  labels:
    severity: critical
    tier: 1
  annotations:
    summary: "Redis latency P99 critique sur {{ $labels.instance }}"
    description: |
      Latence P99: {{ $value | humanize }}ms (seuil: 100ms)

      Impact: Timeouts applicatifs, UX dÃ©gradÃ©e.

      Diagnostic immÃ©diat:
      1. redis-cli LATENCY DOCTOR
      2. redis-cli SLOWLOG GET 10
      3. redis-cli INFO stats | grep latest_fork_usec
      4. VÃ©rifier dashboard Grafana pour cause (spike ops? memory?)

      Causes frÃ©quentes:
      - Fork (BGSAVE) en cours
      - Commandes lentes (KEYS, HGETALL)
      - Swap actif
      - Saturation rÃ©seau

      Runbook: https://wiki.company.com/redis-latency
```

**Justification** : Latence > 100ms = timeouts, impact direct users

#### Alerte 3 : Memory Critical (OOM Imminent)

```yaml
- alert: RedisMemoryCritical
  expr: |
    (redis_memory_used_bytes / redis_memory_maxmemory_bytes) * 100 > 95
  for: 2m
  labels:
    severity: critical
    tier: 1
  annotations:
    summary: "Redis mÃ©moire CRITIQUE sur {{ $labels.instance }}"
    description: |
      Utilisation: {{ $value | humanize1024 }}% (seuil: 95%)

      Impact: OOM imminent, Ã©victions ou crashes dans les minutes qui suivent.

      Action URGENTE:
      1. VÃ©rifier politique Ã©viction: redis-cli CONFIG GET maxmemory-policy
      2. Si noeviction: basculer temporairement sur allkeys-lru
         redis-cli CONFIG SET maxmemory-policy allkeys-lru
      3. Augmenter maxmemory si RAM disponible:
         redis-cli CONFIG SET maxmemory 8gb
      4. Identifier les gros consommateurs:
         redis-cli --bigkeys
      5. Planifier scaling (+ RAM ou cluster)

      Runbook: https://wiki.company.com/redis-oom
```

**Justification** : OOM = crash imminent ou Ã©victions massives

#### Alerte 4 : Replication Broken (Master-Replica)

```yaml
- alert: RedisReplicationBroken
  expr: |
    redis_master_link_up{redis_role="replica"} == 0
  for: 2m
  labels:
    severity: critical
    tier: 1
  annotations:
    summary: "Replica {{ $labels.instance }} dÃ©connectÃ© du master"
    description: |
      Le replica ne peut plus se synchroniser avec le master.

      Impact:
      - Perte de redondance
      - Risque de split-brain si failover
      - Backup compromis

      Diagnostic:
      1. Depuis le replica: redis-cli INFO replication
      2. VÃ©rifier rÃ©seau entre master et replica
      3. VÃ©rifier authentification (password/ACL)
      4. VÃ©rifier logs replica: grep "MASTER" /var/log/redis/redis.log

      Si prolongÃ© > 15 min:
      - Envisager resync manuel
      - Ou provisionner nouveau replica

      Runbook: https://wiki.company.com/redis-replication
```

**Justification** : RÃ©plication cassÃ©e = perte de HA, risque de data loss

#### Alerte 5 : Cluster State FAIL

```yaml
- alert: RedisClusterFail
  expr: redis_cluster_state == 0
  for: 1m
  labels:
    severity: critical
    tier: 1
  annotations:
    summary: "Redis Cluster en Ã©tat FAIL"
    description: |
      Le cluster Redis est en Ã©tat FAIL (slots non couverts ou nodes down).

      Impact:
      - Certaines clÃ©s inaccessibles
      - RequÃªtes Ã©chouent avec CLUSTERDOWN
      - Perte partielle ou totale de service

      Action immÃ©diate:
      1. Identifier les nodes en fail:
         redis-cli CLUSTER NODES | grep fail
      2. VÃ©rifier les slots non couverts:
         redis-cli CLUSTER INFO | grep cluster_slots
      3. Tenter de restaurer les nodes en fail
      4. Si node irrÃ©mÃ©diablement down:
         - Fail over vers replica
         - Ou rÃ©assigner les slots

      Runbook: https://wiki.company.com/redis-cluster-fail
```

**Justification** : Cluster FAIL = service potentiellement down

### 2.2 Tier 2 : Alertes Warning (Slack)

Ces alertes nÃ©cessitent attention mais pas rÃ©veil nocturne.

#### Alerte 6 : Memory High (Trend)

```yaml
- alert: RedisMemoryHigh
  expr: |
    (redis_memory_used_bytes / redis_memory_maxmemory_bytes) * 100 > 80
  for: 15m
  labels:
    severity: warning
    tier: 2
  annotations:
    summary: "Memory Ã©levÃ©e sur {{ $labels.instance }}"
    description: |
      Utilisation: {{ $value | humanize }}%

      Pas d'urgence immÃ©diate mais surveillance nÃ©cessaire.

      Actions Ã  planifier (heures ouvrÃ©es):
      1. Analyser la tendance sur 7j (dashboard Grafana)
      2. Si croissance continue: planifier scaling
      3. VÃ©rifier Ã©victions: redis-cli INFO stats | grep evicted_keys
      4. Optimiser si possible (compression, TTL, structures)

      Projection OOM:
      {{ query "predict_linear(redis_memory_used_bytes[7d], 86400*7)" | humanize }} dans 7j
```

#### Alerte 7 : Hit Ratio Low

```yaml
- alert: RedisHitRatioLow
  expr: |
    100 * (
      rate(redis_keyspace_hits_total[10m]) /
      (rate(redis_keyspace_hits_total[10m]) + rate(redis_keyspace_misses_total[10m]))
    ) < 80
  for: 30m
  labels:
    severity: warning
    tier: 2
  annotations:
    summary: "Hit ratio faible sur {{ $labels.instance }}"
    description: |
      Hit ratio: {{ $value | humanizePercentage }}

      Impact: Charge accrue sur la base de donnÃ©es backend.

      Investigations:
      1. Comparer avec baseline 7j
      2. VÃ©rifier Ã©victions rÃ©centes
      3. Analyser pattern d'accÃ¨s (nouveau trafic?)
      4. VÃ©rifier TTL (trop courts?)

      Si dÃ©gradation continue:
      - ConsidÃ©rer cache warming
      - Augmenter TTL si appropriÃ©
      - Augmenter capacitÃ© mÃ©moire
```

#### Alerte 8 : Fragmentation High

```yaml
- alert: RedisFragmentationHigh
  expr: redis_mem_fragmentation_ratio > 1.5
  for: 1h
  labels:
    severity: warning
    tier: 2
  annotations:
    summary: "Fragmentation Ã©levÃ©e sur {{ $labels.instance }}"
    description: |
      Fragmentation ratio: {{ $value }}
      MÃ©moire gaspillÃ©e: {{ query "redis_mem_fragmentation_bytes{instance='$instance'}" | humanize1024 }}

      Impact: Surconsommation mÃ©moire ({{ $value | humanize }}Ã— l'usage rÃ©el)

      Actions:
      1. Activer active defrag si pas dÃ©jÃ  fait:
         redis-cli CONFIG SET activedefrag yes
      2. Monitorer Ã©volution sur 24h
      3. Si pas d'amÃ©lioration: planifier redÃ©marrage maintenance

      Note: Fragmentation > 2.0 justifie un redÃ©marrage
```

#### Alerte 9 : Evictions Active

```yaml
- alert: RedisEvictingKeys
  expr: rate(redis_evicted_keys_total[5m]) > 10
  for: 10m
  labels:
    severity: warning
    tier: 2
  annotations:
    summary: "Ã‰victions actives sur {{ $labels.instance }}"
    description: |
      Taux d'Ã©viction: {{ $value }} clÃ©s/sec

      Impact: Perte de donnÃ©es chaudes, hit ratio dÃ©gradÃ©.

      Causes:
      - Maxmemory trop petit pour le workload
      - Dataset croissant sans scaling

      Actions:
      1. Confirmer politique Ã©viction (allkeys-lru recommandÃ©)
      2. Estimer besoin mÃ©moire rÃ©el
      3. Planifier augmentation maxmemory ou scaling
```

#### Alerte 10 : Fork Latency High

```yaml
- alert: RedisForkLatencyHigh
  expr: redis_latest_fork_seconds > 2
  for: 5m
  labels:
    severity: warning
    tier: 2
  annotations:
    summary: "Fork latency Ã©levÃ© sur {{ $labels.instance }}"
    description: |
      DerniÃ¨re latence fork: {{ $value }}s

      Impact: Spike de latence lors des BGSAVE/AOF rewrite

      Causes probables:
      1. THP activÃ© (Transparent Huge Pages)
      2. Dataset large avec RAM limite
      3. Fragmentation Ã©levÃ©e

      Actions:
      1. VÃ©rifier THP: cat /sys/kernel/mm/transparent_hugepage/enabled
      2. Si [always]: dÃ©sactiver (echo never)
      3. ConsidÃ©rer augmentation RAM si dataset > 50% RAM totale
```

#### Alerte 11 : Slowlog Growing

```yaml
- alert: RedisSlowlogGrowing
  expr: rate(redis_slowlog_length[10m]) > 1
  for: 15m
  labels:
    severity: warning
    tier: 2
  annotations:
    summary: "Slowlog en croissance sur {{ $labels.instance }}"
    description: |
      {{ $value }} commandes lentes/min enregistrÃ©es

      Impact potentiel: Commandes bloquantes dÃ©gradant la performance.

      Investigation:
      1. redis-cli SLOWLOG GET 10
      2. Identifier patterns (KEYS, HGETALL, etc.)
      3. Tracer dans le code applicatif
      4. Remplacer par alternatives (SCAN, HSCAN)

      Dashboard: [lien vers Grafana slowlog panel]
```

#### Alerte 12 : Connected Clients High

```yaml
- alert: RedisClientsHigh
  expr: |
    redis_connected_clients / redis_config_maxclients > 0.8
  for: 10m
  labels:
    severity: warning
    tier: 2
  annotations:
    summary: "Nombre de clients Ã©levÃ© sur {{ $labels.instance }}"
    description: |
      Clients: {{ query "redis_connected_clients{instance='$instance'}" }}
      Maxclients: {{ query "redis_config_maxclients{instance='$instance'}" }}
      Utilisation: {{ $value | humanizePercentage }}

      Risque: Refus de nouvelles connexions si maxclients atteint.

      Investigation:
      1. VÃ©rifier si normal (scaling app?)
      2. Suspecter connection leak si croissance anormale
      3. redis-cli CLIENT LIST | wc -l

      Si anormal:
      - VÃ©rifier connection pooling applicatif
      - Augmenter maxclients si lÃ©gitime
```

### 2.3 Tier 3 : Alertes Info (Dashboard)

Ces alertes sont informatives, pas d'action requise.

```yaml
- alert: RedisRestarted
  expr: changes(redis_uptime_in_seconds[5m]) > 0
  labels:
    severity: info
  annotations:
    summary: "Redis {{ $labels.instance }} a redÃ©marrÃ©"
    description: |
      L'instance a Ã©tÃ© redÃ©marrÃ©e il y a {{ query "redis_uptime_in_seconds{instance='$instance'}" }}s

      VÃ©rifier:
      - RedÃ©marrage planifiÃ©? (maintenance)
      - Ou crash? (vÃ©rifier logs)

- alert: RedisBGSaveInProgress
  expr: redis_rdb_bgsave_in_progress == 1
  labels:
    severity: info
  annotations:
    summary: "BGSAVE en cours sur {{ $labels.instance }}"
    description: |
      Backup RDB en cours. Possible spike de latence.
      DurÃ©e typique: {{ query "redis_rdb_last_bgsave_time_sec{instance='$instance'}" }}s
```

## 3. Configuration Prometheus Alertmanager

### 3.1 Architecture Alertmanager

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Prometheus Server                  â”‚
â”‚  (Ã‰valuation des rÃ¨gles d'alerte)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â”‚ Alertes actives
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Alertmanager                    â”‚
â”‚                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Grouping & Deduplication          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚               â”‚                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Silencing & Inhibition            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚               â”‚                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Routing                           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚         â”‚          â”‚
      â–¼         â–¼          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Slack  â”‚ â”‚PagerDutyâ”‚ â”‚  Email  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 Configuration Alertmanager (alertmanager.yml)

```yaml
global:
  # RÃ©solution URL Prometheus (pour liens dans alertes)
  resolve_timeout: 5m

  # Configuration Slack globale
  slack_api_url: 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL'

  # Configuration PagerDuty globale
  pagerduty_url: 'https://events.pagerduty.com/v2/enqueue'

# Templates custom
templates:
  - '/etc/alertmanager/templates/*.tmpl'

# Routing des alertes
route:
  # Labels de regroupement
  group_by: ['alertname', 'instance', 'severity']

  # Attendre 30s pour grouper alertes similaires
  group_wait: 30s

  # Intervalle entre groupes
  group_interval: 5m

  # Ne pas rÃ©pÃ©ter avant 4h
  repeat_interval: 4h

  # Receiver par dÃ©faut
  receiver: 'slack-warnings'

  # Routes spÃ©cifiques
  routes:
    # Critiques â†’ PagerDuty
    - match:
        severity: critical
      receiver: 'pagerduty-critical'
      group_wait: 10s
      repeat_interval: 1h
      continue: true  # Continuer vers autres routes

    # Critiques â†’ Slack aussi
    - match:
        severity: critical
      receiver: 'slack-critical'
      group_wait: 10s

    # Warnings â†’ Slack
    - match:
        severity: warning
      receiver: 'slack-warnings'

    # Info â†’ Slack optionnel
    - match:
        severity: info
      receiver: 'slack-info'
      group_interval: 30m
      repeat_interval: 24h

    # Alertes Redis spÃ©cifiques â†’ Channel dÃ©diÃ©
    - match_re:
        alertname: 'Redis.*'
      receiver: 'slack-redis'
      continue: true

# Inhibition (suppression conditionnelle)
inhibit_rules:
  # Si Redis down, supprimer toutes les autres alertes Redis
  - source_match:
      alertname: 'RedisDown'
    target_match_re:
      alertname: 'Redis.*'
    equal: ['instance']

  # Si cluster fail, supprimer alertes nodes individuels
  - source_match:
      alertname: 'RedisClusterFail'
    target_match_re:
      alertname: 'Redis.*'
    equal: ['cluster']

# Receivers (destinations)
receivers:
  # PagerDuty Critical
  - name: 'pagerduty-critical'
    pagerduty_configs:
      - service_key: 'YOUR_PAGERDUTY_SERVICE_KEY'
        severity: 'critical'
        description: '{{ .GroupLabels.alertname }}: {{ .CommonAnnotations.summary }}'
        details:
          firing: '{{ template "pagerduty.default.instances" .Alerts.Firing }}'
          resolved: '{{ template "pagerduty.default.instances" .Alerts.Resolved }}'
          num_firing: '{{ .Alerts.Firing | len }}'
          num_resolved: '{{ .Alerts.Resolved | len }}'

  # Slack Critical
  - name: 'slack-critical'
    slack_configs:
      - channel: '#redis-alerts-critical'
        color: 'danger'
        title: ':rotating_light: {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          *Summary:* {{ .Annotations.summary }}
          *Description:* {{ .Annotations.description }}
          *Instance:* {{ .Labels.instance }}
          *Severity:* {{ .Labels.severity }}
          {{ end }}
        actions:
          - type: button
            text: 'Runbook :book:'
            url: '{{ (index .Alerts 0).Annotations.runbook_url }}'
          - type: button
            text: 'Dashboard :chart_with_upwards_trend:'
            url: 'https://grafana.company.com/d/redis-overview'
          - type: button
            text: 'Silence :mute:'
            url: '{{ .ExternalURL }}/#/silences/new?filter=%7B{{ .GroupLabels.SortedPairs.Values | join "," }}%7D'

  # Slack Warnings
  - name: 'slack-warnings'
    slack_configs:
      - channel: '#redis-alerts'
        color: 'warning'
        title: ':warning: {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          *Summary:* {{ .Annotations.summary }}
          *Instance:* {{ .Labels.instance }}
          {{ end }}

  # Slack Redis dÃ©diÃ©
  - name: 'slack-redis'
    slack_configs:
      - channel: '#redis-team'
        color: '{{ if eq .Status "firing" }}warning{{ else }}good{{ end }}'
        title: '{{ .GroupLabels.alertname }}'
        text: |
          {{ .CommonAnnotations.summary }}

          Affected instances: {{ .Alerts.Firing | len }}

  # Slack Info
  - name: 'slack-info'
    slack_configs:
      - channel: '#redis-info'
        color: 'good'
        title: 'â„¹ï¸ {{ .GroupLabels.alertname }}'
        text: '{{ .CommonAnnotations.summary }}'
```

### 3.3 Templates personnalisÃ©s

**Template Slack enrichi** (`/etc/alertmanager/templates/slack.tmpl`) :
```go
{{ define "slack.title" }}
[{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .GroupLabels.SortedPairs.Values | join " " }}
{{ end }}

{{ define "slack.text" }}
{{ range .Alerts }}
*Alert:* {{ .Labels.alertname }}
*Severity:* {{ .Labels.severity }}
*Instance:* {{ .Labels.instance }}
*Summary:* {{ .Annotations.summary }}

*Description:*
{{ .Annotations.description }}

*Value:* {{ .Value }}
*Started at:* {{ .StartsAt.Format "2006-01-02 15:04:05 MST" }}
{{ if ne .Status "firing" }}*Resolved at:* {{ .EndsAt.Format "2006-01-02 15:04:05 MST" }}{{ end }}

---
{{ end }}

*Runbook:* {{ .CommonAnnotations.runbook_url }}
*Dashboard:* https://grafana.company.com/d/redis-overview
*Silence:* {{ .ExternalURL }}/#/silences/new
{{ end }}
```

### 3.4 Silences programmatiques

**API pour crÃ©er un silence** :
```bash
# Silence pendant maintenance (2h)
START=$(date -u +%Y-%m-%dT%H:%M:%SZ)
END=$(date -u -d '+2 hours' +%Y-%m-%dT%H:%M:%SZ)

curl -X POST http://alertmanager:9093/api/v2/silences \
  -H "Content-Type: application/json" \
  -d '{
    "matchers": [
      {
        "name": "instance",
        "value": "redis-prod-1:9121",
        "isRegex": false
      }
    ],
    "startsAt": "'$START'",
    "endsAt": "'$END'",
    "createdBy": "ops-team",
    "comment": "Maintenance planifiÃ©e: dÃ©fragmentation mÃ©moire"
  }'
```

**Script pour maintenance** :
```bash
#!/bin/bash
# silence-redis.sh

INSTANCE="$1"
DURATION_HOURS="$2"
REASON="$3"

if [ -z "$INSTANCE" ] || [ -z "$DURATION_HOURS" ]; then
  echo "Usage: $0 <instance> <duration_hours> <reason>"
  exit 1
fi

START=$(date -u +%Y-%m-%dT%H:%M:%SZ)
END=$(date -u -d "+${DURATION_HOURS} hours" +%Y-%m-%dT%H:%M:%SZ)

curl -X POST http://alertmanager:9093/api/v2/silences \
  -H "Content-Type: application/json" \
  -d "{
    \"matchers\": [
      {\"name\": \"instance\", \"value\": \"${INSTANCE}\", \"isRegex\": false}
    ],
    \"startsAt\": \"${START}\",
    \"endsAt\": \"${END}\",
    \"createdBy\": \"$(whoami)\",
    \"comment\": \"${REASON}\"
  }"

echo "Silence crÃ©Ã© pour ${INSTANCE} pendant ${DURATION_HOURS}h"
echo "Raison: ${REASON}"
```

**Utilisation** :
```bash
./silence-redis.sh redis-prod-1:9121 2 "RedÃ©marrage planifiÃ© pour dÃ©fragmentation"
```

## 4. Notification Channels

### 4.1 PagerDuty (On-call)

**Configuration du service** :
```yaml
pagerduty_configs:
  - service_key: 'YOUR_SERVICE_KEY'

    # Routing key (PagerDuty Events API v2)
    routing_key: 'YOUR_ROUTING_KEY'

    # URL custom (si on-premise)
    url: 'https://events.pagerduty.com/v2/enqueue'

    # Severity mapping
    severity: '{{ if eq .Labels.severity "critical" }}critical{{ else }}warning{{ end }}'

    # DÃ©tails de l'alerte
    description: '{{ .CommonAnnotations.summary }}'

    client: 'Prometheus Alertmanager'
    client_url: '{{ .ExternalURL }}'

    details:
      firing: '{{ .Alerts.Firing | len }}'
      resolved: '{{ .Alerts.Resolved | len }}'
      alertname: '{{ .GroupLabels.alertname }}'
      instance: '{{ .GroupLabels.instance }}'
      severity: '{{ .GroupLabels.severity }}'
      description: '{{ .CommonAnnotations.description }}'
      runbook: '{{ .CommonAnnotations.runbook_url }}'
      dashboard: 'https://grafana.company.com/d/redis-overview'
```

**Escalation policy PagerDuty** :
```
Level 1: Primary on-call (immÃ©diat)
  â†“ (15 min sans ACK)
Level 2: Secondary on-call
  â†“ (15 min sans ACK)
Level 3: Engineering manager
  â†“ (30 min sans ACK)
Level 4: CTO (incidents majeurs)
```

### 4.2 Slack

**IntÃ©gration webhook** :
```yaml
slack_configs:
  - api_url: 'https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXX'
    channel: '#redis-alerts'

    # Username et icon
    username: 'Alertmanager'
    icon_emoji: ':prometheus:'

    # Titre dynamique
    title: |
      {{ if eq .Status "firing" }}:fire:{{ else }}:white_check_mark:{{ end }}
      {{ .GroupLabels.alertname }}

    # URL du titre
    title_link: 'https://grafana.company.com/d/redis-overview'

    # Couleur
    color: |
      {{ if eq .Status "resolved" }}good
      {{ else if eq .CommonLabels.severity "warning" }}warning
      {{ else }}danger{{ end }}

    # Corps du message
    text: |
      {{ range .Alerts }}
      *Instance:* {{ .Labels.instance }}
      *Severity:* {{ .Labels.severity }}
      *Summary:* {{ .Annotations.summary }}
      {{ if .Annotations.description }}
      *Details:*
      {{ .Annotations.description }}
      {{ end }}
      {{ end }}

    # Actions (boutons)
    actions:
      - type: button
        text: 'View Dashboard'
        url: 'https://grafana.company.com/d/redis-overview'
      - type: button
        text: 'Runbook'
        url: '{{ (index .Alerts 0).Annotations.runbook_url }}'
      - type: button
        text: 'Silence 1h'
        url: '{{ .ExternalURL }}/#/silences/new'

    # Champs additionnels
    fields:
      - title: 'Environment'
        value: '{{ .CommonLabels.env }}'
        short: true
      - title: 'Region'
        value: '{{ .CommonLabels.region }}'
        short: true
      - title: 'Firing'
        value: '{{ .Alerts.Firing | len }}'
        short: true
      - title: 'Resolved'
        value: '{{ .Alerts.Resolved | len }}'
        short: true
```

### 4.3 Email

**Configuration SMTP** :
```yaml
# global config
global:
  smtp_smarthost: 'smtp.company.com:587'
  smtp_from: 'alertmanager@company.com'
  smtp_auth_username: 'alertmanager'
  smtp_auth_password: 'secure_password'
  smtp_require_tls: true

# receiver
receivers:
  - name: 'email-oncall'
    email_configs:
      - to: 'oncall-redis@company.com'
        headers:
          Subject: |
            [{{ .Status | toUpper }}] {{ .GroupLabels.alertname }}
        html: |
          <!DOCTYPE html>
          <html>
          <head>
            <style>
              .alert { padding: 10px; margin: 10px 0; border-radius: 4px; }
              .critical { background-color: #ffebee; border-left: 4px solid #f44336; }
              .warning { background-color: #fff3e0; border-left: 4px solid #ff9800; }
              .info { background-color: #e3f2fd; border-left: 4px solid #2196f3; }
            </style>
          </head>
          <body>
            <h2>{{ .GroupLabels.alertname }}</h2>
            {{ range .Alerts }}
            <div class="alert {{ .Labels.severity }}">
              <h3>{{ .Labels.instance }}</h3>
              <p><strong>Summary:</strong> {{ .Annotations.summary }}</p>
              <p><strong>Description:</strong></p>
              <pre>{{ .Annotations.description }}</pre>
              <p><strong>Started:</strong> {{ .StartsAt.Format "2006-01-02 15:04:05 MST" }}</p>
              {{ if .Annotations.runbook_url }}
              <p><a href="{{ .Annotations.runbook_url }}">View Runbook</a></p>
              {{ end }}
            </div>
            {{ end }}
          </body>
          </html>
```

### 4.4 Webhook Custom

**IntÃ©gration avec systÃ¨me de ticketing** :
```yaml
receivers:
  - name: 'jira-tickets'
    webhook_configs:
      - url: 'https://jira-webhook.company.com/create-ticket'
        send_resolved: true
        http_config:
          bearer_token: 'YOUR_API_TOKEN'

        # Max alerts dans une notification
        max_alerts: 10
```

**Webhook receiver (Python Flask)** :
```python
from flask import Flask, request, jsonify
import requests

app = Flask(__name__)

JIRA_URL = "https://jira.company.com/rest/api/2/issue"
JIRA_TOKEN = "your_jira_token"

@app.route('/create-ticket', methods=['POST'])
def create_ticket():
    alert_data = request.json

    for alert in alert_data.get('alerts', []):
        if alert['status'] == 'firing':
            # CrÃ©er ticket JIRA
            ticket = {
                "fields": {
                    "project": {"key": "REDIS"},
                    "summary": f"{alert['labels']['alertname']} on {alert['labels']['instance']}",
                    "description": alert['annotations']['description'],
                    "issuetype": {"name": "Incident"},
                    "priority": {"name": "High" if alert['labels']['severity'] == 'critical' else "Medium"},
                    "labels": ["redis", "alertmanager", alert['labels']['severity']]
                }
            }

            response = requests.post(
                JIRA_URL,
                json=ticket,
                headers={
                    "Authorization": f"Bearer {JIRA_TOKEN}",
                    "Content-Type": "application/json"
                }
            )

            if response.status_code == 201:
                print(f"Ticket crÃ©Ã©: {response.json()['key']}")

        elif alert['status'] == 'resolved':
            # RÃ©soudre le ticket correspondant
            pass  # Logique de rÃ©solution

    return jsonify({"status": "ok"}), 200

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

## 5. Runbooks et Documentation

### 5.1 Structure d'un runbook

**Template** :
```markdown
# Runbook: [Nom de l'alerte]

## MÃ©tadonnÃ©es
- **Alerte:** RedisDown
- **Severity:** Critical
- **SLA:** RÃ©solution < 15 minutes
- **DerniÃ¨re mise Ã  jour:** 2024-01-15
- **Responsable:** Ã‰quipe Infrastructure

## SymptÃ´mes
- Redis instance ne rÃ©pond plus
- Applications en erreur avec timeout Redis
- MÃ©triques Prometheus: `redis_up == 0`

## Impact
- **Utilisateurs:** 100% des fonctionnalitÃ©s dÃ©pendantes indisponibles
- **Business:** Perte de revenue, impossibilitÃ© de traiter les commandes
- **SLA:** Violation du SLA de disponibilitÃ© 99.9%

## Diagnostic rapide (< 2 min)
```bash
# 1. VÃ©rifier le process
ssh redis-server
sudo systemctl status redis

# 2. VÃ©rifier les logs rÃ©cents
sudo journalctl -u redis -n 100 --no-pager

# 3. VÃ©rifier dmesg (OOM killer?)
sudo dmesg | tail -50 | grep -i redis

# 4. Tenter connexion
redis-cli PING
```

## Causes frÃ©quentes
1. **OOM Killer** (70%)
   - SymptÃ´me: Dans dmesg: "Out of memory: Kill process"
   - Cause: MÃ©moire insuffisante, maxmemory trop Ã©levÃ©

2. **Crash/Segfault** (15%)
   - SymptÃ´me: Core dump dans /var/crash
   - Cause: Bug Redis, corruption mÃ©moire

3. **Kill manuel** (10%)
   - SymptÃ´me: Dans logs: "Received SIGTERM"
   - Cause: Intervention humaine, automation

4. **ProblÃ¨me disque** (5%)
   - SymptÃ´me: "Can't save in background: fork: Cannot allocate memory"
   - Cause: Disque plein, I/O error

## RÃ©solution

### Ã‰tape 1: RedÃ©marrage d'urgence
```bash
# RedÃ©marrer Redis
sudo systemctl restart redis

# VÃ©rifier dÃ©marrage
sudo systemctl status redis
redis-cli PING
```

### Ã‰tape 2: Si Ã©chec redÃ©marrage
```bash
# VÃ©rifier config
sudo redis-server /etc/redis/redis.conf --test-memory 1024

# VÃ©rifier permissions
ls -la /var/lib/redis
ls -la /var/log/redis

# Tenter dÃ©marrage manuel (debug)
sudo -u redis redis-server /etc/redis/redis.conf
```

### Ã‰tape 3: Si OOM dÃ©tectÃ©
```bash
# 1. RÃ©duire maxmemory temporairement
sudo sed -i 's/maxmemory .*/maxmemory 4gb/' /etc/redis/redis.conf

# 2. RedÃ©marrer
sudo systemctl restart redis

# 3. Planifier scaling urgent
```

### Ã‰tape 4: Restauration depuis backup (si nÃ©cessaire)
```bash
# 1. ArrÃªter Redis
sudo systemctl stop redis

# 2. Restaurer dump.rdb
sudo cp /backup/dump.rdb /var/lib/redis/

# 3. Permissions
sudo chown redis:redis /var/lib/redis/dump.rdb

# 4. DÃ©marrer
sudo systemctl start redis
```

## Post-mortem
- [ ] Documenter la cause racine
- [ ] Mettre Ã  jour les runbooks si nÃ©cessaire
- [ ] CrÃ©er ticket pour actions prÃ©ventives
- [ ] Communiquer aux stakeholders

## Escalation
Si non rÃ©solu en 15 min:
1. Appeler Secondary on-call
2. CrÃ©er incident Statuspage
3. Escalader au manager

## Liens utiles
- Dashboard: https://grafana.company.com/d/redis-overview
- Logs: https://kibana.company.com/app/redis
- Wiki: https://wiki.company.com/redis
```

### 5.2 GÃ©nÃ©ration automatique de runbooks

**Script pour gÃ©nÃ©rer runbooks depuis annotations** :
```python
#!/usr/bin/env python3
# generate-runbooks.py

import yaml
import os

def generate_runbook(alert):
    alertname = alert['alert']
    annotations = alert['annotations']
    labels = alert['labels']

    runbook = f"""# Runbook: {alertname}

## MÃ©tadonnÃ©es
- **Alerte:** {alertname}
- **Severity:** {labels.get('severity', 'N/A')}
- **Tier:** {labels.get('tier', 'N/A')}

## SymptÃ´mes
{annotations.get('summary', 'N/A')}

## Description
{annotations.get('description', 'N/A')}

## Impact
{annotations.get('impact', 'Ã€ documenter')}

## Diagnostic
{annotations.get('diagnostic', 'Ã€ documenter')}

## RÃ©solution
{annotations.get('resolution', 'Ã€ documenter')}

## Liens
- Dashboard: {annotations.get('dashboard_url', 'N/A')}
- Logs: {annotations.get('logs_url', 'N/A')}
"""

    filename = f"runbooks/{alertname}.md"
    os.makedirs('runbooks', exist_ok=True)

    with open(filename, 'w') as f:
        f.write(runbook)

    print(f"Generated: {filename}")

def main():
    # Charger les rÃ¨gles d'alerte
    with open('prometheus-rules.yml', 'r') as f:
        rules = yaml.safe_load(f)

    for group in rules.get('groups', []):
        for rule in group.get('rules', []):
            if 'alert' in rule:
                generate_runbook(rule)

if __name__ == '__main__':
    main()
```

## 6. Testing et Validation

### 6.1 Tests unitaires des alertes

**Outil: promtool** :
```bash
# Valider syntaxe des rÃ¨gles
promtool check rules /etc/prometheus/rules/*.yml

# Tester une rÃ¨gle avec donnÃ©es fictives
promtool test rules test-alerts.yml
```

**Fichier de test** (`test-alerts.yml`) :
```yaml
# Test RedisDown alert
rule_files:
  - ../prometheus-rules.yml

evaluation_interval: 1m

tests:
  # Test 1: Redis up â†’ pas d'alerte
  - interval: 1m
    input_series:
      - series: 'redis_up{instance="redis-1:9121"}'
        values: '1+0x10'  # Valeur 1 pendant 10 minutes

    alert_rule_test:
      - eval_time: 5m
        alertname: RedisDown
        exp_alerts: []  # Pas d'alerte attendue

  # Test 2: Redis down â†’ alerte aprÃ¨s 1 min
  - interval: 1m
    input_series:
      - series: 'redis_up{instance="redis-1:9121"}'
        values: '1 1 0+0x10'  # Down aprÃ¨s 2 min

    alert_rule_test:
      - eval_time: 2m
        alertname: RedisDown
        exp_alerts: []  # Pas encore (for: 1m)

      - eval_time: 3m
        alertname: RedisDown
        exp_alerts:
          - exp_labels:
              severity: critical
              instance: redis-1:9121
            exp_annotations:
              summary: "Redis instance redis-1:9121 is DOWN"
```

**ExÃ©cution** :
```bash
promtool test rules test-alerts.yml
# Unit Testing:  test-alerts.yml
#   SUCCESS
```

### 6.2 Tests d'intÃ©gration (chaos engineering)

**Simuler une panne Redis** :
```bash
#!/bin/bash
# test-redis-down-alert.sh

echo "=== Test: Redis Down Alert ==="

# 1. ArrÃªter Redis
echo "Stopping Redis..."
sudo systemctl stop redis
echo "Redis stopped at $(date)"

# 2. Attendre l'alerte (1 min for + 30s group_wait)
echo "Waiting 90 seconds for alert..."
sleep 90

# 3. VÃ©rifier Alertmanager
ALERTS=$(curl -s http://alertmanager:9093/api/v2/alerts | jq '.[] | select(.labels.alertname=="RedisDown")')

if [ -n "$ALERTS" ]; then
    echo "âœ… SUCCESS: Alert triggered"
    echo "$ALERTS" | jq .
else
    echo "âŒ FAILURE: Alert not triggered"
    exit 1
fi

# 4. RedÃ©marrer Redis
echo "Restarting Redis..."
sudo systemctl start redis

# 5. Attendre rÃ©solution
echo "Waiting for resolution..."
sleep 60

# 6. VÃ©rifier rÃ©solution
ALERTS=$(curl -s http://alertmanager:9093/api/v2/alerts | jq '.[] | select(.labels.alertname=="RedisDown" and .status.state=="active")')

if [ -z "$ALERTS" ]; then
    echo "âœ… SUCCESS: Alert resolved"
else
    echo "âš ï¸  Alert still active"
fi

echo "=== Test Complete ==="
```

### 6.3 Tests de notification

**Script de test notification** :
```bash
#!/bin/bash
# test-notifications.sh

# GÃ©nÃ©rer une alerte test via Alertmanager
curl -X POST http://alertmanager:9093/api/v1/alerts \
  -H "Content-Type: application/json" \
  -d '[
    {
      "labels": {
        "alertname": "TestAlert",
        "severity": "warning",
        "instance": "test-instance",
        "job": "test"
      },
      "annotations": {
        "summary": "Test alert - please ignore",
        "description": "This is a test alert to validate notifications"
      },
      "startsAt": "'$(date -u +%Y-%m-%dT%H:%M:%SZ)'",
      "endsAt": "'$(date -u -d '+5 minutes' +%Y-%m-%dT%H:%M:%SZ)'"
    }
  ]'

echo "Test alert sent. Check Slack/PagerDuty/Email."
```

## 7. MÃ©triques sur les Alertes

### 7.1 Self-monitoring des alertes

**MÃ©triques Alertmanager** :
```promql
# Nombre d'alertes actives
alertmanager_alerts{state="active"}

# Nombre d'alertes silencÃ©es
alertmanager_silences{state="active"}

# Notifications envoyÃ©es
rate(alertmanager_notifications_total[5m])

# Taux d'Ã©chec des notifications
rate(alertmanager_notifications_failed_total[5m]) /
rate(alertmanager_notifications_total[5m])

# Latence des notifications
alertmanager_notification_latency_seconds
```

**Dashboard Alertmanager** :
```json
{
  "panels": [
    {
      "title": "Active Alerts",
      "targets": [{
        "expr": "sum(alertmanager_alerts{state='active'}) by (alertname)"
      }]
    },
    {
      "title": "Notification Success Rate",
      "targets": [{
        "expr": "1 - (rate(alertmanager_notifications_failed_total[5m]) / rate(alertmanager_notifications_total[5m]))"
      }]
    }
  ]
}
```

### 7.2 Alertes sur les alertes

```yaml
# Meta-alerting
- alert: AlertmanagerDown
  expr: up{job="alertmanager"} == 0
  for: 5m
  labels:
    severity: critical
  annotations:
    summary: "Alertmanager is down"

- alert: TooManyAlerts
  expr: sum(alertmanager_alerts{state="active"}) > 50
  for: 10m
  labels:
    severity: warning
  annotations:
    summary: "Too many active alerts ({{ $value }})"
    description: "Possible alert storm or systemic issue"

- alert: AlertNotificationFailing
  expr: |
    rate(alertmanager_notifications_failed_total[5m]) /
    rate(alertmanager_notifications_total[5m]) > 0.1
  for: 10m
  labels:
    severity: warning
  annotations:
    summary: "Alert notifications failing"
    description: "{{ $value | humanizePercentage }} of notifications are failing"
```

## 8. Best Practices RÃ©capitulatives

### 8.1 Do's âœ…

- **Alerter sur les symptÃ´mes**, pas les causes
- **Une alerte = une action** claire et documentÃ©e
- **Runbooks** obligatoires pour alertes critiques
- **Tester** rÃ©guliÃ¨rement les alertes (chaos engineering)
- **Tuner** agressivement les seuils (Ã©viter faux positifs)
- **Grouper** les alertes similaires
- **Documenter** toutes les rÃ©solutions d'alertes
- **Review** mensuelle des alertes (ajouter/supprimer/ajuster)
- **Silencer** pendant les maintenances
- **Monitorer** les mÃ©triques d'alerting elles-mÃªmes

### 8.2 Don'ts âŒ

- âŒ Alerter sur des mÃ©triques non actionnables
- âŒ CrÃ©er des alertes "pour information"
- âŒ Avoir plus de 3 niveaux de sÃ©vÃ©ritÃ©
- âŒ Alertes sans runbook (critiques)
- âŒ Seuils arbitraires non basÃ©s sur l'expÃ©rience
- âŒ PÃ©riode "for" trop courte (< 1 min)
- âŒ RÃ©pÃ©ter les alertes trop frÃ©quemment
- âŒ Ignorer les alertes sans les corriger
- âŒ Laisser des alertes en mode "flapping"
- âŒ Alerter sur des Ã©vÃ©nements planifiÃ©s

### 8.3 Checklist de production

- [ ] Alertes Tier 1 (critiques) dÃ©finies et testÃ©es
- [ ] Alertes Tier 2 (warnings) configurÃ©es
- [ ] Runbooks crÃ©Ã©s pour toutes les alertes critiques
- [ ] Alertmanager configurÃ© avec HA
- [ ] Notifications PagerDuty/Slack fonctionnelles
- [ ] Routing correct par sÃ©vÃ©ritÃ©
- [ ] Inhibition rules configurÃ©es
- [ ] Silences programmables (API)
- [ ] Tests automatisÃ©s des alertes
- [ ] Dashboard de monitoring des alertes
- [ ] Escalation policy dÃ©finie
- [ ] Documentation Ã  jour
- [ ] Review mensuelle planifiÃ©e

## 9. MÃ©triques de QualitÃ© des Alertes

### 9.1 KPIs Ã  tracker

```promql
# Ratio signal/bruit
(count(ALERTS{alertstate="firing", severity="critical", resolved="false"})) /
(count(ALERTS{alertstate="firing"}))

# Temps moyen de rÃ©solution (MTTR)
avg(ALERTS_resolved_timestamp - ALERTS_firing_timestamp) by (alertname)

# Taux de faux positifs
count(ALERTS{resolved="true", duration="<5m"}) /
count(ALERTS{resolved="true"})

# Alert fatigue indicator
rate(alertmanager_alerts{state="active"}[24h]) >
avg_over_time(alertmanager_alerts{state="active"}[7d]) * 2
```

### 9.2 Objectifs

| MÃ©trique | Cible | Excellent | Critique |
|----------|-------|-----------|----------|
| **Faux positifs** | < 5% | < 2% | > 20% |
| **MTTR** | < 15 min | < 5 min | > 60 min |
| **Alertes/jour** | < 10 | < 5 | > 50 |
| **Couverture incidents** | 100% | 100% | < 90% |

## Conclusion

Un systÃ¨me d'alerting efficace est l'Ã©quilibre entre :

1. **SensibilitÃ©** : DÃ©tecter tous les vrais problÃ¨mes
2. **SpÃ©cificitÃ©** : Ã‰viter les faux positifs
3. **ActionnabilitÃ©** : Chaque alerte a une action claire
4. **MaintenabilitÃ©** : Review et amÃ©lioration continue

**Principes finaux** :
- Moins d'alertes = Mieux (si couverture complÃ¨te)
- Alerte = SymptÃ´me utilisateur + Action requise
- Runbooks = Obligatoires pour critiques
- Testing = Garant de la fiabilitÃ©
- Review = AmÃ©lioration continue

Un bon systÃ¨me d'alerting Redis permet de dormir tranquille tout en ayant la certitude d'Ãªtre rÃ©veillÃ© si nÃ©cessaire.

---

**Prochaine section** : 13.7 - Logs et audit trail (logging, conformitÃ©, forensics)

â­ï¸ [Logs et audit trail](/13-monitoring-observabilite/07-logs-audit-trail.md)
