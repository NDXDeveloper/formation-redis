ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 14.2 - Memory Analysis : --bigkeys, --memkeys et memory doctor

## ğŸ¯ Objectifs de cette section

- MaÃ®triser les outils natifs d'analyse mÃ©moire Redis
- Identifier et rÃ©soudre les problÃ¨mes de consommation mÃ©moire
- Diagnostiquer les big keys et leur impact sur les performances
- Optimiser l'utilisation mÃ©moire en production
- Mettre en place une stratÃ©gie de monitoring mÃ©moire proactive

---

## ğŸ“š Introduction : La mÃ©moire dans Redis

### Pourquoi l'analyse mÃ©moire est critique

Redis est une base de donnÃ©es **in-memory** :
- Toutes les donnÃ©es sont en RAM
- La mÃ©moire est la ressource la plus prÃ©cieuse
- Une mauvaise gestion â†’ OOM (Out Of Memory) â†’ crash

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  REDIS MEMORY ARCHITECTURE                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Data Memory (clÃ©s + valeurs)         â”‚    â”‚
â”‚  â”‚  - Strings, Lists, Sets, Hashes...    â”‚    â”‚
â”‚  â”‚  - 80-95% de la mÃ©moire totale        â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                â†“                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Overhead Memory                      â”‚    â”‚
â”‚  â”‚  - Structures internes                â”‚    â”‚
â”‚  â”‚  - Dictionnaires, pointeurs           â”‚    â”‚
â”‚  â”‚  - 5-15% de la mÃ©moire                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                â†“                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Buffer Memory                        â”‚    â”‚
â”‚  â”‚  - Client buffers                     â”‚    â”‚
â”‚  â”‚  - Replication buffer                 â”‚    â”‚
â”‚  â”‚  - AOF buffer                         â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                â†“                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Fragmentation                        â”‚    â”‚
â”‚  â”‚  - MÃ©moire inutilisÃ©e mais allouÃ©e    â”‚    â”‚
â”‚  â”‚  - Variable selon l'allocateur        â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Les symptÃ´mes de problÃ¨mes mÃ©moire

| SymptÃ´me | Cause probable | Impact |
|----------|----------------|--------|
| **OOM (Out Of Memory)** | MÃ©moire saturÃ©e | Crash Redis |
| **Ã‰victions frÃ©quentes** | Proche de maxmemory | Perte de donnÃ©es |
| **Fragmentation Ã©levÃ©e** | Allocation/dÃ©sallocation | Gaspillage RAM |
| **Latence Ã©levÃ©e** | Big keys | Blocage des commandes |
| **RÃ©plication lente** | Buffer de rÃ©plication plein | Lag master/replica |
| **Swap utilisÃ©** | RAM insuffisante | Performances catastrophiques |

### MÃ©triques mÃ©moire essentielles

```bash
# Commande rapide pour snapshot mÃ©moire
redis-cli INFO memory

# MÃ©triques clÃ©s Ã  surveiller :
used_memory_human        # MÃ©moire utilisÃ©e par les donnÃ©es
used_memory_rss_human    # MÃ©moire rÃ©elle (RSS) du process
used_memory_peak_human   # Pic historique
mem_fragmentation_ratio  # Ratio de fragmentation
maxmemory               # Limite configurÃ©e
evicted_keys            # Nombre de clÃ©s Ã©victÃ©es
```

---

## ğŸ”§ Tool 1 : redis-cli --bigkeys

### Qu'est-ce que --bigkeys ?

Un outil de **scan et analyse** qui parcourt tout le keyspace pour identifier les plus grosses clÃ©s.

**Fonctionnement** :
- Utilise SCAN en interne (non-bloquant)
- Ã‰chantillonne et mesure chaque type de donnÃ©es
- GÃ©nÃ¨re un rapport statistique

### Utilisation de base

```bash
# Scan complet du keyspace
redis-cli --bigkeys

# Avec authentification
redis-cli -a password --bigkeys

# SpÃ©cifier la base de donnÃ©es
redis-cli -n 1 --bigkeys

# Avec intervalle entre les scans (pour rÃ©duire l'impact)
redis-cli --bigkeys -i 0.1  # 100ms entre chaque scan
```

### Format de sortie

```bash
$ redis-cli --bigkeys

# Scanning the entire keyspace to find biggest keys as well as
# average sizes per key type.  You can use -i 0.1 to sleep 0.1 sec
# per 100 SCAN commands (not usually needed).

[00.00%] Sampled 1000 keys so far
[00.50%] Sampled 5000 keys so far
[01.00%] Sampled 10000 keys so far
...

-------- summary -------

Sampled 1000000 keys in the keyspace!
Total key length in bytes is 25000000 (avg len 25.00)

Biggest string found 'cache:homepage:en' has 5242880 bytes
Biggest list   found 'queue:tasks' has 100000 items
Biggest set    found 'tags:all' has 50000 members
Biggest hash   found 'user:12345:profile' has 10000 fields
Biggest zset   found 'leaderboard:global' has 1000000 members
Biggest stream found 'events:log' has 500000 entries

0 strings with 0 bytes (00.00% of keys, avg size 0.00)
450000 lists with 45000000 items (45.00% of keys, avg size 100.00)
300000 sets with 15000000 members (30.00% of keys, avg size 50.00)
200000 hashs with 40000000 fields (20.00% of keys, avg size 200.00)
50000 zsets with 25000000 members (05.00% of keys, avg size 500.00)
0 streams with 0 entries (00.00% of keys, avg size 0.00)
```

### InterprÃ©tation des rÃ©sultats

#### 1. Identifier les big keys problÃ©matiques

**Seuils problÃ©matiques** :

```
Type      | Taille normale | Big Key    | TrÃ¨s problÃ©matique
----------|----------------|------------|-------------------
String    | < 100 KB       | > 1 MB     | > 10 MB
List      | < 1000 items   | > 10K      | > 100K items
Set       | < 1000 members | > 10K      | > 100K members
Hash      | < 1000 fields  | > 10K      | > 100K fields
Sorted Set| < 1000 members | > 10K      | > 100K members
Stream    | < 10K entries  | > 100K     | > 1M entries
```

#### 2. Analyser la distribution

```bash
# Si vous avez beaucoup de big keys :
# â†’ ProblÃ¨me de modÃ¨le de donnÃ©es

# Distribution inÃ©gale (Pareto 80/20) :
# 80% de la mÃ©moire utilisÃ©e par 20% des clÃ©s
# â†’ Optimiser ces 20%

# Moyenne Ã©levÃ©e mais pas de grosses clÃ©s :
# â†’ ProblÃ¨me de volume global
```

### Limites de --bigkeys

âŒ **Ce que --bigkeys NE fait PAS** :
- Ne donne pas la mÃ©moire rÃ©elle utilisÃ©e (seulement le nombre d'Ã©lÃ©ments)
- N'analyse pas le contenu des clÃ©s
- Ne dÃ©tecte pas les "hot keys" (clÃ©s trÃ¨s sollicitÃ©es)
- Ã‰chantillonne seulement (pas exhaustif pour tous les types)

âœ… **Ce que --bigkeys FAIT bien** :
- Scan rapide et sÃ©curisÃ© (non-bloquant)
- Vue d'ensemble statistique
- Identification des outliers

---

## ğŸ” Tool 2 : redis-cli --memkeys (Redis 4.0+)

### Qu'est-ce que --memkeys ?

Une version **amÃ©liorÃ©e de --bigkeys** qui mesure la **mÃ©moire rÃ©elle** consommÃ©e, pas seulement le nombre d'Ã©lÃ©ments.

âš ï¸ **Note** : FonctionnalitÃ© disponible Ã  partir de Redis 4.0+

### Utilisation

```bash
# Analyse mÃ©moire complÃ¨te
redis-cli --memkeys

# Avec sampling (plus rapide)
redis-cli --memkeys --memkeys-samples 1000

# Avec intervalle pour rÃ©duire l'impact
redis-cli --memkeys -i 0.1
```

### Format de sortie

```bash
$ redis-cli --memkeys

# Scanning the entire keyspace to find biggest keys as well as
# average sizes per key type.

[00.00%] Sampled 1000 keys so far
...

-------- summary -------

Sampled 100000 keys in the keyspace!

Biggest string found 'cache:homepage:en' has 5242880 bytes
Biggest list   found 'queue:tasks' uses 15728640 bytes
Biggest set    found 'tags:all' uses 2097152 bytes
Biggest hash   found 'user:12345:profile' uses 8388608 bytes
Biggest zset   found 'leaderboard:global' uses 52428800 bytes

10000 strings with 104857600 bytes (10.00% of keys, avg size 10485.76)
30000 lists with 471859200 bytes (30.00% of keys, avg size 15728.64)
...
```

### Avantages de --memkeys vs --bigkeys

| CritÃ¨re | --bigkeys | --memkeys |
|---------|-----------|-----------|
| **MÃ©trique** | Nombre d'Ã©lÃ©ments | Bytes rÃ©els |
| **PrÃ©cision** | Approximative | Exacte |
| **Performance** | Rapide | Plus lent |
| **Version Redis** | Toutes | 4.0+ |
| **Usage** | PremiÃ¨re analyse | Deep dive |

### Cas d'usage optimal

```bash
# 1. First pass : Vue d'ensemble rapide
redis-cli --bigkeys -i 0.1

# 2. Second pass : Analyse mÃ©moire dÃ©taillÃ©e
redis-cli --memkeys --memkeys-samples 10000

# 3. Focus : Analyse spÃ©cifique d'une clÃ©
redis-cli MEMORY USAGE user:12345:profile SAMPLES 0
```

---

## ğŸ©º Tool 3 : MEMORY DOCTOR

### Qu'est-ce que MEMORY DOCTOR ?

Un **diagnostic automatisÃ©** qui analyse l'Ã©tat de la mÃ©moire et fournit des recommandations.

### Utilisation

```bash
# Diagnostic complet
redis-cli MEMORY DOCTOR
```

### Exemples de sorties

#### Cas 1 : Fragmentation Ã©levÃ©e

```
Sam, I detected a few issues in this Redis instance memory implants:

 * High fragmentation: This instance has a memory fragmentation
   greater than 1.4 (it is 1.52). This fragmentation is mainly
   due to changes in the data you are storing in Redis over time.

   Suggestion: Try to run 'MEMORY PURGE' or restart the instance
   to solve this issue. If the problem persists consider
   'activedefrag' configuration option.
```

#### Cas 2 : MÃ©moire quasi-saturÃ©e

```
Sam, I detected a few issues in this Redis instance memory implants:

 * Peak memory: The memory usage of this instance is very close
   to the peak memory usage. Current: 9.8 GB, Peak: 9.9 GB.

   Suggestion: You are near the memory limit. Consider increasing
   the maxmemory limit, enabling eviction policies, or archiving
   old data.
```

#### Cas 3 : Big keys dÃ©tectÃ©s

```
Sam, I detected a few issues in this Redis instance memory implants:

 * Big keys: Your instance contains keys that are using a lot of
   memory. Run 'redis-cli --bigkeys' to find them.

   Suggestion: Consider splitting large keys into smaller ones or
   using different data structures.
```

#### Cas 4 : Tout va bien

```
Sam, this instance memory is OK.
```

### Analyse des recommandations

| Message | Signification | Action |
|---------|---------------|--------|
| **High fragmentation** | mem_frag_ratio > 1.4 | MEMORY PURGE ou restart |
| **Peak memory** | Proche de maxmemory | Augmenter limite ou Ã©viction |
| **Big keys** | Grosses clÃ©s prÃ©sentes | Identifier et refactorer |
| **High clients count** | Trop de clients connectÃ©s | VÃ©rifier les connection pools |
| **AOF buffer high** | Buffer AOF volumineux | Optimiser AOF ou augmenter RAM |

---

## ğŸ”¬ Commandes MEMORY avancÃ©es (Redis 4.0+)

### MEMORY USAGE <key> [SAMPLES <count>]

Mesure la mÃ©moire exacte utilisÃ©e par une clÃ© spÃ©cifique.

```bash
# MÃ©moire utilisÃ©e par une clÃ©
redis-cli MEMORY USAGE user:12345:profile

# RÃ©sultat : (integer) 8388736  # 8 MB

# Avec Ã©chantillonnage (plus prÃ©cis)
redis-cli MEMORY USAGE user:12345:profile SAMPLES 5

# Pour tous les types
redis-cli MEMORY USAGE mystring
redis-cli MEMORY USAGE mylist
redis-cli MEMORY USAGE myset
redis-cli MEMORY USAGE myhash
redis-cli MEMORY USAGE myzset
redis-cli MEMORY USAGE mystream
```

**Calcul de la mÃ©moire** :
- Inclut la valeur + overhead Redis (pointeurs, structures)
- PrÃ©cision dÃ©pend du paramÃ¨tre SAMPLES (0 = rapide, 5 = prÃ©cis)

### MEMORY STATS

Statistiques dÃ©taillÃ©es sur l'utilisation mÃ©moire.

```bash
redis-cli MEMORY STATS
```

**Sortie dÃ©taillÃ©e** :

```
 1) "peak.allocated"
 2) (integer) 10737418240    # 10 GB pic historique
 3) "total.allocated"
 4) (integer) 8589934592     # 8 GB actuellement allouÃ©s
 5) "startup.allocated"
 6) (integer) 1048576        # 1 MB au dÃ©marrage
 7) "replication.backlog"
 8) (integer) 1048576        # 1 MB backlog rÃ©plication
 9) "clients.slaves"
10) (integer) 0              # 0 bytes pour les replicas
11) "clients.normal"
12) (integer) 16384          # 16 KB pour les clients normaux
13) "aof.buffer"
14) (integer) 0              # AOF buffer vide
15) "db.0"
16) 1) "overhead.hashtable.main"
    2) (integer) 1048576     # Overhead des tables de hash
    3) "overhead.hashtable.expires"
    4) (integer) 524288      # Overhead des expirations
17) "overhead.total"
18) (integer) 2621440        # Total overhead
19) "keys.count"
20) (integer) 1000000        # 1M clÃ©s
21) "keys.bytes-per-key"
22) (integer) 128            # 128 bytes overhead par clÃ©
23) "dataset.bytes"
24) (integer) 8587313152     # 8 GB de donnÃ©es pures
25) "dataset.percentage"
26) "99.97"                  # 99.97% de la mÃ©moire = donnÃ©es
27) "peak.percentage"
28) "79.99"                  # 80% du pic historique
29) "fragmentation"
30) "1.25"                   # Ratio de fragmentation
```

### MEMORY PURGE

Force la libÃ©ration de mÃ©moire fragmentÃ©e (utilise l'allocateur systÃ¨me).

```bash
redis-cli MEMORY PURGE
# RÃ©sultat : OK

# VÃ©rifier l'impact
redis-cli INFO memory | grep mem_fragmentation_ratio
```

âš ï¸ **Attention** : Peut bloquer Redis pendant quelques millisecondes.

### MEMORY MALLOC-STATS

Statistiques de l'allocateur mÃ©moire (jemalloc par dÃ©faut).

```bash
redis-cli MEMORY MALLOC-STATS
```

**Sortie complexe** (extrait) :

```
___ Begin jemalloc statistics ___
Version: 5.2.1
...
Allocated: 8589934592, active: 8724152320, metadata: 159383552,
resident: 9112182784, mapped: 10737418240
...
```

**UtilitÃ©** : Diagnostic avancÃ© de fragmentation et leak mÃ©moire.

---

## ğŸ“Š MÃ©thodologie d'analyse complÃ¨te

### Phase 1 : Vue d'ensemble (5 minutes)

```bash
#!/bin/bash
# quick-memory-check.sh

echo "=== REDIS MEMORY QUICK CHECK ==="
echo ""

# 1. MÃ©triques de base
echo "--- Basic Metrics ---"
redis-cli INFO memory | grep -E "used_memory_human|used_memory_rss_human|used_memory_peak_human|mem_fragmentation_ratio|maxmemory_human|evicted_keys"

echo ""

# 2. MEMORY DOCTOR
echo "--- MEMORY DOCTOR ---"
redis-cli MEMORY DOCTOR

echo ""

# 3. Distribution par base de donnÃ©es
echo "--- Database Sizes ---"
for db in {0..15}; do
    size=$(redis-cli -n $db DBSIZE 2>/dev/null)
    if [ "$size" != "0" ]; then
        echo "DB $db: $size keys"
    fi
done
```

### Phase 2 : Identification des big keys (15 minutes)

```bash
#!/bin/bash
# analyze-bigkeys.sh

TIMESTAMP=$(date +%Y%m%d_%H%M%S)
OUTPUT_DIR="memory_analysis_${TIMESTAMP}"
mkdir -p $OUTPUT_DIR

echo "=== BIG KEYS ANALYSIS ==="
echo ""

# 1. Scan avec --bigkeys
echo "Running --bigkeys scan..."
redis-cli --bigkeys -i 0.1 > ${OUTPUT_DIR}/bigkeys_report.txt 2>&1

# 2. Extraire les plus grosses clÃ©s
echo "Top 10 biggest keys:"
grep "Biggest" ${OUTPUT_DIR}/bigkeys_report.txt

# 3. Mesurer la mÃ©moire de chaque big key
echo ""
echo "Measuring memory usage..."

# Extraire les noms de clÃ©s
grep "Biggest" ${OUTPUT_DIR}/bigkeys_report.txt | \
  awk '{print $(NF-2)}' | \
  sed "s/'//g" | \
  while read key; do
    memory=$(redis-cli MEMORY USAGE "$key" 2>/dev/null)
    type=$(redis-cli TYPE "$key" 2>/dev/null)
    ttl=$(redis-cli TTL "$key" 2>/dev/null)
    echo "Key: $key | Type: $type | Memory: $memory bytes | TTL: $ttl"
  done > ${OUTPUT_DIR}/bigkeys_memory.txt

cat ${OUTPUT_DIR}/bigkeys_memory.txt

echo ""
echo "Reports saved in: $OUTPUT_DIR/"
```

### Phase 3 : Analyse dÃ©taillÃ©e (30 minutes)

```python
#!/usr/bin/env python3
"""
Advanced memory analysis script
"""
import redis
import sys
from collections import defaultdict

def analyze_memory_distribution(host='localhost', port=6379, samples=10000):
    """Analyse la distribution mÃ©moire par pattern de clÃ©s"""

    r = redis.Redis(host=host, port=port, decode_responses=True)

    # Collecter les clÃ©s par pattern
    patterns = defaultdict(list)
    cursor = 0
    scanned = 0

    print("Scanning keyspace...")

    while True:
        cursor, keys = r.scan(cursor, count=100)

        for key in keys:
            # Extraire le pattern (prÃ©fixe avant le premier ':')
            if ':' in key:
                pattern = key.split(':')[0] + ':*'
            else:
                pattern = 'no-prefix'

            # Mesurer la mÃ©moire (sampling)
            if len(patterns[pattern]) < 100:  # Limiter Ã  100 Ã©chantillons par pattern
                try:
                    memory = r.memory_usage(key)
                    if memory:
                        patterns[pattern].append({
                            'key': key,
                            'memory': memory,
                            'type': r.type(key)
                        })
                except:
                    pass

        scanned += len(keys)
        if scanned >= samples or cursor == 0:
            break

        if scanned % 1000 == 0:
            print(f"Scanned {scanned} keys...")

    # Analyse par pattern
    print("\n=== MEMORY DISTRIBUTION BY PATTERN ===\n")

    pattern_stats = []

    for pattern, keys in patterns.items():
        if not keys:
            continue

        total_memory = sum(k['memory'] for k in keys)
        avg_memory = total_memory / len(keys)
        max_memory = max(k['memory'] for k in keys)

        pattern_stats.append({
            'pattern': pattern,
            'samples': len(keys),
            'total_mb': total_memory / 1024 / 1024,
            'avg_bytes': avg_memory,
            'max_bytes': max_memory
        })

    # Trier par mÃ©moire totale
    pattern_stats.sort(key=lambda x: x['total_mb'], reverse=True)

    # Afficher les rÃ©sultats
    print(f"{'Pattern':<30} {'Samples':<10} {'Total (MB)':<15} {'Avg (bytes)':<15} {'Max (bytes)':<15}")
    print("-" * 90)

    for stat in pattern_stats[:20]:  # Top 20
        print(f"{stat['pattern']:<30} {stat['samples']:<10} {stat['total_mb']:<15.2f} {stat['avg_bytes']:<15.0f} {stat['max_bytes']:<15.0f}")

    # Calculer le total
    total_sampled_mb = sum(s['total_mb'] for s in pattern_stats)
    print("-" * 90)
    print(f"{'TOTAL (sampled)':<30} {scanned:<10} {total_sampled_mb:<15.2f}")

    # Identifier les problÃ¨mes
    print("\n=== ISSUES DETECTED ===\n")

    for stat in pattern_stats:
        issues = []

        if stat['avg_bytes'] > 1024 * 1024:  # Moyenne > 1MB
            issues.append(f"High average size: {stat['avg_bytes']/1024/1024:.2f} MB")

        if stat['max_bytes'] > 10 * 1024 * 1024:  # Max > 10MB
            issues.append(f"Very large key found: {stat['max_bytes']/1024/1024:.2f} MB")

        if issues:
            print(f"Pattern: {stat['pattern']}")
            for issue in issues:
                print(f"  âš ï¸  {issue}")
            print()

if __name__ == "__main__":
    analyze_memory_distribution()
```

---

## ğŸ­ Patterns de problÃ¨mes mÃ©moire classiques

### Pattern 1 : Big String Keys

**Signature** :
```bash
redis-cli --bigkeys
# Biggest string found 'cache:page:homepage' has 52428800 bytes (50MB)
```

**Diagnostic** :

```bash
# VÃ©rifier le contenu
redis-cli --raw GET cache:page:homepage | head -c 1000

# Type d'encoding
redis-cli OBJECT ENCODING cache:page:homepage
# RÃ©sultat : "raw" (indique un string volumineux)

# MÃ©moire exacte
redis-cli MEMORY USAGE cache:page:homepage
# RÃ©sultat : 52428800
```

**Solutions** :

```bash
# Option 1 : Compression cÃ´tÃ© application
# Avant : stocker HTML brut (50MB)
# AprÃ¨s : stocker HTML gzippÃ© (5MB)

# Option 2 : Fragmenter en plusieurs clÃ©s
# Au lieu de : cache:page:homepage (50MB)
# Faire :
#   cache:page:homepage:header (5MB)
#   cache:page:homepage:body (40MB)
#   cache:page:homepage:footer (5MB)

# Option 3 : Utiliser RedisJSON pour structure
# Permet d'accÃ©der Ã  des parties sans tout charger
JSON.SET cache:page:homepage $ '{"header": "...", "body": "...", "footer": "..."}'
JSON.GET cache:page:homepage $.header
```

### Pattern 2 : Large Hash avec beaucoup de champs

**Signature** :
```bash
redis-cli --bigkeys
# Biggest hash found 'user:12345:profile' has 100000 fields
```

**Diagnostic** :

```bash
# Nombre de champs
redis-cli HLEN user:12345:profile
# RÃ©sultat : 100000

# MÃ©moire totale
redis-cli MEMORY USAGE user:12345:profile
# RÃ©sultat : 15728640 (15MB)

# Ã‰chantillonner quelques champs
redis-cli HSCAN user:12345:profile 0 COUNT 10
```

**Causes courantes** :
- ModÃ¨le de donnÃ©es inadaptÃ©
- Accumulation sans nettoyage
- Utilisation comme "table" SQL

**Solutions** :

```bash
# âŒ UN SEUL gros hash
HSET user:12345:profile field1 value1 field2 value2 ... field100000 value100000

# âœ… SPLIT en plusieurs hash par catÃ©gorie
HSET user:12345:basic name "John" email "john@example.com"
HSET user:12345:preferences theme "dark" language "en"
HSET user:12345:stats loginCount "150" lastLogin "2024-12-11"

# âœ… Ou utiliser une structure diffÃ©rente
# Redis Streams ou Sorted Sets selon le cas d'usage
```

### Pattern 3 : Sorted Sets massifs

**Signature** :
```bash
redis-cli --bigkeys
# Biggest zset found 'leaderboard:global' has 10000000 members
```

**Diagnostic** :

```bash
# Taille du sorted set
redis-cli ZCARD leaderboard:global
# RÃ©sultat : 10000000

# MÃ©moire
redis-cli MEMORY USAGE leaderboard:global
# RÃ©sultat : 800000000 (800MB!)

# VÃ©rifier la distribution des scores
redis-cli ZRANGE leaderboard:global 0 10 WITHSCORES
redis-cli ZREVRANGE leaderboard:global 0 10 WITHSCORES
```

**Solutions** :

```bash
# Option 1 : Limiter la taille (top N)
# Garder seulement le top 100K
ZREMRANGEBYRANK leaderboard:global 0 -100001

# Option 2 : Archiver les vieux scores
# DÃ©placer les anciens dans une autre structure
ZRANGEBYSCORE leaderboard:global -inf (timestamp-30days) | xargs ZREM

# Option 3 : Partitionner par pÃ©riode
# Au lieu de : leaderboard:global (10M membres)
# Faire :
#   leaderboard:2024-12 (100K membres)
#   leaderboard:2024-11 (100K membres)
#   ...
```

### Pattern 4 : Lists comme queues non-drainÃ©es

**Signature** :
```bash
redis-cli --bigkeys
# Biggest list found 'queue:pending' has 5000000 items
```

**Diagnostic** :

```bash
# Taille de la liste
redis-cli LLEN queue:pending
# RÃ©sultat : 5000000

# VÃ©rifier si elle grossit
redis-cli LLEN queue:pending
# Attendre 10 secondes
redis-cli LLEN queue:pending
# Si augmente â†’ problÃ¨me de consumer

# Ã‰chantillon du dÃ©but et de la fin
redis-cli LRANGE queue:pending 0 5
redis-cli LRANGE queue:pending -5 -1
```

**Causes** :
- Producers trop rapides
- Consumers en panne
- Logique de traitement trop lente

**Solutions** :

```bash
# Option 1 : Augmenter les consumers
# Scaler horizontalement le traitement

# Option 2 : Rate limiting sur les producers
# Limiter l'injection dans la queue

# Option 3 : Migrer vers Redis Streams
# Meilleure gestion des consumers groups
XADD mystream * field1 value1 field2 value2
XREADGROUP GROUP mygroup consumer1 COUNT 100 STREAMS mystream >

# Option 4 : TTL sur les messages (avec Streams)
# Supprimer automatiquement les vieux messages
XTRIM mystream MAXLEN ~ 100000
```

### Pattern 5 : ClÃ©s temporaires non-nettoyÃ©es

**Signature** :
```bash
redis-cli --bigkeys
# Beaucoup de clÃ©s avec pattern "temp:*" ou "lock:*"
```

**Diagnostic** :

```bash
# Compter les clÃ©s par pattern
redis-cli --scan --pattern "temp:*" | wc -l

# VÃ©rifier les TTL
redis-cli --scan --pattern "temp:*" | head -20 | \
  while read key; do
    ttl=$(redis-cli TTL "$key")
    echo "$key : TTL=$ttl"
  done
```

**Causes** :
- TTL non dÃ©fini
- Crashes avant nettoyage
- Logique de cleanup dÃ©faillante

**Solutions** :

```bash
# âœ… TOUJOURS dÃ©finir un TTL sur les clÃ©s temporaires
SET temp:session:abc123 value EX 3600  # 1 heure

# âœ… Utiliser SETEX pour atomicitÃ©
SETEX temp:lock:resource 30 "locked"

# âœ… Script de nettoyage pÃ©riodique
# Cleanup job toutes les heures
0 * * * * /usr/local/bin/redis-cleanup-temp-keys.sh

# âœ… Monitoring des clÃ©s sans TTL
redis-cli --scan --pattern "temp:*" | \
  while read key; do
    ttl=$(redis-cli TTL "$key")
    if [ "$ttl" = "-1" ]; then
      echo "WARNING: $key has no TTL"
    fi
  done
```

---

## ğŸš¨ Guide de rÃ©solution : Out Of Memory imminent

### ScÃ©nario : Redis proche de maxmemory

**Alertes** :
- `used_memory` > 90% de `maxmemory`
- Ã‰victions commencent
- Applications signalent des erreurs

### Investigation immÃ©diate (< 5 minutes)

```bash
# 1. Ã‰tat actuel
redis-cli INFO memory | grep -E "used_memory_human|maxmemory_human|evicted_keys"

# 2. VÃ©rifier les big keys
redis-cli --bigkeys -i 0.01 | grep "Biggest"

# 3. MEMORY DOCTOR
redis-cli MEMORY DOCTOR
```

### Actions d'urgence

#### Option 1 : Augmenter temporairement maxmemory

```bash
# âš ï¸ Solution temporaire uniquement
redis-cli CONFIG SET maxmemory 16gb

# VÃ©rifier RAM disponible sur le serveur
free -h

# Attention : ne pas dÃ©passer 80% de la RAM totale
```

#### Option 2 : Activer/ajuster la politique d'Ã©viction

```bash
# Si pas d'Ã©viction configurÃ©e
redis-cli CONFIG SET maxmemory-policy allkeys-lru

# Ou politique plus agressive
redis-cli CONFIG SET maxmemory-policy volatile-lru
```

#### Option 3 : Nettoyage manuel urgent

```bash
# Identifier et supprimer des big keys non-critiques
redis-cli --bigkeys | grep "Biggest"

# Supprimer des clÃ©s temporaires obsolÃ¨tes
redis-cli --scan --pattern "temp:*" | \
  while read key; do
    ttl=$(redis-cli TTL "$key")
    if [ "$ttl" = "-1" ]; then
      redis-cli DEL "$key"
    fi
  done

# Nettoyer des caches expirÃ©s
redis-cli --scan --pattern "cache:*" | \
  head -1000 | \
  xargs redis-cli DEL
```

### Analyse post-crise (< 1 heure)

```python
#!/usr/bin/env python3
"""
Post-mortem memory analysis
"""
import redis
import json
from datetime import datetime

def post_mortem_analysis(host='localhost', port=6379):
    r = redis.Redis(host=host, port=port, decode_responses=True)

    report = {
        'timestamp': datetime.now().isoformat(),
        'memory_info': {},
        'top_memory_consumers': [],
        'recommendations': []
    }

    # MÃ©triques mÃ©moire
    info = r.info('memory')
    report['memory_info'] = {
        'used_memory_mb': info['used_memory'] / 1024 / 1024,
        'used_memory_peak_mb': info['used_memory_peak'] / 1024 / 1024,
        'fragmentation_ratio': info['mem_fragmentation_ratio'],
        'evicted_keys': info.get('evicted_keys', 0)
    }

    # Scanner les big keys
    print("Scanning for big keys...")
    cursor = 0
    big_keys = []

    while True:
        cursor, keys = r.scan(cursor, count=100)

        for key in keys:
            try:
                memory = r.memory_usage(key)
                if memory and memory > 1024 * 1024:  # > 1MB
                    big_keys.append({
                        'key': key,
                        'type': r.type(key),
                        'memory_mb': memory / 1024 / 1024,
                        'ttl': r.ttl(key)
                    })
            except:
                pass

        if cursor == 0:
            break

    # Top 20
    big_keys.sort(key=lambda x: x['memory_mb'], reverse=True)
    report['top_memory_consumers'] = big_keys[:20]

    # Recommandations
    if report['memory_info']['fragmentation_ratio'] > 1.5:
        report['recommendations'].append(
            "High fragmentation detected. Consider restarting Redis or enabling active defragmentation."
        )

    if report['memory_info']['evicted_keys'] > 0:
        report['recommendations'].append(
            f"{report['memory_info']['evicted_keys']} keys were evicted. Increase maxmemory or optimize data model."
        )

    if len(big_keys) > 10:
        report['recommendations'].append(
            f"{len(big_keys)} keys are using more than 1MB each. Review data model and consider splitting large keys."
        )

    # Sauvegarder le rapport
    filename = f"memory_postmortem_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(filename, 'w') as f:
        json.dump(report, f, indent=2)

    print(f"\nPost-mortem report saved to: {filename}")
    print("\n=== SUMMARY ===")
    print(f"Used memory: {report['memory_info']['used_memory_mb']:.2f} MB")
    print(f"Peak memory: {report['memory_info']['used_memory_peak_mb']:.2f} MB")
    print(f"Fragmentation: {report['memory_info']['fragmentation_ratio']:.2f}")
    print(f"Evicted keys: {report['memory_info']['evicted_keys']}")
    print(f"\nTop 5 memory consumers:")
    for i, key in enumerate(report['top_memory_consumers'][:5], 1):
        print(f"  {i}. {key['key']} ({key['type']}): {key['memory_mb']:.2f} MB")

if __name__ == "__main__":
    post_mortem_analysis()
```

---

## ğŸ“ˆ Monitoring continu et prÃ©ventif

### MÃ©triques clÃ©s Ã  monitorer

```yaml
# Prometheus metrics to track
- redis_memory_used_bytes
- redis_memory_max_bytes
- redis_memory_fragmentation_ratio
- redis_evicted_keys_total
- redis_memory_used_peak_bytes
- redis_mem_clients_slaves
- redis_mem_clients_normal
```

### Alertes recommandÃ©es

```yaml
# alerting-rules.yml

groups:
  - name: redis_memory
    rules:
      # Alerte : MÃ©moire > 80%
      - alert: RedisMemoryHigh
        expr: redis_memory_used_bytes / redis_memory_max_bytes > 0.8
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Redis memory usage is high"
          description: "{{ $labels.instance }} is using {{ $value | humanizePercentage }} of maxmemory"

      # Alerte : MÃ©moire > 95%
      - alert: RedisMemoryCritical
        expr: redis_memory_used_bytes / redis_memory_max_bytes > 0.95
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Redis memory usage is critical"
          description: "{{ $labels.instance }} is using {{ $value | humanizePercentage }} of maxmemory - OOM imminent"

      # Alerte : Fragmentation Ã©levÃ©e
      - alert: RedisHighFragmentation
        expr: redis_memory_fragmentation_ratio > 1.5
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Redis memory fragmentation is high"
          description: "{{ $labels.instance }} fragmentation ratio is {{ $value }}"

      # Alerte : Ã‰victions actives
      - alert: RedisEvictionsOccurring
        expr: rate(redis_evicted_keys_total[5m]) > 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Redis is evicting keys"
          description: "{{ $labels.instance }} is evicting {{ $value }} keys/sec"
```

### Dashboard Grafana

**Panels essentiels** :

```
1. Memory Usage (gauge)
   - used_memory vs maxmemory
   - Seuils : 80% warning, 95% critical

2. Memory Fragmentation (time series)
   - Ratio de fragmentation dans le temps
   - Ligne de rÃ©fÃ©rence Ã  1.5

3. Evictions (counter)
   - Nombre de clÃ©s Ã©victÃ©es
   - Rate par seconde

4. Big Keys Alert (table)
   - Liste des plus grosses clÃ©s
   - Mise Ã  jour quotidienne

5. Memory by Component (pie chart)
   - Dataset vs overhead vs buffers
```

---

## ğŸ“ Best Practices

### Configuration

âœ… **DO**
- DÃ©finir `maxmemory` Ã  80% de la RAM disponible
- Activer `maxmemory-policy` appropriÃ©e
- Configurer `activedefrag yes` si fragmentation rÃ©currente
- Monitorer en continu la mÃ©moire

âŒ **DON'T**
- Ne jamais laisser `maxmemory 0` (illimitÃ©) en production
- Ne pas ignorer les alertes de mÃ©moire
- Ne pas stocker de gros objets binaires dans Redis sans compression

### ModÃ¨le de donnÃ©es

âœ… **DO**
- Fragmenter les big keys en plusieurs petites clÃ©s
- Utiliser les structures de donnÃ©es appropriÃ©es
- DÃ©finir des TTL sur les donnÃ©es temporaires
- Compresser les gros strings cÃ´tÃ© application

âŒ **DON'T**
- Ne pas utiliser Redis comme stockage de fichiers
- Ne pas stocker des documents XML/JSON > 1MB sans structure
- Ne pas laisser des listes/sets grossir indÃ©finiment

### Maintenance

âœ… **DO**
- Scanner rÃ©guliÃ¨rement avec `--bigkeys` (hebdomadaire)
- Analyser `MEMORY STATS` mensuellement
- Auditer les patterns de clÃ©s
- Nettoyer les clÃ©s obsolÃ¨tes

âŒ **DON'T**
- Ne jamais ignorer `MEMORY DOCTOR`
- Ne pas nÃ©gliger la fragmentation > 1.5
- Ne pas reporter les nettoyages de clÃ©s obsolÃ¨tes

---

## ğŸ”— Checklist d'analyse mÃ©moire

### Analyse rapide (quotidienne)

- [ ] VÃ©rifier `used_memory` vs `maxmemory`
- [ ] Consulter `mem_fragmentation_ratio`
- [ ] VÃ©rifier `evicted_keys`
- [ ] ExÃ©cuter `MEMORY DOCTOR`

### Analyse approfondie (hebdomadaire)

- [ ] ExÃ©cuter `redis-cli --bigkeys`
- [ ] Analyser la distribution par pattern
- [ ] Identifier les croissances anormales
- [ ] VÃ©rifier les clÃ©s sans TTL

### Audit complet (mensuel)

- [ ] ExÃ©cuter `redis-cli --memkeys`
- [ ] Analyser `MEMORY STATS` en dÃ©tail
- [ ] Auditer le modÃ¨le de donnÃ©es
- [ ] Planifier les optimisations nÃ©cessaires
- [ ] Documenter les changements

---

## ğŸ“š Ressources complÃ©mentaires

### Documentation officielle
- [Redis MEMORY commands](https://redis.io/commands/?group=server)
- [Memory Optimization](https://redis.io/docs/management/optimization/memory-optimization/)
- [Understanding Redis memory](https://redis.io/docs/management/optimization/memory-optimization/)

### Outils
- **Redis Insight** : GUI avec analyseur mÃ©moire intÃ©grÃ©
- **redis-rdb-tools** : Analyse offline des fichiers RDB
- **redis-memory-analyzer** : Outil tiers d'analyse

### Scripts utiles
- Scripts d'analyse dans cette section (Python, Bash)
- Dashboards Grafana prÃ©-configurÃ©s
- Alertes Prometheus

---

## ğŸ¯ Points clÃ©s Ã  retenir

1. **--bigkeys pour vue d'ensemble** â†’ Rapide mais approximatif
2. **--memkeys pour prÃ©cision** â†’ Plus lent mais exact (Redis 4.0+)
3. **MEMORY DOCTOR** â†’ Diagnostic automatisÃ©, toujours l'Ã©couter
4. **MEMORY USAGE** â†’ Mesure exacte par clÃ©
5. **Big keys = goulot d'Ã©tranglement** â†’ Fragmenter ou optimiser
6. **Monitoring continu** â†’ PrÃ©venir vaut mieux que guÃ©rir
7. **Fragmentation > 1.5** â†’ Action nÃ©cessaire
8. **Ã‰victions = signal d'alarme** â†’ Augmenter RAM ou optimiser

---

**ğŸš€ Section suivante** : [14.3 - Debugging avancÃ© : MONITOR, CLIENT LIST, CLIENT KILL](./03-debugging-avance-monitor-client.md)

â­ï¸ [Debugging avancÃ© : MONITOR, CLIENT LIST, CLIENT KILL](/14-performance-troubleshooting/03-debugging-avance-monitor-client.md)
