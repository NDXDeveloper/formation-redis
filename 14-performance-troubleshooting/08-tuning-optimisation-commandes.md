ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 14.8 - Tuning et optimisation des commandes

## ğŸ¯ Objectifs de cette section

- MaÃ®triser la complexitÃ© algorithmique des commandes Redis
- Identifier et remplacer les commandes inefficaces
- Optimiser les patterns d'accÃ¨s aux donnÃ©es
- Utiliser les techniques avancÃ©es (pipelining, transactions)
- Mesurer et amÃ©liorer les performances
- Adopter les best practices d'optimisation

---

## ğŸ“š Introduction : L'importance de l'optimisation

### Pourquoi optimiser les commandes ?

Redis est rapide, mais **une commande mal choisie peut bloquer toute l'instance**.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  REDIS = SINGLE-THREADED                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                 â”‚
â”‚  Commande O(1)  : 0.05ms  âœ…                    â”‚
â”‚  Commande O(N)  : 50ms    âš ï¸                    â”‚
â”‚  Commande O(NÂ²) : 5000ms  âŒ CATASTROPHE        â”‚
â”‚                                                 â”‚
â”‚  Une seule commande lente bloque TOUTES         â”‚
â”‚  les autres requÃªtes en attente                 â”‚
â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Impact d'une commande lente** :

```
Exemple : KEYS * sur 10M de clÃ©s

Temps d'exÃ©cution : 5 secondes
Pendant ce temps :
  - 0 autres commandes exÃ©cutÃ©es
  - 50,000 requÃªtes en attente
  - Timeout cÃ´tÃ© clients
  - DÃ©gradation complÃ¨te du service
```

### Les 3 piliers de l'optimisation

```
1. COMPLEXITÃ‰ ALGORITHMIQUE
   â””â”€ Choisir des commandes O(1) ou O(log N)

2. VOLUME DE DONNÃ‰ES
   â””â”€ Limiter la quantitÃ© de donnÃ©es transfÃ©rÃ©es

3. NOMBRE D'APPELS
   â””â”€ RÃ©duire les round-trips rÃ©seau
```

---

## ğŸ“Š ComplexitÃ© algorithmique des commandes

### Notation Big O : Rappel

```
O(1)      : Temps constant (idÃ©al)
O(log N)  : Logarithmique (trÃ¨s bon)
O(N)      : LinÃ©aire (acceptable si N petit)
O(N log N): N log N (coÃ»teux)
O(NÂ²)     : Quadratique (Ã‰VITER)
O(N*M)    : Produit (Ã‰VITER)
```

### Tableau de complexitÃ© par structure

#### Strings

| Commande | ComplexitÃ© | Notes |
|----------|------------|-------|
| GET | O(1) | âœ… IdÃ©al |
| SET | O(1) | âœ… IdÃ©al |
| INCR/DECR | O(1) | âœ… Atomique |
| GETRANGE | O(N) | N = longueur substring |
| SETRANGE | O(N) | N = longueur Ã  Ã©crire |
| APPEND | O(1) | âœ… Amorti |
| STRLEN | O(1) | âœ… IdÃ©al |

#### Lists

| Commande | ComplexitÃ© | Notes |
|----------|------------|-------|
| LPUSH/RPUSH | O(1) | âœ… IdÃ©al |
| LPOP/RPOP | O(1) | âœ… IdÃ©al |
| LLEN | O(1) | âœ… IdÃ©al |
| LINDEX | O(N) | N = index (lent au milieu) |
| LRANGE | O(S+N) | S = offset, N = count |
| LINSERT | O(N) | âš ï¸ Ã‰viter si possible |
| LSET | O(N) | âš ï¸ Ã‰viter si possible |
| LTRIM | O(N) | N = Ã©lÃ©ments supprimÃ©s |

#### Sets

| Commande | ComplexitÃ© | Notes |
|----------|------------|-------|
| SADD | O(1) | âœ… Par Ã©lÃ©ment |
| SREM | O(N) | N = nombre d'Ã©lÃ©ments |
| SISMEMBER | O(1) | âœ… IdÃ©al |
| SCARD | O(1) | âœ… IdÃ©al |
| SMEMBERS | O(N) | âŒ Ã‰VITER (retourne tout) |
| SSCAN | O(1) | âœ… Par itÃ©ration |
| SINTER | O(N*M) | N = plus petit set |
| SUNION | O(N) | N = somme tous sets |
| SDIFF | O(N) | N = somme tous sets |

#### Hashes

| Commande | ComplexitÃ© | Notes |
|----------|------------|-------|
| HGET | O(1) | âœ… IdÃ©al |
| HSET | O(1) | âœ… IdÃ©al |
| HMGET | O(N) | N = nombre de fields |
| HMSET | O(N) | N = nombre de fields |
| HLEN | O(1) | âœ… IdÃ©al |
| HGETALL | O(N) | âŒ Ã‰VITER (retourne tout) |
| HSCAN | O(1) | âœ… Par itÃ©ration |
| HKEYS/HVALS | O(N) | âš ï¸ Retourne tout |
| HINCRBY | O(1) | âœ… Atomique |

#### Sorted Sets

| Commande | ComplexitÃ© | Notes |
|----------|------------|-------|
| ZADD | O(log N) | âœ… TrÃ¨s bon |
| ZREM | O(M log N) | M = Ã©lÃ©ments Ã  supprimer |
| ZSCORE | O(1) | âœ… IdÃ©al |
| ZRANK | O(log N) | âœ… TrÃ¨s bon |
| ZRANGE | O(log N + M) | M = Ã©lÃ©ments retournÃ©s |
| ZRANGEBYSCORE | O(log N + M) | M = Ã©lÃ©ments retournÃ©s |
| ZCARD | O(1) | âœ… IdÃ©al |
| ZCOUNT | O(log N) | âœ… TrÃ¨s bon |
| ZINCRBY | O(log N) | âœ… TrÃ¨s bon |
| ZUNIONSTORE | O(N + M log M) | CoÃ»teux |
| ZINTERSTORE | O(N*M log M) | TrÃ¨s coÃ»teux |

#### Commandes gÃ©nÃ©rales

| Commande | ComplexitÃ© | Notes |
|----------|------------|-------|
| KEYS | O(N) | âŒ JAMAIS en production |
| SCAN | O(1) | âœ… Par itÃ©ration |
| EXISTS | O(N) | N = nombre de clÃ©s |
| DEL | O(N) | N = nombre de clÃ©s |
| UNLINK | O(1) | âœ… Async (mieux que DEL) |
| RENAME | O(1) | âœ… IdÃ©al |
| TYPE | O(1) | âœ… IdÃ©al |
| EXPIRE | O(1) | âœ… IdÃ©al |
| TTL | O(1) | âœ… IdÃ©al |

---

## ğŸš« Commandes Ã  Ã©viter en production

### La liste noire

#### 1. KEYS *

```bash
# âŒ JAMAIS CELA
KEYS *
KEYS user:*
KEYS cache:*

# Pourquoi c'est catastrophique :
# - O(N) oÃ¹ N = total de clÃ©s dans la DB
# - Bloque Redis pendant toute l'exÃ©cution
# - Peut prendre plusieurs secondes sur 10M+ clÃ©s
# - Aucune possibilitÃ© d'interrompre

# âœ… TOUJOURS CELA
SCAN 0 MATCH user:* COUNT 100
# - O(1) par itÃ©ration
# - Non-bloquant
# - Peut Ãªtre pausÃ©/repris
```

#### 2. SMEMBERS sur gros sets

```bash
# âŒ Ã‰viter
SMEMBERS large_set
# Si le set a 100K membres â†’ 100K Ã©lÃ©ments retournÃ©s d'un coup

# âœ… Utiliser SSCAN
SSCAN large_set 0 COUNT 100
# Retourne 100 Ã©lÃ©ments Ã  la fois
```

#### 3. HGETALL sur gros hashes

```bash
# âŒ Ã‰viter
HGETALL user:12345:data
# Si le hash a 50K fields â†’ 50K fields retournÃ©s

# âœ… Alternative 1 : RÃ©cupÃ©rer seulement les champs nÃ©cessaires
HMGET user:12345:data field1 field2 field3

# âœ… Alternative 2 : Utiliser HSCAN
HSCAN user:12345:data 0 COUNT 100
```

#### 4. LRANGE 0 -1 sur grosses listes

```bash
# âŒ Ã‰viter
LRANGE queue:tasks 0 -1
# RÃ©cupÃ¨re toute la liste

# âœ… Paginer
LRANGE queue:tasks 0 99    # Premiers 100
LRANGE queue:tasks 100 199 # 100 suivants
```

#### 5. SORT avec patterns complexes

```bash
# âŒ TrÃ¨s coÃ»teux
SORT mylist BY weight:* GET object:* GET name:*
# O(N log N) + accÃ¨s Ã  d'autres clÃ©s

# âœ… PrÃ©-calculer et utiliser Sorted Sets
ZADD sorted_list score1 "element1" score2 "element2"
ZRANGE sorted_list 0 -1 WITHSCORES
```

#### 6. OpÃ©rations ensemblistes sur gros sets

```bash
# âŒ CoÃ»teux
SUNION set1 set2 set3 set4 set5
# O(N) oÃ¹ N = somme de tous les sets

# âŒ TrÃ¨s coÃ»teux
SINTER large_set1 large_set2 large_set3
# O(N*M) oÃ¹ N = plus petit set, M = nombre de sets

# âœ… Limiter le nombre de sets
# âœ… PrÃ©calculer avec SUNIONSTORE si rÃ©sultat utilisÃ© multiple fois
SUNIONSTORE result set1 set2 set3
# Puis rÃ©cupÃ©rer par parties avec SSCAN
```

---

## âœ… Alternatives optimisÃ©es

### Pattern 1 : Remplacer KEYS par SCAN

```python
#!/usr/bin/env python3
"""
Scanner efficacement le keyspace
"""
import redis

def scan_keys_efficient(r, pattern='*', count=100):
    """
    Alternative Ã  KEYS - utilise SCAN
    """
    cursor = 0
    keys_found = []

    while True:
        cursor, keys = r.scan(cursor, match=pattern, count=count)
        keys_found.extend(keys)

        if cursor == 0:
            break

    return keys_found

# Usage
r = redis.Redis()
user_keys = scan_keys_efficient(r, pattern='user:*', count=1000)
print(f"Found {len(user_keys)} user keys")
```

### Pattern 2 : Remplacer SMEMBERS par SSCAN

```python
def get_set_members_efficient(r, key, batch_size=100):
    """
    Alternative Ã  SMEMBERS - utilise SSCAN
    """
    cursor = 0
    members = []

    while True:
        cursor, batch = r.sscan(key, cursor, count=batch_size)
        members.extend(batch)

        if cursor == 0:
            break

    return members

# Usage
members = get_set_members_efficient(r, 'large_set', batch_size=500)
```

### Pattern 3 : Remplacer HGETALL par HMGET

```python
# âŒ RÃ©cupÃ©rer tout
data = r.hgetall('user:12345')

# âœ… RÃ©cupÃ©rer seulement ce qui est nÃ©cessaire
data = r.hmget('user:12345', ['name', 'email', 'age', 'city'])

# âœ… Si vraiment besoin de tout, utiliser HSCAN
def get_hash_efficient(r, key, batch_size=100):
    cursor = 0
    hash_data = {}

    while True:
        cursor, data = r.hscan(key, cursor, count=batch_size)
        hash_data.update(data)

        if cursor == 0:
            break

    return hash_data
```

### Pattern 4 : PrÃ©-calcul avec Sorted Sets

```python
# Exemple : Leaderboard avec scores calculÃ©s

# âŒ Mauvais : Calculer Ã  chaque fois
def get_leaderboard_slow(r):
    users = r.smembers('users')
    leaderboard = []

    for user in users:
        score = calculate_complex_score(r, user)  # CoÃ»teux!
        leaderboard.append((user, score))

    leaderboard.sort(key=lambda x: x[1], reverse=True)
    return leaderboard[:10]

# âœ… Bon : PrÃ©-calculer dans un Sorted Set
def update_leaderboard(r, user):
    score = calculate_complex_score(r, user)
    r.zadd('leaderboard', {user: score})

def get_leaderboard_fast(r):
    return r.zrevrange('leaderboard', 0, 9, withscores=True)
```

### Pattern 5 : Batch operations

```python
# âŒ Appels individuels
for i in range(1000):
    r.set(f'key:{i}', f'value:{i}')
# 1000 round-trips rÃ©seau!

# âœ… Utiliser pipeline
pipe = r.pipeline()
for i in range(1000):
    pipe.set(f'key:{i}', f'value:{i}')
pipe.execute()
# 1 seul round-trip!

# âœ… Ou MSET pour les strings
mapping = {f'key:{i}': f'value:{i}' for i in range(1000)}
r.mset(mapping)
```

---

## ğŸš€ Techniques d'optimisation avancÃ©es

### 1. Pipelining

**Principe** : Envoyer plusieurs commandes d'un coup sans attendre les rÃ©ponses individuelles.

```python
import redis
import time

r = redis.Redis()

# Sans pipeline
start = time.time()
for i in range(10000):
    r.set(f'key:{i}', f'value:{i}')
    r.get(f'key:{i}')
duration_no_pipeline = time.time() - start

# Avec pipeline
start = time.time()
pipe = r.pipeline()
for i in range(10000):
    pipe.set(f'key:{i}', f'value:{i}')
    pipe.get(f'key:{i}')
pipe.execute()
duration_pipeline = time.time() - start

print(f"Sans pipeline: {duration_no_pipeline:.2f}s")
print(f"Avec pipeline: {duration_pipeline:.2f}s")
print(f"Speedup: {duration_no_pipeline/duration_pipeline:.1f}x")

# RÃ©sultat typique :
# Sans pipeline: 12.34s
# Avec pipeline: 0.45s
# Speedup: 27.4x
```

**Quand utiliser** :
- âœ… Batch d'Ã©critures
- âœ… Batch de lectures indÃ©pendantes
- âœ… Mix lectures/Ã©critures sans dÃ©pendances
- âŒ Commandes dÃ©pendantes (utiliser transactions)

### 2. Transactions avec MULTI/EXEC

```python
# Pour des opÃ©rations atomiques dÃ©pendantes

def transfer_points(r, from_user, to_user, points):
    """
    Transfert atomique de points entre utilisateurs
    """
    pipe = r.pipeline()

    # WATCH pour optimistic locking
    pipe.watch(f'user:{from_user}:points')

    # VÃ©rifier le solde
    balance = int(pipe.get(f'user:{from_user}:points') or 0)

    if balance < points:
        pipe.unwatch()
        return False

    # Transaction atomique
    pipe.multi()
    pipe.decrby(f'user:{from_user}:points', points)
    pipe.incrby(f'user:{to_user}:points', points)

    try:
        pipe.execute()
        return True
    except redis.WatchError:
        # Retry si conflit
        return transfer_points(r, from_user, to_user, points)
```

### 3. Lua Scripts pour atomicitÃ© complexe

```python
# Script Lua pour opÃ©ration atomique complexe
lua_script = """
local key = KEYS[1]
local value = redis.call('GET', key)

if not value then
    return 0
end

local num = tonumber(value)
if num < 100 then
    redis.call('INCR', key)
    return 1
else
    return -1
end
"""

# Charger le script
increment_if_less_than_100 = r.register_script(lua_script)

# Utiliser
result = increment_if_less_than_100(keys=['counter'])
```

**Avantages Lua** :
- AtomicitÃ© garantie
- Pas de round-trips rÃ©seau
- Logic complexe cÃ´tÃ© serveur
- Performance optimale

### 4. Compression des donnÃ©es

```python
import json
import gzip
import base64

def set_compressed(r, key, data, ttl=None):
    """
    Stocker des donnÃ©es compressÃ©es
    """
    # SÃ©rialiser
    json_data = json.dumps(data)

    # Compresser
    compressed = gzip.compress(json_data.encode())

    # Encoder en base64 pour stockage
    encoded = base64.b64encode(compressed)

    # Stocker
    if ttl:
        r.setex(key, ttl, encoded)
    else:
        r.set(key, encoded)

def get_compressed(r, key):
    """
    RÃ©cupÃ©rer des donnÃ©es compressÃ©es
    """
    encoded = r.get(key)
    if not encoded:
        return None

    # DÃ©coder
    compressed = base64.b64decode(encoded)

    # DÃ©compresser
    json_data = gzip.decompress(compressed).decode()

    # DÃ©sÃ©rialiser
    return json.loads(json_data)

# Usage
large_data = {'users': [{'name': f'user{i}', 'data': 'x'*1000} for i in range(100)]}

# Comparaison taille
import sys
normal_size = sys.getsizeof(json.dumps(large_data))
compressed_size = sys.getsizeof(gzip.compress(json.dumps(large_data).encode()))

print(f"Normal: {normal_size} bytes")
print(f"Compressed: {compressed_size} bytes")
print(f"Ratio: {normal_size/compressed_size:.1f}x")
```

### 5. Client-side caching (Redis 6+)

```python
import redis

# Activer le client-side caching
r = redis.Redis(decode_responses=True)

# Tracking activÃ©
r.client_tracking_on()

# Les GET suivants seront cachÃ©s cÃ´tÃ© client
value = r.get('frequently_accessed_key')

# Redis notifiera automatiquement si la clÃ© change
# Le cache client sera invalidÃ©

# DÃ©sactiver si nÃ©cessaire
r.client_tracking_off()
```

---

## ğŸ“ˆ Profiling et mesure des performances

### Script de profiling complet

```python
#!/usr/bin/env python3
"""
Profiling avancÃ© des commandes Redis
"""
import redis
import time
import statistics
from collections import defaultdict

class RedisCommandProfiler:
    def __init__(self, host='localhost', port=6379):
        self.r = redis.Redis(host=host, port=port, decode_responses=True)
        self.results = defaultdict(list)

    def profile_command(self, name, func, iterations=1000):
        """
        Profile une commande spÃ©cifique
        """
        print(f"Profiling {name}...")

        latencies = []

        for i in range(iterations):
            start = time.perf_counter()
            try:
                func(i)
            except Exception as e:
                print(f"  Error: {e}")
                continue
            end = time.perf_counter()

            latencies.append((end - start) * 1000)  # ms

        self.results[name] = latencies

    def report(self):
        """
        GÃ©nÃ¨re un rapport dÃ©taillÃ©
        """
        print("\n" + "=" * 80)
        print("REDIS COMMAND PROFILING REPORT")
        print("=" * 80)

        # Tableau de rÃ©sultats
        print(f"\n{'Command':<30} {'Avg (ms)':<12} {'P50':<12} {'P95':<12} {'P99':<12} {'Max':<12}")
        print("-" * 80)

        # Trier par latence moyenne
        sorted_results = sorted(
            self.results.items(),
            key=lambda x: statistics.mean(x[1]),
            reverse=True
        )

        for name, latencies in sorted_results:
            avg = statistics.mean(latencies)
            p50 = statistics.median(latencies)
            p95 = sorted(latencies)[int(len(latencies) * 0.95)]
            p99 = sorted(latencies)[int(len(latencies) * 0.99)]
            max_lat = max(latencies)

            # Indicateur de performance
            if avg < 0.1:
                indicator = "ğŸŸ¢"
            elif avg < 1:
                indicator = "ğŸŸ¡"
            else:
                indicator = "ğŸ”´"

            print(f"{name:<30} {avg:<12.3f} {p50:<12.3f} {p95:<12.3f} {p99:<12.3f} {max_lat:<12.3f} {indicator}")

        print("-" * 80)

        # Recommandations
        print("\n" + "=" * 80)
        print("RECOMMENDATIONS")
        print("=" * 80)

        for name, latencies in sorted_results:
            avg = statistics.mean(latencies)

            if avg > 10:
                print(f"\nâš ï¸  {name}")
                print(f"   Average latency: {avg:.2f}ms")
                print(f"   Action: This command is very slow. Consider:")
                print(f"   - Using alternative commands (SCAN instead of KEYS)")
                print(f"   - Reducing data size")
                print(f"   - Using pipelining if possible")

            elif avg > 1:
                print(f"\nâ„¹ï¸  {name}")
                print(f"   Average latency: {avg:.2f}ms")
                print(f"   Suggestion: Monitor this command, consider optimization")

# Exemple d'utilisation
def run_profiling():
    profiler = RedisCommandProfiler()

    # Setup : crÃ©er des donnÃ©es de test
    r = profiler.r

    # CrÃ©er une list
    r.delete('test:list')
    for i in range(10000):
        r.rpush('test:list', f'item{i}')

    # CrÃ©er un hash
    r.delete('test:hash')
    for i in range(10000):
        r.hset('test:hash', f'field{i}', f'value{i}')

    # CrÃ©er un set
    r.delete('test:set')
    for i in range(10000):
        r.sadd('test:set', f'member{i}')

    # CrÃ©er un sorted set
    r.delete('test:zset')
    for i in range(10000):
        r.zadd('test:zset', {f'member{i}': i})

    # Profile diffÃ©rentes commandes

    # Strings
    profiler.profile_command(
        'GET (simple)',
        lambda i: r.get(f'test:key:{i}')
    )

    profiler.profile_command(
        'SET (simple)',
        lambda i: r.set(f'test:key:{i}', f'value{i}')
    )

    # Lists
    profiler.profile_command(
        'LPUSH',
        lambda i: r.lpush('test:list:temp', f'item{i}')
    )

    profiler.profile_command(
        'LRANGE (first 10)',
        lambda i: r.lrange('test:list', 0, 9)
    )

    profiler.profile_command(
        'LRANGE (first 100)',
        lambda i: r.lrange('test:list', 0, 99)
    )

    profiler.profile_command(
        'LRANGE (all 10K)',
        lambda i: r.lrange('test:list', 0, -1),
        iterations=100  # Moins d'itÃ©rations car lent
    )

    # Hashes
    profiler.profile_command(
        'HGET',
        lambda i: r.hget('test:hash', f'field{i%10000}')
    )

    profiler.profile_command(
        'HMGET (10 fields)',
        lambda i: r.hmget('test:hash', [f'field{j}' for j in range(10)])
    )

    profiler.profile_command(
        'HGETALL (10K fields)',
        lambda i: r.hgetall('test:hash'),
        iterations=100
    )

    # Sets
    profiler.profile_command(
        'SISMEMBER',
        lambda i: r.sismember('test:set', f'member{i%10000}')
    )

    profiler.profile_command(
        'SMEMBERS (10K members)',
        lambda i: r.smembers('test:set'),
        iterations=100
    )

    # Sorted Sets
    profiler.profile_command(
        'ZSCORE',
        lambda i: r.zscore('test:zset', f'member{i%10000}')
    )

    profiler.profile_command(
        'ZRANGE (first 10)',
        lambda i: r.zrange('test:zset', 0, 9)
    )

    profiler.profile_command(
        'ZRANGE (first 100)',
        lambda i: r.zrange('test:zset', 0, 99)
    )

    # Scan operations
    profiler.profile_command(
        'SSCAN (100 at a time)',
        lambda i: r.sscan('test:set', 0, count=100)
    )

    profiler.profile_command(
        'HSCAN (100 at a time)',
        lambda i: r.hscan('test:hash', 0, count=100)
    )

    # Pipeline
    def pipeline_test(i):
        pipe = r.pipeline()
        for j in range(10):
            pipe.get(f'test:key:{j}')
        pipe.execute()

    profiler.profile_command(
        'Pipeline (10 GET)',
        pipeline_test
    )

    # Rapport
    profiler.report()

    # Cleanup
    r.delete('test:list', 'test:list:temp', 'test:hash', 'test:set', 'test:zset')

if __name__ == "__main__":
    run_profiling()
```

---

## ğŸ¯ Optimisation par use case

### Use Case 1 : Cache applicatif

```python
# âŒ Anti-pattern : Cache inefficace
def get_user_slow(user_id):
    user = r.get(f'user:{user_id}')
    if not user:
        user = db.query_user(user_id)  # DB query
        r.set(f'user:{user_id}', json.dumps(user))  # Cache sans TTL!
    return json.loads(user)

# âœ… Pattern optimisÃ©
def get_user_optimized(user_id):
    # 1. Cache avec TTL
    user = r.get(f'user:{user_id}')

    if not user:
        user = db.query_user(user_id)
        # TTL pour Ã©viter stale data
        r.setex(f'user:{user_id}', 3600, json.dumps(user))
    else:
        user = json.loads(user)

    return user

# âœ…âœ… Pattern avancÃ© avec compression
def get_user_advanced(user_id):
    # Cache L1 : mÃ©moire locale (trÃ¨s rapide)
    if user_id in local_cache:
        if time.time() - local_cache[user_id]['time'] < 60:  # 1 min
            return local_cache[user_id]['data']

    # Cache L2 : Redis (rapide)
    user_compressed = r.get(f'user:{user_id}')

    if user_compressed:
        user = decompress(user_compressed)
    else:
        # Cache miss : DB
        user = db.query_user(user_id)
        r.setex(f'user:{user_id}', 3600, compress(user))

    # Mettre en cache local
    local_cache[user_id] = {'data': user, 'time': time.time()}

    return user
```

### Use Case 2 : Compteurs temps rÃ©el

```python
# âŒ Anti-pattern : IncrÃ©ments individuels
def track_pageview_slow(page_id):
    r.incr(f'pageviews:{page_id}')
    r.incr(f'pageviews:{page_id}:today')
    r.incr(f'pageviews:total')
# 3 round-trips rÃ©seau!

# âœ… Pattern optimisÃ© : Pipeline
def track_pageview_optimized(page_id):
    pipe = r.pipeline()
    pipe.incr(f'pageviews:{page_id}')
    pipe.incr(f'pageviews:{page_id}:today')
    pipe.incr(f'pageviews:total')
    pipe.execute()
# 1 seul round-trip!

# âœ…âœ… Pattern avancÃ© : Batch local puis flush
class PageviewTracker:
    def __init__(self, r, flush_interval=1.0):
        self.r = r
        self.buffer = defaultdict(int)
        self.flush_interval = flush_interval
        self.last_flush = time.time()

    def track(self, page_id):
        # IncrÃ©menter en mÃ©moire
        self.buffer[f'pageviews:{page_id}'] += 1
        self.buffer[f'pageviews:total'] += 1

        # Flush si nÃ©cessaire
        if time.time() - self.last_flush > self.flush_interval:
            self.flush()

    def flush(self):
        if not self.buffer:
            return

        # Flush en batch vers Redis
        pipe = self.r.pipeline()
        for key, count in self.buffer.items():
            pipe.incrby(key, count)
        pipe.execute()

        # Reset
        self.buffer.clear()
        self.last_flush = time.time()

# Usage
tracker = PageviewTracker(r)
for i in range(10000):
    tracker.track(f'page_{i % 100}')
tracker.flush()  # Flush final
```

### Use Case 3 : Leaderboard

```python
# âŒ Anti-pattern : Calcul Ã  chaque lecture
def get_leaderboard_slow():
    users = r.smembers('users')
    scores = []

    for user in users:
        # Calcul du score (coÃ»teux!)
        score = calculate_score(r, user)
        scores.append((user, score))

    scores.sort(key=lambda x: x[1], reverse=True)
    return scores[:10]

# âœ… Pattern optimisÃ© : Sorted Set
def update_user_score(user_id, score):
    r.zadd('leaderboard', {user_id: score})

def get_leaderboard_optimized():
    # O(log N + M) oÃ¹ M = 10
    return r.zrevrange('leaderboard', 0, 9, withscores=True)

# âœ…âœ… Pattern avancÃ© : Multiple leaderboards
def update_user_score_advanced(user_id, score, country):
    pipe = r.pipeline()

    # Global leaderboard
    pipe.zadd('leaderboard:global', {user_id: score})

    # Country leaderboard
    pipe.zadd(f'leaderboard:country:{country}', {user_id: score})

    # Time-based leaderboards
    today = datetime.now().strftime('%Y-%m-%d')
    pipe.zadd(f'leaderboard:daily:{today}', {user_id: score})

    pipe.execute()

def get_leaderboard_with_rank(user_id):
    """RÃ©cupÃ¨re top 10 + rang de l'utilisateur"""
    pipe = r.pipeline()

    # Top 10
    pipe.zrevrange('leaderboard:global', 0, 9, withscores=True)

    # Rang de l'utilisateur
    pipe.zrevrank('leaderboard:global', user_id)

    # Score de l'utilisateur
    pipe.zscore('leaderboard:global', user_id)

    results = pipe.execute()

    return {
        'top_10': results[0],
        'user_rank': results[1] + 1 if results[1] is not None else None,
        'user_score': results[2]
    }
```

---

## ğŸ“‹ Best Practices et checklist

### Checklist d'optimisation

**Avant le dÃ©ploiement** :
- [ ] Toutes les commandes ont une complexitÃ© â‰¤ O(log N)
- [ ] Pas de KEYS, SMEMBERS, HGETALL en production
- [ ] Pipelining utilisÃ© pour les batch operations
- [ ] TTL dÃ©finis sur toutes les donnÃ©es temporaires
- [ ] Compression activÃ©e pour les gros objets
- [ ] Monitoring des commandes lentes (SLOWLOG)

**Choix de structures** :
- [ ] Strings pour les valeurs simples (< 100KB)
- [ ] Hashes pour les objets (au lieu de JSON strings)
- [ ] Lists pour les queues (pas pour l'accÃ¨s alÃ©atoire)
- [ ] Sets pour l'unicitÃ©
- [ ] Sorted Sets pour le classement
- [ ] Ã‰viter les big keys (fragmenter si > 10K Ã©lÃ©ments)

**Patterns d'accÃ¨s** :
- [ ] SCAN au lieu de KEYS
- [ ] SSCAN/HSCAN au lieu de SMEMBERS/HGETALL
- [ ] HMGET au lieu de HGETALL quand possible
- [ ] Pipelining pour les batch operations
- [ ] Lua scripts pour la logique complexe atomique

### RÃ¨gles d'or

```
1. PrivilÃ©gier O(1) et O(log N)
   â””â”€ Ã‰viter O(N), O(NÂ²), O(N*M)

2. Minimiser les round-trips
   â””â”€ Pipeline, MGET, HMGET

3. Ne jamais bloquer Redis
   â””â”€ Pas de KEYS, utiliser SCAN

4. Fragmenter les big keys
   â””â”€ Taille max recommandÃ©e : 10K Ã©lÃ©ments

5. Toujours dÃ©finir des TTL
   â””â”€ Ã‰viter les memory leaks

6. Mesurer avant d'optimiser
   â””â”€ SLOWLOG, profiling, benchmarks

7. Cache intelligemment
   â””â”€ TTL appropriÃ©s, compression si gros

8. Penser atomicitÃ©
   â””â”€ Transactions, Lua scripts quand nÃ©cessaire
```

### Anti-patterns Ã  Ã©viter

```
âŒ KEYS * en production
âŒ SMEMBERS sur gros sets
âŒ HGETALL sur gros hashes
âŒ Boucles de commandes individuelles
âŒ Big keys (> 10K Ã©lÃ©ments)
âŒ Pas de TTL sur donnÃ©es temporaires
âŒ JSON strings au lieu de hashes
âŒ LRANGE 0 -1 sur grosses listes
âŒ SORT avec patterns complexes
âŒ Multiples SINTER/SUNION sans cache
```

---

## ğŸ¯ Points clÃ©s Ã  retenir

1. **ComplexitÃ© algorithmique** â†’ Toujours vÃ©rifier le Big O
2. **SCAN > KEYS** â†’ Jamais KEYS en production
3. **Pipeline** â†’ RÃ©duire les round-trips rÃ©seau
4. **Lua scripts** â†’ AtomicitÃ© et performance
5. **Fragmenter big keys** â†’ Max 10K Ã©lÃ©ments
6. **TTL partout** â†’ Ã‰viter memory leaks
7. **Profiler** â†’ SLOWLOG et benchmarks
8. **Structures appropriÃ©es** â†’ Hash > JSON string

---

**ğŸš€ Section suivante** : [14.9 - Benchmarking avec redis-benchmark](./09-benchmarking-redis-benchmark.md)

â­ï¸ [Benchmarking avec redis-benchmark](/14-performance-troubleshooting/09-benchmarking-redis-benchmark.md)
