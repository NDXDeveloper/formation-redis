ðŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 15.4 Google Cloud Memorystore

## ðŸŽ¯ Objectifs

- MaÃ®triser l'architecture Memorystore et ses deux tiers
- Configurer des instances production avec Terraform et gcloud CLI
- ImplÃ©menter Private Service Connect pour l'isolation rÃ©seau
- DÃ©ployer des read replicas pour la scalabilitÃ© en lecture
- IntÃ©grer Memorystore avec GKE (Google Kubernetes Engine)
- Optimiser les coÃ»ts et performances sur GCP

---

## ðŸ—ï¸ Architecture et positionnement

### Vue d'ensemble de Memorystore

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Google Cloud Memorystore for Redis                 â”‚
â”‚                                                              â”‚
â”‚  Philosophy: SimplicitÃ© et intÃ©gration native GCP            â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚    Basic     â”‚              â”‚  Standard    â”‚              â”‚
â”‚  â”‚    Tier      â”‚              â”‚     HA       â”‚              â”‚
â”‚  â”‚              â”‚              â”‚              â”‚              â”‚
â”‚  â”‚ Single node  â”‚              â”‚Primary + Rep â”‚              â”‚
â”‚  â”‚ No HA        â”‚              â”‚ Auto failoverâ”‚              â”‚
â”‚  â”‚ Dev/Test     â”‚              â”‚ Production   â”‚              â”‚
â”‚  â”‚              â”‚              â”‚              â”‚              â”‚
â”‚  â”‚ 1-300 GB     â”‚              â”‚ 5-300 GB     â”‚              â”‚
â”‚  â”‚              â”‚              â”‚              â”‚              â”‚
â”‚  â”‚ â‚¬0.035/GB-h  â”‚              â”‚ â‚¬0.087/GB-h  â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                                              â”‚
â”‚  Unique Features:                                            â”‚
â”‚  â”œâ”€â”€ Managed fully by Google (minimal config)                â”‚
â”‚  â”œâ”€â”€ Private Service Connect (VPC native)                    â”‚
â”‚  â”œâ”€â”€ Read replicas (Standard only, up to 5)                  â”‚
â”‚  â”œâ”€â”€ Automated daily backups                                 â”‚
â”‚  â”œâ”€â”€ Import/export to Google Cloud Storage                   â”‚
â”‚  â””â”€â”€ Deep integration with GKE, Cloud Run, App Engine        â”‚
â”‚                                                              â”‚
â”‚  Limitations vs AWS/Azure:                                   â”‚
â”‚  â”œâ”€â”€ No native clustering (single instance up to 300GB)      â”‚
â”‚  â”œâ”€â”€ No cross-region replication                             â”‚
â”‚  â”œâ”€â”€ No Redis modules (Stack)                                â”‚
â”‚  â”œâ”€â”€ Less configuration options                              â”‚
â”‚  â””â”€â”€ Simpler = less flexibility                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Architecture rÃ©seau avec Private Service Connect

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Google Cloud VPC Network                  â”‚
â”‚                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Subnet: applications (10.0.1.0/24)         â”‚    â”‚
â”‚  â”‚                                                    â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚  â”‚  â”‚   GKE Pod   â”‚  â”‚   GKE Pod   â”‚  â”‚  VM / GCE  â”‚  â”‚    â”‚
â”‚  â”‚  â”‚  (app-1)    â”‚  â”‚  (app-2)    â”‚  â”‚  Instance  â”‚  â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚  â”‚         â”‚                â”‚                â”‚        â”‚    â”‚
â”‚  â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚    â”‚
â”‚  â”‚                          â”‚                         â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                             â”‚                              â”‚
â”‚                             â”‚ Private connection           â”‚
â”‚                             â”‚ (no public IP)               â”‚
â”‚                             â–¼                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚    Private Service Connect (Managed by Google)      â”‚   â”‚
â”‚  â”‚                                                     â”‚   â”‚
â”‚  â”‚    Reserved IP Range: 10.0.2.0/29 (8 IPs)           â”‚   â”‚
â”‚  â”‚                                                     â”‚   â”‚
â”‚  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚   â”‚
â”‚  â”‚    â”‚  Memorystore Instance (Standard HA)    â”‚       â”‚   â”‚
â”‚  â”‚    â”‚                                        â”‚       â”‚   â”‚
â”‚  â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚       â”‚   â”‚
â”‚  â”‚    â”‚  â”‚   Primary    â”‚  â”‚   Replica    â”‚    â”‚       â”‚   â”‚
â”‚  â”‚    â”‚  â”‚   (Zone A)   â”‚  â”‚   (Zone B)   â”‚    â”‚       â”‚   â”‚
â”‚  â”‚    â”‚  â”‚              â”‚  â”‚              â”‚    â”‚       â”‚   â”‚
â”‚  â”‚    â”‚  â”‚  10.0.2.2    â”‚  â”‚  10.0.2.3    â”‚    â”‚       â”‚   â”‚
â”‚  â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚       â”‚   â”‚
â”‚  â”‚    â”‚                                        â”‚       â”‚   â”‚
â”‚  â”‚    â”‚  â€¢ Automatic failover                  â”‚       â”‚   â”‚
â”‚  â”‚    â”‚  â€¢ Async replication                   â”‚       â”‚   â”‚
â”‚  â”‚    â”‚  â€¢ Read replicas optional              â”‚       â”‚   â”‚
â”‚  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚   â”‚
â”‚  â”‚                                                     â”‚   â”‚
â”‚  â”‚    Private DNS: redis-instance.*.redis.googleapis   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                            â”‚
â”‚  Key Network Features:                                     â”‚
â”‚  â”œâ”€â”€ No public IP (100% private)                           â”‚
â”‚  â”œâ”€â”€ VPC peering automatic (managed by Google)             â”‚
â”‚  â”œâ”€â”€ Private Service Connect handles routing               â”‚
â”‚  â”œâ”€â”€ Cross-project VPC peering supported                   â”‚
â”‚  â””â”€â”€ Firewall rules not needed (private by design)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Comparaison Basic vs Standard

| CaractÃ©ristique | Basic Tier | Standard Tier |
|-----------------|------------|---------------|
| **DisponibilitÃ©** |
| Nodes | 1 | 2 (primary + replica) |
| SLA | None | 99.9% |
| Automatic failover | âŒ | âœ… (30-60s) |
| Zone placement | Single zone | Multi-zone |
| Downtime | Maintenance downtime | Zero-downtime upgrades |
| **CapacitÃ©** |
| Min RAM | 1 GB | 5 GB |
| Max RAM | 300 GB | 300 GB |
| Max connections | ~20K | ~40K |
| **Performance** |
| Throughput | Medium | High |
| Read replicas | âŒ | âœ… (0-5 replicas) |
| **Persistence** |
| Snapshots | âœ… Daily | âœ… Daily |
| Point-in-time recovery | âŒ | âŒ |
| Export/Import | âœ… GCS | âœ… GCS |
| **RÃ©seau** |
| VPC peering | âœ… | âœ… |
| Private IP only | âœ… | âœ… |
| Transit encryption | âœ… (TLS) | âœ… (TLS) |
| **Monitoring** |
| Cloud Monitoring | âœ… | âœ… |
| Cloud Logging | âœ… | âœ… |
| Alerting | âœ… | âœ… |
| **Pricing** |
| Cost per GB-hour | â‚¬0.035/GB-h | â‚¬0.087/GB-h |
| Cost 50GB/month | ~â‚¬1,277 | ~â‚¬3,176 |
| Use case | Dev/Test | Production |

---

## ðŸ“‹ Configuration avec Terraform

### Module Terraform complet

```hcl
# Google Cloud Memorystore for Redis - Production Configuration
# Provider: hashicorp/google ~> 5.0

terraform {
  required_version = ">= 1.5"

  required_providers {
    google = {
      source  = "hashicorp/google"
      version = "~> 5.10"
    }
    random = {
      source  = "hashicorp/random"
      version = "~> 3.5"
    }
  }
}

provider "google" {
  project = var.project_id
  region  = var.region
}

# Variables
variable "project_id" {
  type        = string
  description = "GCP Project ID"
}

variable "region" {
  type        = string
  description = "GCP region"
  default     = "europe-west1"
}

variable "environment" {
  type        = string
  description = "Environment name"
  default     = "production"
}

variable "redis_tier" {
  type        = string
  description = "Redis tier: BASIC or STANDARD_HA"
  default     = "STANDARD_HA"

  validation {
    condition     = contains(["BASIC", "STANDARD_HA"], var.redis_tier)
    error_message = "Redis tier must be BASIC or STANDARD_HA"
  }
}

variable "memory_size_gb" {
  type        = number
  description = "Memory size in GB"
  default     = 50

  validation {
    condition     = var.memory_size_gb >= 1 && var.memory_size_gb <= 300
    error_message = "Memory size must be between 1 and 300 GB"
  }
}

variable "redis_version" {
  type        = string
  description = "Redis version"
  default     = "REDIS_7_2"

  validation {
    condition = contains([
      "REDIS_6_X",
      "REDIS_7_0",
      "REDIS_7_2"
    ], var.redis_version)
    error_message = "Invalid Redis version"
  }
}

variable "replica_count" {
  type        = number
  description = "Number of read replicas (0-5, Standard tier only)"
  default     = 2

  validation {
    condition     = var.replica_count >= 0 && var.replica_count <= 5
    error_message = "Replica count must be between 0 and 5"
  }
}

variable "enable_auth" {
  type        = bool
  description = "Enable AUTH (password protection)"
  default     = true
}

variable "enable_transit_encryption" {
  type        = bool
  description = "Enable TLS encryption"
  default     = true
}

variable "labels" {
  type        = map(string)
  description = "Labels to apply to resources"
  default = {
    managed_by = "terraform"
  }
}

# Local variables
locals {
  name_prefix = "${var.environment}-redis"

  common_labels = merge(
    var.labels,
    {
      environment = var.environment
      terraform   = "true"
    }
  )
}

# VPC Network
resource "google_compute_network" "main" {
  name                    = "${local.name_prefix}-vpc"
  auto_create_subnetworks = false

  description = "VPC for ${var.environment} environment"
}

# Subnet for applications
resource "google_compute_subnetwork" "apps" {
  name          = "${local.name_prefix}-apps-subnet"
  ip_cidr_range = "10.0.1.0/24"
  region        = var.region
  network       = google_compute_network.main.id

  # Enable Private Google Access (for GCS, etc.)
  private_ip_google_access = true

  # Enable flow logs
  log_config {
    aggregation_interval = "INTERVAL_5_SEC"
    flow_sampling        = 0.5
    metadata             = "INCLUDE_ALL_METADATA"
  }
}

# Reserved IP range for Memorystore (Private Service Connect)
resource "google_compute_global_address" "redis_private_ip" {
  name          = "${local.name_prefix}-private-ip"
  purpose       = "VPC_PEERING"
  address_type  = "INTERNAL"
  prefix_length = 16
  network       = google_compute_network.main.id

  description = "Reserved IP range for Memorystore"
}

# Private Service Connection (required for Memorystore)
resource "google_service_networking_connection" "redis_private_vpc" {
  network                 = google_compute_network.main.id
  service                 = "servicenetworking.googleapis.com"
  reserved_peering_ranges = [google_compute_global_address.redis_private_ip.name]
}

# Cloud Router for Cloud NAT (if needed for internet access)
resource "google_compute_router" "main" {
  name    = "${local.name_prefix}-router"
  region  = var.region
  network = google_compute_network.main.id

  bgp {
    asn = 64514
  }
}

# Cloud NAT (for outbound internet from private instances)
resource "google_compute_router_nat" "main" {
  name                               = "${local.name_prefix}-nat"
  router                             = google_compute_router.main.name
  region                             = var.region
  nat_ip_allocate_option             = "AUTO_ONLY"
  source_subnetwork_ip_ranges_to_nat = "ALL_SUBNETWORKS_ALL_IP_RANGES"

  log_config {
    enable = true
    filter = "ERRORS_ONLY"
  }
}

# Firewall rule: Allow Redis from app subnet
resource "google_compute_firewall" "allow_redis" {
  name    = "${local.name_prefix}-allow-redis"
  network = google_compute_network.main.name

  allow {
    protocol = "tcp"
    ports    = ["6379"]
  }

  source_ranges = ["10.0.1.0/24"]  # App subnet
  target_tags   = ["redis"]

  description = "Allow Redis access from application subnet"
}

# Firewall rule: Allow internal communication
resource "google_compute_firewall" "allow_internal" {
  name    = "${local.name_prefix}-allow-internal"
  network = google_compute_network.main.name

  allow {
    protocol = "tcp"
  }

  allow {
    protocol = "udp"
  }

  allow {
    protocol = "icmp"
  }

  source_ranges = ["10.0.0.0/16"]

  description = "Allow internal VPC communication"
}

# Random AUTH password (if enabled)
resource "random_password" "redis_auth" {
  count   = var.enable_auth ? 1 : 0
  length  = 32
  special = false
}

# Secret Manager for storing AUTH password
resource "google_secret_manager_secret" "redis_auth" {
  count     = var.enable_auth ? 1 : 0
  secret_id = "${local.name_prefix}-auth-password"

  labels = local.common_labels

  replication {
    auto {}
  }
}

resource "google_secret_manager_secret_version" "redis_auth" {
  count       = var.enable_auth ? 1 : 0
  secret      = google_secret_manager_secret.redis_auth[0].id
  secret_data = random_password.redis_auth[0].result
}

# Memorystore Redis Instance (Standard HA)
resource "google_redis_instance" "main" {
  name               = "${local.name_prefix}-instance"
  tier               = var.redis_tier
  memory_size_gb     = var.memory_size_gb
  region             = var.region
  redis_version      = var.redis_version
  display_name       = "${var.environment} Redis Instance"

  # Network
  authorized_network      = google_compute_network.main.id
  connect_mode            = "PRIVATE_SERVICE_ACCESS"
  reserved_ip_range       = google_compute_global_address.redis_private_ip.address

  # Location (primary zone)
  location_id             = "${var.region}-b"
  alternative_location_id = var.redis_tier == "STANDARD_HA" ? "${var.region}-c" : null

  # Redis configuration
  redis_configs = {
    maxmemory-policy            = "allkeys-lru"
    notify-keyspace-events      = "Ex"
    timeout                     = "300"

    # Active defragmentation
    activedefrag                = "yes"

    # LFU tuning (for allkeys-lfu if used)
    lfu-log-factor              = "10"
    lfu-decay-time              = "1"

    # Slow log
    slowlog-log-slower-than     = "10000"  # 10ms
    slowlog-max-len             = "128"
  }

  # Maintenance policy (weekly window)
  maintenance_policy {
    weekly_maintenance_window {
      day = "SUNDAY"
      start_time {
        hours   = 5
        minutes = 0
      }
      duration = "3600s"  # 1 hour
    }
  }

  # Persistence configuration (RDB snapshots)
  persistence_config {
    persistence_mode    = "RDB"
    rdb_snapshot_period = "ONE_HOUR"  # ONE_HOUR, SIX_HOURS, TWELVE_HOURS, TWENTY_FOUR_HOURS
    rdb_snapshot_start_time = formatdate("YYYY-MM-DD'T'hh:mm:ss'Z'", timestamp())
  }

  # Authentication
  auth_enabled = var.enable_auth

  # TLS encryption
  transit_encryption_mode = var.enable_transit_encryption ? "SERVER_AUTHENTICATION" : "DISABLED"

  # Read replicas (Standard tier only)
  replica_count      = var.redis_tier == "STANDARD_HA" ? var.replica_count : 0
  read_replicas_mode = var.redis_tier == "STANDARD_HA" && var.replica_count > 0 ? "READ_REPLICAS_ENABLED" : "READ_REPLICAS_DISABLED"

  labels = local.common_labels

  depends_on = [
    google_service_networking_connection.redis_private_vpc
  ]

  # Lifecycle
  lifecycle {
    prevent_destroy = true  # Protect production data

    ignore_changes = [
      persistence_config[0].rdb_snapshot_start_time  # Ignore auto-generated timestamp
    ]
  }
}

# Cloud Monitoring - Alert Policy for CPU
resource "google_monitoring_alert_policy" "redis_cpu" {
  display_name = "${local.name_prefix} High CPU"
  combiner     = "OR"

  conditions {
    display_name = "Redis CPU > 80%"

    condition_threshold {
      filter          = "resource.type = \"redis_instance\" AND resource.labels.instance_id = \"${google_redis_instance.main.id}\" AND metric.type = \"redis.googleapis.com/stats/cpu_utilization\""
      duration        = "300s"
      comparison      = "COMPARISON_GT"
      threshold_value = 0.8

      aggregations {
        alignment_period   = "60s"
        per_series_aligner = "ALIGN_MEAN"
      }
    }
  }

  notification_channels = [google_monitoring_notification_channel.email.id]

  alert_strategy {
    auto_close = "1800s"  # 30 minutes
  }

  documentation {
    content   = "Redis CPU utilization is above 80%. Consider scaling up or optimizing queries."
    mime_type = "text/markdown"
  }
}

# Cloud Monitoring - Alert Policy for Memory
resource "google_monitoring_alert_policy" "redis_memory" {
  display_name = "${local.name_prefix} High Memory"
  combiner     = "OR"

  conditions {
    display_name = "Redis Memory > 85%"

    condition_threshold {
      filter          = "resource.type = \"redis_instance\" AND resource.labels.instance_id = \"${google_redis_instance.main.id}\" AND metric.type = \"redis.googleapis.com/stats/memory/usage_ratio\""
      duration        = "300s"
      comparison      = "COMPARISON_GT"
      threshold_value = 0.85

      aggregations {
        alignment_period   = "60s"
        per_series_aligner = "ALIGN_MEAN"
      }
    }
  }

  notification_channels = [google_monitoring_notification_channel.email.id]

  alert_strategy {
    auto_close = "1800s"
  }

  documentation {
    content   = "Redis memory usage is above 85%. Consider increasing instance size or implementing eviction policy."
    mime_type = "text/markdown"
  }
}

# Cloud Monitoring - Alert Policy for Replication Lag
resource "google_monitoring_alert_policy" "redis_replication_lag" {
  count        = var.redis_tier == "STANDARD_HA" ? 1 : 0
  display_name = "${local.name_prefix} High Replication Lag"
  combiner     = "OR"

  conditions {
    display_name = "Redis Replication Lag > 5s"

    condition_threshold {
      filter          = "resource.type = \"redis_instance\" AND resource.labels.instance_id = \"${google_redis_instance.main.id}\" AND metric.type = \"redis.googleapis.com/replication/master/slaves/lag\""
      duration        = "300s"
      comparison      = "COMPARISON_GT"
      threshold_value = 5

      aggregations {
        alignment_period   = "60s"
        per_series_aligner = "ALIGN_MAX"
      }
    }
  }

  notification_channels = [google_monitoring_notification_channel.email.id]

  alert_strategy {
    auto_close = "1800s"
  }

  documentation {
    content   = "Redis replication lag is above 5 seconds. Check network and load."
    mime_type = "text/markdown"
  }
}

# Cloud Monitoring - Alert Policy for Connections
resource "google_monitoring_alert_policy" "redis_connections" {
  display_name = "${local.name_prefix} High Connections"
  combiner     = "OR"

  conditions {
    display_name = "Redis Connected Clients > 5000"

    condition_threshold {
      filter          = "resource.type = \"redis_instance\" AND resource.labels.instance_id = \"${google_redis_instance.main.id}\" AND metric.type = \"redis.googleapis.com/clients/connected\""
      duration        = "300s"
      comparison      = "COMPARISON_GT"
      threshold_value = 5000

      aggregations {
        alignment_period   = "60s"
        per_series_aligner = "ALIGN_MAX"
      }
    }
  }

  notification_channels = [google_monitoring_notification_channel.email.id]

  alert_strategy {
    auto_close = "1800s"
  }

  documentation {
    content   = "Redis has more than 5000 connected clients. Investigate connection leaks."
    mime_type = "text/markdown"
  }
}

# Notification Channel (Email)
resource "google_monitoring_notification_channel" "email" {
  display_name = "Platform Team Email"
  type         = "email"

  labels = {
    email_address = "platform-team@example.com"
  }
}

# Optional: Notification Channel (Slack)
resource "google_monitoring_notification_channel" "slack" {
  count        = 0  # Set to 1 to enable
  display_name = "Slack Notifications"
  type         = "slack"

  labels = {
    channel_name = "#redis-alerts"
  }

  sensitive_labels {
    auth_token = var.slack_auth_token  # Add this variable if using Slack
  }
}

# Cloud Logging - Log Sink (export to BigQuery for analysis)
resource "google_logging_project_sink" "redis_logs" {
  name        = "${local.name_prefix}-logs-sink"
  destination = "bigquery.googleapis.com/projects/${var.project_id}/datasets/${google_bigquery_dataset.redis_logs.dataset_id}"

  filter = "resource.type = \"redis_instance\" AND resource.labels.instance_id = \"${google_redis_instance.main.id}\""

  unique_writer_identity = true
}

# BigQuery Dataset for logs
resource "google_bigquery_dataset" "redis_logs" {
  dataset_id  = "${replace(local.name_prefix, "-", "_")}_logs"
  location    = var.region
  description = "Redis logs for analysis"

  default_table_expiration_ms = 2592000000  # 30 days

  labels = local.common_labels
}

# Grant log sink write permissions to BigQuery
resource "google_bigquery_dataset_iam_member" "log_sink_writer" {
  dataset_id = google_bigquery_dataset.redis_logs.dataset_id
  role       = "roles/bigquery.dataEditor"
  member     = google_logging_project_sink.redis_logs.writer_identity
}

# Cloud Storage bucket for export/import
resource "google_storage_bucket" "redis_backups" {
  name          = "${var.project_id}-${local.name_prefix}-backups"
  location      = var.region
  force_destroy = false

  uniform_bucket_level_access = true

  versioning {
    enabled = true
  }

  lifecycle_rule {
    condition {
      age = 30  # Keep backups for 30 days
    }
    action {
      type = "Delete"
    }
  }

  lifecycle_rule {
    condition {
      age                = 7
      matches_storage_class = ["STANDARD"]
    }
    action {
      type          = "SetStorageClass"
      storage_class = "NEARLINE"
    }
  }

  labels = local.common_labels
}

# IAM binding for Cloud Build (if using CI/CD)
resource "google_project_iam_member" "cloudbuild_redis_admin" {
  count   = 0  # Set to 1 if using Cloud Build
  project = var.project_id
  role    = "roles/redis.admin"
  member  = "serviceAccount:${var.project_id}@cloudbuild.gserviceaccount.com"
}

# Outputs
output "redis_host" {
  description = "Redis instance host (private IP)"
  value       = google_redis_instance.main.host
}

output "redis_port" {
  description = "Redis instance port"
  value       = google_redis_instance.main.port
}

output "redis_connection_string" {
  description = "Redis connection string"
  value       = var.enable_auth ? "rediss://:${random_password.redis_auth[0].result}@${google_redis_instance.main.host}:${google_redis_instance.main.port}" : "redis://${google_redis_instance.main.host}:${google_redis_instance.main.port}"
  sensitive   = true
}

output "redis_auth_string" {
  description = "Redis AUTH password"
  value       = var.enable_auth ? random_password.redis_auth[0].result : null
  sensitive   = true
}

output "redis_instance_id" {
  description = "Redis instance ID"
  value       = google_redis_instance.main.id
}

output "redis_instance_name" {
  description = "Redis instance name"
  value       = google_redis_instance.main.name
}

output "redis_read_endpoint" {
  description = "Redis read endpoint (for read replicas)"
  value       = google_redis_instance.main.read_endpoint
}

output "redis_current_location_id" {
  description = "Current location of Redis primary"
  value       = google_redis_instance.main.current_location_id
}

output "vpc_network_id" {
  description = "VPC network ID"
  value       = google_compute_network.main.id
}

output "vpc_network_name" {
  description = "VPC network name"
  value       = google_compute_network.main.name
}

output "app_subnet_id" {
  description = "Application subnet ID"
  value       = google_compute_subnetwork.apps.id
}

output "backup_bucket_name" {
  description = "GCS bucket name for backups"
  value       = google_storage_bucket.redis_backups.name
}

output "logs_dataset_id" {
  description = "BigQuery dataset ID for logs"
  value       = google_bigquery_dataset.redis_logs.dataset_id
}

output "auth_secret_id" {
  description = "Secret Manager secret ID for AUTH password"
  value       = var.enable_auth ? google_secret_manager_secret.redis_auth[0].secret_id : null
}
```

### Variables file (terraform.tfvars)

```hcl
# Production configuration
project_id   = "my-gcp-project-prod"
region       = "europe-west1"
environment  = "production"

# Redis configuration
redis_tier          = "STANDARD_HA"
memory_size_gb      = 50
redis_version       = "REDIS_7_2"
replica_count       = 2
enable_auth         = true
enable_transit_encryption = true

# Labels
labels = {
  team        = "platform"
  cost_center = "engineering"
  managed_by  = "terraform"
}
```

### DÃ©ploiement

```bash
#!/bin/bash
# Deploy Google Cloud Memorystore with Terraform

set -euo pipefail

PROJECT_ID="my-gcp-project-prod"
REGION="europe-west1"

echo "=== Deploying Memorystore for Redis ==="

# Set project
gcloud config set project $PROJECT_ID

# Enable required APIs
echo "Enabling required APIs..."
gcloud services enable \
  redis.googleapis.com \
  servicenetworking.googleapis.com \
  compute.googleapis.com \
  monitoring.googleapis.com \
  logging.googleapis.com \
  secretmanager.googleapis.com \
  bigquery.googleapis.com \
  storage.googleapis.com

# Initialize Terraform
terraform init

# Format
terraform fmt

# Validate
terraform validate

# Plan
terraform plan \
  -var="project_id=$PROJECT_ID" \
  -var="region=$REGION" \
  -out=tfplan

# Apply
echo "Applying configuration..."
terraform apply tfplan

# Get outputs
echo ""
echo "=== Deployment Complete ==="
echo ""
terraform output redis_host
terraform output redis_port
echo ""
echo "To get connection string (sensitive):"
echo "terraform output -raw redis_connection_string"
```

---

## ðŸ”§ Configuration avec gcloud CLI

### DÃ©ploiement via CLI

```bash
#!/bin/bash
# Create Memorystore instance with gcloud CLI

PROJECT_ID="my-gcp-project-prod"
REGION="europe-west1"
INSTANCE_NAME="production-redis-instance"
NETWORK="projects/$PROJECT_ID/global/networks/prod-vpc"

# Create VPC network
gcloud compute networks create prod-vpc \
  --subnet-mode=custom \
  --project=$PROJECT_ID

# Create subnet
gcloud compute networks subnets create apps-subnet \
  --network=prod-vpc \
  --region=$REGION \
  --range=10.0.1.0/24 \
  --enable-private-ip-google-access \
  --project=$PROJECT_ID

# Reserve IP range for Private Service Connect
gcloud compute addresses create redis-ip-range \
  --global \
  --purpose=VPC_PEERING \
  --prefix-length=16 \
  --network=prod-vpc \
  --project=$PROJECT_ID

# Create private connection
gcloud services vpc-peerings connect \
  --service=servicenetworking.googleapis.com \
  --ranges=redis-ip-range \
  --network=prod-vpc \
  --project=$PROJECT_ID

# Create Memorystore instance (Standard HA)
gcloud redis instances create $INSTANCE_NAME \
  --tier=STANDARD_HA \
  --size=50 \
  --region=$REGION \
  --zone=$REGION-b \
  --alternative-zone=$REGION-c \
  --redis-version=redis_7_2 \
  --network=$NETWORK \
  --connect-mode=PRIVATE_SERVICE_ACCESS \
  --enable-auth \
  --transit-encryption-mode=SERVER_AUTHENTICATION \
  --persistence-mode=RDB \
  --rdb-snapshot-period=one-hour \
  --rdb-snapshot-start-time=2024-01-01T03:00:00Z \
  --replica-count=2 \
  --read-replicas-mode=READ_REPLICAS_ENABLED \
  --redis-config=maxmemory-policy=allkeys-lru \
  --redis-config=notify-keyspace-events=Ex \
  --redis-config=activedefrag=yes \
  --maintenance-window-day=SUNDAY \
  --maintenance-window-hour=5 \
  --project=$PROJECT_ID

# Wait for instance to be ready
echo "Waiting for instance to be ready..."
gcloud redis instances describe $INSTANCE_NAME \
  --region=$REGION \
  --project=$PROJECT_ID \
  --format="value(state)"

# Get connection info
echo ""
echo "=== Instance Created ==="
gcloud redis instances describe $INSTANCE_NAME \
  --region=$REGION \
  --project=$PROJECT_ID \
  --format="table(host,port,authString,currentLocationId)"

# Export instance details to file
gcloud redis instances describe $INSTANCE_NAME \
  --region=$REGION \
  --project=$PROJECT_ID \
  --format=json > redis-instance-config.json

echo ""
echo "Instance details saved to redis-instance-config.json"
```

### Gestion des snapshots

```bash
#!/bin/bash
# Snapshot management

INSTANCE_NAME="production-redis-instance"
REGION="europe-west1"
PROJECT_ID="my-gcp-project-prod"
BUCKET_NAME="gs://my-project-redis-backups"

# Export instance to Cloud Storage
echo "Exporting Redis data to GCS..."
gcloud redis instances export \
  $INSTANCE_NAME \
  $BUCKET_NAME/backup-$(date +%Y%m%d-%H%M%S).rdb \
  --region=$REGION \
  --project=$PROJECT_ID

# Import from Cloud Storage
echo "Importing Redis data from GCS..."
gcloud redis instances import \
  $INSTANCE_NAME \
  $BUCKET_NAME/backup-20240615-120000.rdb \
  --region=$REGION \
  --project=$PROJECT_ID

# List available backups
echo "Available backups:"
gsutil ls $BUCKET_NAME/

# Get instance state during export/import
gcloud redis instances describe $INSTANCE_NAME \
  --region=$REGION \
  --project=$PROJECT_ID \
  --format="value(state)"
```

---

## ðŸš€ IntÃ©gration avec GKE

### DÃ©ploiement d'application connectÃ©e Ã  Memorystore

```yaml
# kubernetes/redis-client-deployment.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: redis-config
  namespace: default
data:
  redis-host: "10.0.2.2"  # Memorystore private IP
  redis-port: "6379"
  redis-tls-enabled: "true"

---
apiVersion: v1
kind: Secret
metadata:
  name: redis-credentials
  namespace: default
type: Opaque
stringData:
  redis-password: ""  # Populate from Secret Manager or manually

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-client-app
  namespace: default
  labels:
    app: redis-client
spec:
  replicas: 3
  selector:
    matchLabels:
      app: redis-client
  template:
    metadata:
      labels:
        app: redis-client
    spec:
      # GKE Workload Identity (recommended for GCP services)
      serviceAccountName: redis-client-sa

      containers:
      - name: app
        image: gcr.io/my-project/redis-client-app:v1.0.0

        env:
        # Redis connection configuration
        - name: REDIS_HOST
          valueFrom:
            configMapKeyRef:
              name: redis-config
              key: redis-host

        - name: REDIS_PORT
          valueFrom:
            configMapKeyRef:
              name: redis-config
              key: redis-port

        - name: REDIS_PASSWORD
          valueFrom:
            secretKeyRef:
              name: redis-credentials
              key: redis-password

        - name: REDIS_TLS_ENABLED
          valueFrom:
            configMapKeyRef:
              name: redis-config
              key: redis-tls-enabled

        # Connection string (constructed)
        - name: REDIS_URL
          value: "rediss://:$(REDIS_PASSWORD)@$(REDIS_HOST):$(REDIS_PORT)"

        ports:
        - name: http
          containerPort: 8080
          protocol: TCP

        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"

        livenessProbe:
          httpGet:
            path: /healthz
            port: http
          initialDelaySeconds: 30
          periodSeconds: 10

        readinessProbe:
          httpGet:
            path: /ready
            port: http
          initialDelaySeconds: 5
          periodSeconds: 5

      # Ensure pods are spread across zones
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - redis-client
              topologyKey: topology.kubernetes.io/zone

---
apiVersion: v1
kind: Service
metadata:
  name: redis-client-service
  namespace: default
spec:
  selector:
    app: redis-client
  ports:
  - name: http
    protocol: TCP
    port: 80
    targetPort: http
  type: ClusterIP
```

### Service Account avec Workload Identity

```yaml
# kubernetes/service-account.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: redis-client-sa
  namespace: default
  annotations:
    # Link to GCP Service Account (Workload Identity)
    iam.gke.io/gcp-service-account: redis-client@my-project.iam.gserviceaccount.com
```

```bash
#!/bin/bash
# Setup Workload Identity for GKE to access Secret Manager

PROJECT_ID="my-gcp-project-prod"
GCP_SA_NAME="redis-client"
K8S_SA_NAME="redis-client-sa"
K8S_NAMESPACE="default"

# Create GCP Service Account
gcloud iam service-accounts create $GCP_SA_NAME \
  --display-name="Redis Client Service Account" \
  --project=$PROJECT_ID

# Grant access to Secret Manager
gcloud projects add-iam-policy-binding $PROJECT_ID \
  --member="serviceAccount:${GCP_SA_NAME}@${PROJECT_ID}.iam.gserviceaccount.com" \
  --role="roles/secretmanager.secretAccessor"

# Allow Kubernetes SA to impersonate GCP SA (Workload Identity)
gcloud iam service-accounts add-iam-policy-binding \
  ${GCP_SA_NAME}@${PROJECT_ID}.iam.gserviceaccount.com \
  --role=roles/iam.workloadIdentityUser \
  --member="serviceAccount:${PROJECT_ID}.svc.id.goog[${K8S_NAMESPACE}/${K8S_SA_NAME}]"

echo "Workload Identity configured successfully"
```

### Example application (Python)

```python
# app.py - Python Flask app connecting to Memorystore
import os
import redis
from flask import Flask, jsonify

app = Flask(__name__)

# Redis configuration from environment
REDIS_HOST = os.getenv('REDIS_HOST', 'localhost')
REDIS_PORT = int(os.getenv('REDIS_PORT', 6379))
REDIS_PASSWORD = os.getenv('REDIS_PASSWORD', '')
REDIS_TLS_ENABLED = os.getenv('REDIS_TLS_ENABLED', 'false').lower() == 'true'

# Initialize Redis client
redis_client = redis.Redis(
    host=REDIS_HOST,
    port=REDIS_PORT,
    password=REDIS_PASSWORD,
    ssl=REDIS_TLS_ENABLED,
    ssl_cert_reqs=None if REDIS_TLS_ENABLED else False,
    decode_responses=True,
    socket_connect_timeout=5,
    socket_timeout=5,
    retry_on_timeout=True,
    health_check_interval=30
)

@app.route('/healthz')
def health():
    """Health check endpoint"""
    try:
        redis_client.ping()
        return jsonify({"status": "healthy", "redis": "connected"}), 200
    except Exception as e:
        return jsonify({"status": "unhealthy", "error": str(e)}), 503

@app.route('/ready')
def ready():
    """Readiness check endpoint"""
    try:
        redis_client.ping()
        return jsonify({"status": "ready"}), 200
    except Exception as e:
        return jsonify({"status": "not_ready", "error": str(e)}), 503

@app.route('/set/<key>/<value>')
def set_key(key, value):
    """Set a key-value pair"""
    try:
        redis_client.set(key, value, ex=3600)  # 1 hour TTL
        return jsonify({"status": "success", "key": key, "value": value}), 200
    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500

@app.route('/get/<key>')
def get_key(key):
    """Get a value by key"""
    try:
        value = redis_client.get(key)
        if value is None:
            return jsonify({"status": "not_found", "key": key}), 404
        return jsonify({"status": "success", "key": key, "value": value}), 200
    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500

@app.route('/info')
def redis_info():
    """Get Redis info"""
    try:
        info = redis_client.info('server')
        return jsonify({
            "status": "success",
            "redis_version": info.get('redis_version'),
            "uptime_days": info.get('uptime_in_days'),
            "connected_clients": redis_client.info('clients').get('connected_clients')
        }), 200
    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8080)
```

### Dockerfile

```dockerfile
# Dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application
COPY app.py .

# Run as non-root user
RUN useradd -m -u 1000 appuser && chown -R appuser:appuser /app
USER appuser

EXPOSE 8080

CMD ["gunicorn", "--bind", "0.0.0.0:8080", "--workers", "4", "--timeout", "60", "app:app"]
```

```txt
# requirements.txt
flask==3.0.0
redis==5.0.1
gunicorn==21.2.0
```

---

## ðŸ“Š Monitoring avec Cloud Monitoring

### Dashboard personnalisÃ©

```json
{
  "displayName": "Redis Production Dashboard",
  "mosaicLayout": {
    "columns": 12,
    "tiles": [
      {
        "width": 6,
        "height": 4,
        "widget": {
          "title": "Redis CPU Utilization",
          "xyChart": {
            "dataSets": [
              {
                "timeSeriesQuery": {
                  "timeSeriesFilter": {
                    "filter": "resource.type=\"redis_instance\" AND metric.type=\"redis.googleapis.com/stats/cpu_utilization\"",
                    "aggregation": {
                      "alignmentPeriod": "60s",
                      "perSeriesAligner": "ALIGN_MEAN"
                    }
                  }
                },
                "plotType": "LINE",
                "targetAxis": "Y1"
              }
            ],
            "yAxis": {
              "label": "CPU %",
              "scale": "LINEAR"
            }
          }
        }
      },
      {
        "xPos": 6,
        "width": 6,
        "height": 4,
        "widget": {
          "title": "Redis Memory Usage Ratio",
          "xyChart": {
            "dataSets": [
              {
                "timeSeriesQuery": {
                  "timeSeriesFilter": {
                    "filter": "resource.type=\"redis_instance\" AND metric.type=\"redis.googleapis.com/stats/memory/usage_ratio\"",
                    "aggregation": {
                      "alignmentPeriod": "60s",
                      "perSeriesAligner": "ALIGN_MEAN"
                    }
                  }
                },
                "plotType": "LINE",
                "targetAxis": "Y1"
              }
            ],
            "yAxis": {
              "label": "Memory Ratio",
              "scale": "LINEAR"
            },
            "thresholds": [
              {
                "value": 0.85,
                "color": "YELLOW",
                "direction": "ABOVE"
              },
              {
                "value": 0.95,
                "color": "RED",
                "direction": "ABOVE"
              }
            ]
          }
        }
      },
      {
        "yPos": 4,
        "width": 6,
        "height": 4,
        "widget": {
          "title": "Redis Operations (Calls/sec)",
          "xyChart": {
            "dataSets": [
              {
                "timeSeriesQuery": {
                  "timeSeriesFilter": {
                    "filter": "resource.type=\"redis_instance\" AND metric.type=\"redis.googleapis.com/stats/ops_per_sec\"",
                    "aggregation": {
                      "alignmentPeriod": "60s",
                      "perSeriesAligner": "ALIGN_RATE"
                    }
                  }
                },
                "plotType": "LINE",
                "targetAxis": "Y1"
              }
            ]
          }
        }
      },
      {
        "xPos": 6,
        "yPos": 4,
        "width": 6,
        "height": 4,
        "widget": {
          "title": "Connected Clients",
          "xyChart": {
            "dataSets": [
              {
                "timeSeriesQuery": {
                  "timeSeriesFilter": {
                    "filter": "resource.type=\"redis_instance\" AND metric.type=\"redis.googleapis.com/clients/connected\"",
                    "aggregation": {
                      "alignmentPeriod": "60s",
                      "perSeriesAligner": "ALIGN_MAX"
                    }
                  }
                },
                "plotType": "LINE",
                "targetAxis": "Y1"
              }
            ]
          }
        }
      },
      {
        "yPos": 8,
        "width": 6,
        "height": 4,
        "widget": {
          "title": "Cache Hit Ratio",
          "xyChart": {
            "dataSets": [
              {
                "timeSeriesQuery": {
                  "timeSeriesFilter": {
                    "filter": "resource.type=\"redis_instance\" AND metric.type=\"redis.googleapis.com/stats/cache_hit_ratio\"",
                    "aggregation": {
                      "alignmentPeriod": "300s",
                      "perSeriesAligner": "ALIGN_MEAN"
                    }
                  }
                },
                "plotType": "LINE",
                "targetAxis": "Y1"
              }
            ],
            "yAxis": {
              "label": "Hit Ratio",
              "scale": "LINEAR"
            },
            "thresholds": [
              {
                "value": 0.8,
                "color": "YELLOW",
                "direction": "BELOW"
              }
            ]
          }
        }
      },
      {
        "xPos": 6,
        "yPos": 8,
        "width": 6,
        "height": 4,
        "widget": {
          "title": "Replication Lag (seconds)",
          "xyChart": {
            "dataSets": [
              {
                "timeSeriesQuery": {
                  "timeSeriesFilter": {
                    "filter": "resource.type=\"redis_instance\" AND metric.type=\"redis.googleapis.com/replication/master/slaves/lag\"",
                    "aggregation": {
                      "alignmentPeriod": "60s",
                      "perSeriesAligner": "ALIGN_MAX"
                    }
                  }
                },
                "plotType": "LINE",
                "targetAxis": "Y1"
              }
            ],
            "thresholds": [
              {
                "value": 5,
                "color": "YELLOW",
                "direction": "ABOVE"
              },
              {
                "value": 10,
                "color": "RED",
                "direction": "ABOVE"
              }
            ]
          }
        }
      }
    ]
  }
}
```

### MÃ©triques disponibles

```yaml
Performance Metrics:
â”œâ”€â”€ redis.googleapis.com/stats/cpu_utilization
â”‚   â””â”€â”€ CPU usage (0-1)
â”œâ”€â”€ redis.googleapis.com/stats/ops_per_sec
â”‚   â””â”€â”€ Operations per second
â”œâ”€â”€ redis.googleapis.com/commands/calls
â”‚   â””â”€â”€ Command calls (by command type)
â””â”€â”€ redis.googleapis.com/stats/average_ttl
    â””â”€â”€ Average TTL of keys with TTL

Memory Metrics:
â”œâ”€â”€ redis.googleapis.com/stats/memory/usage
â”‚   â””â”€â”€ Used memory (bytes)
â”œâ”€â”€ redis.googleapis.com/stats/memory/usage_ratio
â”‚   â””â”€â”€ Used memory ratio (0-1)
â”œâ”€â”€ redis.googleapis.com/stats/memory/system_memory_usage_ratio
â”‚   â””â”€â”€ System memory usage
â””â”€â”€ redis.googleapis.com/stats/evicted_keys
    â””â”€â”€ Number of evicted keys

Connections:
â”œâ”€â”€ redis.googleapis.com/clients/connected
â”‚   â””â”€â”€ Connected clients
â”œâ”€â”€ redis.googleapis.com/clients/blocked
â”‚   â””â”€â”€ Blocked clients
â””â”€â”€ redis.googleapis.com/stats/connections/total
    â””â”€â”€ Total connections received

Cache Efficiency:
â”œâ”€â”€ redis.googleapis.com/stats/cache_hit_ratio
â”‚   â””â”€â”€ Hit ratio (0-1)
â”œâ”€â”€ redis.googleapis.com/keyspace/keys
â”‚   â””â”€â”€ Total keys
â””â”€â”€ redis.googleapis.com/keyspace/avg_ttl
    â””â”€â”€ Average TTL

Replication (Standard tier):
â”œâ”€â”€ redis.googleapis.com/replication/master/slaves/lag
â”‚   â””â”€â”€ Replication lag (seconds)
â”œâ”€â”€ redis.googleapis.com/replication/master/slaves/offset
â”‚   â””â”€â”€ Replication offset
â””â”€â”€ redis.googleapis.com/replication/role
    â””â”€â”€ Instance role (master/slave)

Network:
â”œâ”€â”€ redis.googleapis.com/stats/network_traffic
â”‚   â””â”€â”€ Network bytes (in/out)
â””â”€â”€ redis.googleapis.com/stats/reject_connections
    â””â”€â”€ Rejected connections
```

---

## ðŸ’° Pricing dÃ©taillÃ© (2024)

### Grille tarifaire

```yaml
Basic Tier (per GB-hour, europe-west1):
â”œâ”€â”€ Pricing: â‚¬0.035/GB-hour
â”œâ”€â”€ Monthly (730h): â‚¬25.55/GB-month
â””â”€â”€ No SLA

Standard Tier (per GB-hour, europe-west1):
â”œâ”€â”€ Pricing: â‚¬0.087/GB-hour
â”œâ”€â”€ Monthly (730h): â‚¬63.51/GB-month
â””â”€â”€ 99.9% SLA

Examples (Standard Tier, europe-west1):
â”œâ”€â”€ 5 GB:   â‚¬0.435/h  â†’  â‚¬317.55/mois
â”œâ”€â”€ 10 GB:  â‚¬0.870/h  â†’  â‚¬635.10/mois
â”œâ”€â”€ 20 GB:  â‚¬1.740/h  â†’  â‚¬1,270.20/mois
â”œâ”€â”€ 50 GB:  â‚¬4.350/h  â†’  â‚¬3,175.50/mois
â”œâ”€â”€ 100 GB: â‚¬8.700/h  â†’  â‚¬6,351.00/mois
â”œâ”€â”€ 200 GB: â‚¬17.400/h â†’  â‚¬12,702.00/mois
â””â”€â”€ 300 GB: â‚¬26.100/h â†’  â‚¬19,053.00/mois

Read Replicas (Standard tier only):
â”œâ”€â”€ Cost per replica: â‚¬0.029/GB-hour
â”œâ”€â”€ Example: 50GB + 2 replicas
â”‚   â”œâ”€â”€ Primary: â‚¬4.350/h
â”‚   â”œâ”€â”€ Replica 1: â‚¬1.450/h
â”‚   â”œâ”€â”€ Replica 2: â‚¬1.450/h
â”‚   â””â”€â”€ Total: â‚¬7.250/h (â‚¬5,292.50/mois)

CoÃ»ts additionnels:
â”œâ”€â”€ Snapshots: Inclus (automated daily)
â”œâ”€â”€ Cloud Storage (export/import): â‚¬0.020/GB-mois (Standard)
â”œâ”€â”€ Network egress (internet): â‚¬0.12/GB
â”œâ”€â”€ Network egress (same region): Gratuit
â”œâ”€â”€ Network egress (cross-region): â‚¬0.01/GB
â””â”€â”€ Pas de Reserved Instances (prix fixes)

Comparaison par rÃ©gion (Standard 50GB):
â”œâ”€â”€ us-central1:    $3,190/mois  (le moins cher)
â”œâ”€â”€ europe-west1:   â‚¬3,175/mois
â”œâ”€â”€ asia-southeast1: $3,650/mois
â””â”€â”€ australia-southeast1: $4,015/mois (le plus cher)
```

### Calculateur de coÃ»t

```python
# GCP Memorystore Pricing Calculator

class MemorystorePricing:
    """Calculate Memorystore costs"""

    # Pricing per GB-hour (europe-west1, 2024)
    BASIC_PRICE_GB_HOUR = 0.035  # EUR
    STANDARD_PRICE_GB_HOUR = 0.087  # EUR
    REPLICA_PRICE_GB_HOUR = 0.029  # EUR

    HOURS_PER_MONTH = 730

    @classmethod
    def calculate_monthly_cost(
        cls,
        tier: str,
        memory_gb: int,
        replica_count: int = 0
    ) -> dict:
        """
        Calculate monthly cost for Memorystore

        Args:
            tier: 'basic' or 'standard'
            memory_gb: Memory size in GB (1-300)
            replica_count: Number of read replicas (0-5, Standard only)
        """

        if tier.lower() == 'basic':
            price_per_gb_hour = cls.BASIC_PRICE_GB_HOUR
            replica_count = 0  # Basic doesn't support replicas
        else:
            price_per_gb_hour = cls.STANDARD_PRICE_GB_HOUR

        # Primary instance cost
        primary_hourly = memory_gb * price_per_gb_hour
        primary_monthly = primary_hourly * cls.HOURS_PER_MONTH

        # Replica costs
        replica_hourly = 0
        if replica_count > 0 and tier.lower() == 'standard':
            replica_hourly = memory_gb * cls.REPLICA_PRICE_GB_HOUR * replica_count

        replica_monthly = replica_hourly * cls.HOURS_PER_MONTH

        # Total compute
        total_hourly = primary_hourly + replica_hourly
        total_monthly = total_hourly * cls.HOURS_PER_MONTH

        # Additional costs (estimates)
        storage_cost = 50  # â‚¬50/month for exports/imports
        egress_cost = 100  # â‚¬100/month for network egress

        additional_monthly = storage_cost + egress_cost

        return {
            'tier': tier,
            'memory_gb': memory_gb,
            'replica_count': replica_count,
            'primary_hourly': round(primary_hourly, 3),
            'replica_hourly': round(replica_hourly, 3),
            'total_hourly': round(total_hourly, 3),
            'primary_monthly': round(primary_monthly, 2),
            'replica_monthly': round(replica_monthly, 2),
            'compute_monthly': round(total_monthly, 2),
            'additional_monthly': round(additional_monthly, 2),
            'total_monthly': round(total_monthly + additional_monthly, 2),
            'total_annual': round((total_monthly + additional_monthly) * 12, 2)
        }

# Examples
print("=== Scenario 1: Production Standard 50GB + 2 replicas ===")
cost1 = MemorystorePricing.calculate_monthly_cost(
    tier='standard',
    memory_gb=50,
    replica_count=2
)
print(f"Primary: â‚¬{cost1['primary_monthly']:,}/month")
print(f"Replicas (2): â‚¬{cost1['replica_monthly']:,}/month")
print(f"Total: â‚¬{cost1['total_monthly']:,}/month")
print(f"Annual: â‚¬{cost1['total_annual']:,}/year")

print("\n=== Scenario 2: Large cache 200GB Standard ===")
cost2 = MemorystorePricing.calculate_monthly_cost(
    tier='standard',
    memory_gb=200,
    replica_count=3
)
print(f"Total: â‚¬{cost2['total_monthly']:,}/month")
print(f"Annual: â‚¬{cost2['total_annual']:,}/year")

print("\n=== Scenario 3: Dev/Test Basic 10GB ===")
cost3 = MemorystorePricing.calculate_monthly_cost(
    tier='basic',
    memory_gb=10,
    replica_count=0
)
print(f"Total: â‚¬{cost3['total_monthly']:,}/month")
print(f"Annual: â‚¬{cost3['total_annual']:,}/year")

print("\n=== Comparison: Basic vs Standard (50GB) ===")
basic = MemorystorePricing.calculate_monthly_cost('basic', 50, 0)
standard = MemorystorePricing.calculate_monthly_cost('standard', 50, 0)
print(f"Basic:    â‚¬{basic['total_monthly']}/month")
print(f"Standard: â‚¬{standard['total_monthly']}/month")
print(f"Premium for HA: â‚¬{standard['total_monthly'] - basic['total_monthly']}/month")
print(f"Premium %: {((standard['total_monthly'] / basic['total_monthly']) - 1) * 100:.1f}%")
```

Output:
```
=== Scenario 1: Production Standard 50GB + 2 replicas ===
Primary: â‚¬3,175.5/month
Replicas (2): â‚¬2,117/month
Total: â‚¬5,442.5/month
Annual: â‚¬65,310/year

=== Scenario 2: Large cache 200GB Standard ===
Total: â‚¬26,754/month
Annual: â‚¬321,048/year

=== Scenario 3: Dev/Test Basic 10GB ===
Total: â‚¬405.5/month
Annual: â‚¬4,866/year

=== Comparison: Basic vs Standard (50GB) ===
Basic:    â‚¬1,427.5/month
Standard: â‚¬3,325.5/month
Premium for HA: â‚¬1,898/month
Premium %: 132.9%
```

---

## ðŸ“Š Comparaison avec AWS et Azure

### Tableau comparatif dÃ©taillÃ©

| CritÃ¨re | GCP Memorystore | AWS ElastiCache | AWS MemoryDB | Azure Cache Premium |
|---------|-----------------|-----------------|--------------|---------------------|
| **Architecture** |
| Max RAM/instance | 300 GB | 317 GB | 419 GB | 1.2 TB (clustered) |
| Clustering natif | âŒ | âœ… (500 shards) | âœ… (500 shards) | âœ… (10 shards) |
| Read replicas | âœ… (0-5) | âœ… (0-5) | âœ… (0-5) | âŒ |
| **DisponibilitÃ©** |
| SLA | 99.9% | 99.9% / 99.99% | 99.99% | 99.9% / 99.99% |
| Multi-zone | âœ… | âœ… | âœ… (built-in) | âœ… |
| Geo-replication | âŒ | âœ… (Global DS) | âŒ | âœ… (passive) |
| Active-Active | âŒ | âŒ | âŒ | âœ… (Enterprise) |
| **RÃ©seau** |
| VPC native | âœ… (PSC) | Subnet group | Subnet group | âœ… (VNet injection) |
| Private endpoint | âœ… (always) | âœ… | âœ… | âœ… |
| Public endpoint | âŒ | âœ… (optional) | âœ… (optional) | âœ… (optional) |
| **Persistence** |
| RDB | âœ… | âœ… | N/A (WAL) | âœ… |
| AOF | âŒ | âœ… | N/A (WAL) | âœ… |
| Durability | Snapshots | Optional | Guaranteed | Optional |
| **Pricing (50GB Standard/HA)** |
| On-demand | ~â‚¬3,175/mois | ~$360/mois | ~$640/mois | ~â‚¬1,000/mois |
| Reserved/RI | N/A | ~$145/mois | ~$260/mois | ~â‚¬450/mois |
| **Simplicity** | â­â­â­â­â­ | â­â­â­ | â­â­â­ | â­â­â­â­ |
| **Flexibility** | â­â­ | â­â­â­â­ | â­â­â­â­ | â­â­â­â­ |
| **Feature set** | â­â­â­ | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ |

### Forces et faiblesses

**GCP Memorystore - Forces :**
```
âœ… SimplicitÃ© d'utilisation (minimal config)
âœ… IntÃ©gration GCP native (GKE, Cloud Run, etc.)
âœ… Private Service Connect (sÃ©curitÃ© maximale)
âœ… Automated daily backups inclus
âœ… Read replicas faciles Ã  dÃ©ployer
âœ… Tarification simple (pas de surprise)
```

**GCP Memorystore - Faiblesses :**
```
âŒ Pas de clustering (max 300GB single instance)
âŒ Pas de cross-region replication
âŒ Pas de Redis modules (Stack)
âŒ Moins de configuration options
âŒ Pas de Reserved Instances (coÃ»t fixe Ã©levÃ©)
âŒ AOF persistence non disponible
```

### Quand choisir Memorystore ?

```yaml
Utilisez Memorystore si:
â˜‘ Vous Ãªtes dÃ©jÃ  sur GCP
â˜‘ SimplicitÃ© > FlexibilitÃ©
â˜‘ Dataset <300GB (single instance suffit)
â˜‘ IntÃ©gration GKE/Cloud Run prioritaire
â˜‘ Pas besoin de features avancÃ©es
â˜‘ Budget permet prix GCP

N'utilisez PAS Memorystore si:
â˜’ Besoin de clustering (>300GB)
â˜’ Cross-region replication requis
â˜’ Redis modules nÃ©cessaires (Stack)
â˜’ Budget trÃ¨s contraint (pas de RI)
â˜’ Maximum de configuration requis
```

---

## âœ… Best Practices

### Configuration production

```yaml
Instance Configuration:
â”œâ”€â”€ Tier: STANDARD_HA (toujours en prod)
â”œâ”€â”€ Memory: Right-size (monitor usage)
â”œâ”€â”€ Redis version: Latest stable (7.2)
â”œâ”€â”€ Read replicas: 2-3 (pour read-heavy workloads)
â”œâ”€â”€ AUTH: Enabled (toujours)
â”œâ”€â”€ TLS: SERVER_AUTHENTICATION (toujours)
â””â”€â”€ Persistence: RDB avec snapshot horaire

Network:
â”œâ”€â”€ VPC: Dedicated VPC pour chaque environnement
â”œâ”€â”€ Private Service Connect: Oui (jamais d'IP publique)
â”œâ”€â”€ Firewall: Minimal (only necessary rules)
â””â”€â”€ VPC Peering: Pour cross-project si nÃ©cessaire

Monitoring:
â”œâ”€â”€ Cloud Monitoring: Tous les dashboards configurÃ©s
â”œâ”€â”€ Alerting: CPU >80%, Memory >85%, Lag >5s
â”œâ”€â”€ Logging: Export to BigQuery pour analyse
â””â”€â”€ Uptime checks: Si applicable

Backup:
â”œâ”€â”€ Daily snapshots: ActivÃ©s
â”œâ”€â”€ Export to GCS: Hebdomadaire
â”œâ”€â”€ Retention: 30 jours minimum
â””â”€â”€ Test restore: Trimestriel

Security:
â”œâ”€â”€ Workload Identity: Pour GKE
â”œâ”€â”€ Secret Manager: Pour AUTH password
â”œâ”€â”€ IAM: Least privilege
â”œâ”€â”€ Audit logs: ActivÃ©s
â””â”€â”€ VPC Service Controls: Si compliance strict
```

### Optimisation des coÃ»ts

```yaml
Right-sizing:
â”œâ”€â”€ Monitor memory usage trends
â”œâ”€â”€ Scale down si usage <50% constant
â”œâ”€â”€ Start small, scale up as needed
â””â”€â”€ Use Basic tier pour dev/test

Read replicas:
â”œâ”€â”€ Only if read-heavy workload
â”œâ”€â”€ Monitor actual read distribution
â”œâ”€â”€ Consider app-level caching instead
â””â”€â”€ Replicas cost 1/3 of primary

Network:
â”œâ”€â”€ Keep traffic in same region (gratuit)
â”œâ”€â”€ Use Cloud CDN pour edge caching
â”œâ”€â”€ Minimize cross-region calls
â””â”€â”€ Use compression when possible

Exports/Imports:
â”œâ”€â”€ Schedule during low-traffic hours
â”œâ”€â”€ Use lifecycle policies on GCS
â”œâ”€â”€ Compress exports (RDB is compressible)
â””â”€â”€ Delete old backups regularly
```

---

## ðŸŽ¯ Conclusion

### Points clÃ©s Ã  retenir

1. **SimplicitÃ© avant tout**
   - Memorystore = configuration minimale
   - Managed fully par Google
   - Parfait pour use-cases standards

2. **Limitations claires**
   - Pas de clustering (max 300GB)
   - Pas de cross-region
   - Moins de features qu'AWS/Azure

3. **IntÃ©gration GCP native**
   - Private Service Connect
   - Workload Identity avec GKE
   - Cloud Monitoring natif

4. **Pricing simple mais Ã©levÃ©**
   - Pas de Reserved Instances
   - â‚¬63.51/GB-mois (Standard)
   - Read replicas Ã©conomiques (â‚¬21.17/GB-mois)

5. **IdÃ©al pour :**
   - Projets 100% GCP
   - Datasets <300GB
   - Besoin de simplicitÃ©
   - IntÃ©gration GKE forte

### Recommandations finales

**Pour production GCP standard :**
```
Memorystore Standard HA
â”œâ”€â”€ 50 GB RAM
â”œâ”€â”€ 2 read replicas
â”œâ”€â”€ AUTH + TLS enabled
â”œâ”€â”€ Daily RDB snapshots
â””â”€â”€ Cloud Monitoring dashboards
CoÃ»t: ~â‚¬5,400/mois
```

**Alternative si budget contraint :**
```
ConsidÃ©rer AWS ElastiCache avec RI 3 ans
ou Azure Cache Premium avec RI
â†’ Ã‰conomies significatives (~60%)
```

**Alternative si >300GB :**
```
AWS ElastiCache/MemoryDB avec clustering
ou Azure Cache Premium avec sharding
â†’ Memorystore n'est pas adaptÃ©
```

---

**ðŸŽ¯ Section suivante :** Nous allons maintenant explorer **Redis Enterprise Cloud** dans la section 15.5, la solution premium multi-cloud avec toutes les features avancÃ©es.

â­ï¸ [Redis Cloud (Redis Enterprise Cloud)](/15-redis-cloud-conteneurs/05-redis-cloud-enterprise.md)
