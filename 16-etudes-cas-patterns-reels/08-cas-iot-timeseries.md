ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# Cas #8 : IoT et Time-Series avec RedisTimeSeries

## Vue d'ensemble

**Niveau** : â­â­â­ AvancÃ©
**ComplexitÃ© technique** : Ã‰levÃ©e
**Impact production** : Critique (operational intelligence)
**Technologies** : Redis Stack (RedisTimeSeries) + IoT protocols

---

## 1. Contexte et problÃ©matique

### ScÃ©nario business

Une plateforme de monitoring industriel IoT (type ThingWorx, PTC, ou Siemens MindSphere) pour Smart Factory :

**Chiffres clÃ©s** :
- 100 000 capteurs IoT dÃ©ployÃ©s (tempÃ©rature, pression, vibration, Ã©nergie)
- 1 million de data points par seconde
- 86 milliards de data points par jour
- 50 usines connectÃ©es Ã  travers le monde
- SLA : Latence ingestion < 100ms, Query < 500ms
- Retention : 1s granularity (1 semaine), 1min (1 an), 1h (5 ans)
- CoÃ»t actuel : $50k/mois â†’ Objectif : $15k/mois

**Besoins mÃ©tier critiques** :

1. **Ingestion haute frÃ©quence**
   - 1M data points/sec en continu
   - Pics Ã  5M data points/sec
   - Latence < 100ms (sensor â†’ database)
   - Pas de perte de donnÃ©es

2. **Downsampling automatique**
   - Raw data (1s) : 1 semaine
   - Minute aggregates (avg, max, min) : 1 an
   - Hour aggregates : 5 ans
   - Day aggregates : illimitÃ©
   - Automatic compaction en background

3. **RequÃªtes analytics temps rÃ©el**
   - Dashboard : 100 mÃ©triques en < 500ms
   - Time range queries : "Last 24h", "Last 30 days"
   - Aggregations : avg, sum, min, max, stddev, percentiles
   - Multi-sensor queries : "All temp sensors in zone A"

4. **Alerting et anomaly detection**
   - Seuils statiques : temp > 80Â°C
   - Seuils dynamiques : +20% vs moyenne 1h
   - Anomaly detection : ML-based (isolation forest)
   - Alert latency < 5 secondes

### ProblÃ¨mes Ã  rÃ©soudre

#### 1. **Volume et vÃ©locitÃ© des donnÃ©es**

```
Challenge : 1M data points/sec = 86B/jour = 31 Trillion/an

Storage naÃ¯f (PostgreSQL) :
- Size: 31T Ã— 50 bytes = 1.5 PB/an (prohibitif!)
- Write throughput: 1M inserts/sec (impossible sans sharding massif)
- Query: Full table scan sur milliards de rows (minutes)

Solution : Time-series optimized DB
- Compression: 10:1 ratio â†’ 150 TB/an
- Write optimized: Sequential writes, WAL
- Query optimized: IndexÃ© par temps
```

#### 2. **Retention policies et storage tiers**

```
ProblÃ¨me : Balance entre prÃ©cision et coÃ»t

StratÃ©gie multi-tier :
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Granularity â”‚ Retention  â”‚ Storage  â”‚ Cost    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1 second    â”‚ 7 days     â”‚ Hot SSD  â”‚ High    â”‚
â”‚ 1 minute    â”‚ 1 year     â”‚ Warm HDD â”‚ Medium  â”‚
â”‚ 1 hour      â”‚ 5 years    â”‚ Cold S3  â”‚ Low     â”‚
â”‚ 1 day       â”‚ Forever    â”‚ Archive  â”‚ Minimal â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Auto-downsampling :
raw_1s â†’ minute_avg â†’ hour_avg â†’ day_avg
         (automatic compaction rules)
```

#### 3. **Query performance sur large time ranges**

```
ProblÃ¨me : Query "average temperature last 30 days"

Approche naÃ¯ve :
SELECT AVG(value) FROM sensor_data
WHERE sensor_id = 'temp_001'
    AND timestamp >= NOW() - INTERVAL '30 days'
â†’ Scan 30 Ã— 86,400 = 2,592,000 rows
â†’ Latency : 5-30 seconds

Solution : Pre-aggregated data
Query minute_avg (43,200 rows au lieu de 2.6M)
â†’ Latency : < 100ms
```

#### 4. **Multi-sensor aggregations**

```
Besoin : "Average temperature across all 500 sensors in zone A"

Challenge :
- 500 sensors Ã— 86,400 points/day = 43.2M data points
- Cross-sensor aggregation

Solution : Hierarchical aggregation
sensor_001 â†’ avg â†’ â”
sensor_002 â†’ avg â†’ â”œâ”€â†’ zone_A_avg
    ...            â”‚
sensor_500 â†’ avg â†’ â”˜
```

---

## 2. Analyse des alternatives

### Option 1 : PostgreSQL avec TimescaleDB

```sql
-- Extension TimescaleDB
CREATE EXTENSION timescaledb;

-- Hypertable (automatic partitioning)
CREATE TABLE sensor_data (
    time TIMESTAMPTZ NOT NULL,
    sensor_id TEXT NOT NULL,
    value DOUBLE PRECISION,
    metadata JSONB
);

SELECT create_hypertable('sensor_data', 'time');

-- Continuous aggregates (auto-downsampling)
CREATE MATERIALIZED VIEW sensor_data_1min
WITH (timescaledb.continuous) AS
SELECT
    time_bucket('1 minute', time) AS bucket,
    sensor_id,
    AVG(value) AS avg_value,
    MAX(value) AS max_value,
    MIN(value) AS min_value
FROM sensor_data
GROUP BY bucket, sensor_id;

-- Retention policy
SELECT add_retention_policy('sensor_data', INTERVAL '7 days');
```

**Avantages** :
- âœ… SQL familier
- âœ… Continuous aggregates (auto-downsampling)
- âœ… Retention policies automatiques
- âœ… ACID compliance

**InconvÃ©nients** :
- âŒ Latence : 50-500ms queries (index overhead)
- âŒ Write throughput : 100k-500k/sec (limite)
- âŒ Scaling horizontal complexe
- âŒ CoÃ»t : $500-2000/mois pour 1M points/sec

**Benchmark** (TimescaleDB 2.11, 1M data points/sec) :
```
OpÃ©ration                    Latence
----------------------------------------
Write (batch 1000)          50-200ms
Query (1h range)            100-300ms
Query (30d range)           500-2000ms
Continuous aggregate        Background (5-30 min lag)
```

**Verdict** : âš ï¸ **Solide mais coÃ»teux** Ã  grande Ã©chelle.

---

### Option 2 : InfluxDB

```python
from influxdb_client import InfluxDBClient, Point
from influxdb_client.client.write_api import SYNCHRONOUS

client = InfluxDBClient(url="http://localhost:8086", token="...")
write_api = client.write_api(write_options=SYNCHRONOUS)

# Write
point = Point("temperature") \
    .tag("sensor_id", "temp_001") \
    .tag("zone", "A") \
    .field("value", 25.5)

write_api.write(bucket="sensors", record=point)

# Query (Flux language)
query = '''
from(bucket: "sensors")
  |> range(start: -24h)
  |> filter(fn: (r) => r._measurement == "temperature")
  |> aggregateWindow(every: 1m, fn: mean)
'''

result = client.query_api().query(query)
```

**Avantages** :
- âœ… Time-series native
- âœ… High write throughput (500k-1M/sec)
- âœ… Automatic downsampling (tasks)
- âœ… Flux query language (puissant)

**InconvÃ©nients** :
- âš ï¸ Latence : 10-100ms queries
- âŒ CoÃ»t : $500-5000/mois (InfluxDB Cloud)
- âŒ Learning curve (Flux)
- âš ï¸ Scaling : Cluster setup complexe

**Benchmark** (InfluxDB 2.7, 1M points/sec) :
```
OpÃ©ration                    Latence
----------------------------------------
Write (batch 5000)          20-100ms
Query (1h range, 1 sensor)  10-50ms
Query (24h range, 100 sensors) 100-500ms
Downsampling                Background (1-10 min lag)
```

**Verdict** : âœ… **Excellent mais coÃ»teux** pour trÃ¨s grande Ã©chelle.

---

### Option 3 : RedisTimeSeries âœ…

```bash
# Create time-series
TS.CREATE sensor:temp_001:raw
    RETENTION 604800000  # 7 days in ms
    DUPLICATE_POLICY LAST
    LABELS sensor_id temp_001 zone A type temperature

# Create downsampling rules (automatic)
TS.CREATERULE sensor:temp_001:raw sensor:temp_001:1min
    AGGREGATION avg 60000  # 1 min in ms

TS.CREATERULE sensor:temp_001:1min sensor:temp_001:1hour
    AGGREGATION avg 3600000  # 1 hour in ms

# Add data point
TS.ADD sensor:temp_001:raw * 25.5

# Query
TS.RANGE sensor:temp_001:raw - + AGGREGATION avg 60000
```

**Avantages** :
- âœ… Latence : **< 5ms** queries (in-memory)
- âœ… Write throughput : **1M-5M/sec** per instance
- âœ… Automatic downsampling (real-time)
- âœ… Native aggregations (avg, sum, min, max, etc.)
- âœ… CoÃ»t : **$200-500/mois** pour 1M points/sec
- âœ… Simple Ã  opÃ©rer

**InconvÃ©nients** :
- âš ï¸ In-memory : RAM required (mais tiering disponible)
- âš ï¸ Query language moins riche que Flux
- âš ï¸ Pas d'ACID (mais acceptable pour IoT)

**Benchmark** (Redis Stack 7.2, 1M points/sec) :
```
OpÃ©ration                         Latency
--------------------------------------------
Write (TS.ADD)                   0.1-1ms
Write (batch pipeline)           0.05ms per point
Query (1h range, 1 sensor)       2-10ms
Query (24h range, 100 sensors)   50-200ms
Multi-sensor aggregation         10-100ms
Downsampling                     Real-time (< 1s lag)
Memory                           ~50 bytes per data point
```

**Trade-off assumÃ©** :
- â• Performance Ã— 100 (vs TimescaleDB)
- â• CoÃ»t Ã· 10 (vs InfluxDB Cloud)
- â• Latence ultra-faible
- â• Downsampling en temps rÃ©el
- â– RAM required (mais gÃ©rable avec tiering)
- â– Query language moins avancÃ©

**Verdict** : âœ… **Solution optimale** pour IoT haute frÃ©quence.

---

## 3. Architecture proposÃ©e

### 3.1 Vue d'ensemble

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    IoT Devices / Sensors                    â”‚
â”‚  100,000 sensors (temp, pressure, vibration, energy)        â”‚
â”‚  - Sending data every 1 second                              â”‚
â”‚  - Protocol: MQTT, HTTP, CoAP                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ MQTT / HTTP
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              IoT Gateway / Edge Processing                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  - Data validation                                  â”‚    â”‚
â”‚  â”‚  - Preprocessing (unit conversion, filtering)       â”‚    â”‚
â”‚  â”‚  - Batching (100 points per batch)                  â”‚    â”‚
â”‚  â”‚  - Compression                                      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ Batch HTTP
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Ingestion API (Load Balanced)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚  Ingest 1  â”‚   â”‚  Ingest 2  â”‚   â”‚  Ingest N  â”‚           â”‚
â”‚  â”‚  Service   â”‚   â”‚  Service   â”‚   â”‚  Service   â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                â”‚                â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           RedisTimeSeries Cluster                           â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Time-Series Structure (per sensor):                 â”‚   â”‚
â”‚  â”‚                                                      â”‚   â”‚
â”‚  â”‚  sensor:temp_001:raw                                 â”‚   â”‚
â”‚  â”‚  â”œâ”€ Retention: 7 days                                â”‚   â”‚
â”‚  â”‚  â”œâ”€ Labels: {sensor_id, zone, type}                  â”‚   â”‚
â”‚  â”‚  â””â”€ Compaction rules:                                â”‚   â”‚
â”‚  â”‚      â”œâ”€> sensor:temp_001:1min (avg, 1 year)          â”‚   â”‚
â”‚  â”‚      â”‚    â””â”€> sensor:temp_001:1hour (avg, 5 years)   â”‚   â”‚
â”‚  â”‚      â”‚         â””â”€> sensor:temp_001:1day (avg, âˆ)     â”‚   â”‚
â”‚  â”‚      â”œâ”€> sensor:temp_001:1min_max (max, 1 year)      â”‚   â”‚
â”‚  â”‚      â””â”€> sensor:temp_001:1min_min (min, 1 year)      â”‚   â”‚
â”‚  â”‚                                                      â”‚   â”‚
â”‚  â”‚  Multi-level aggregation (automatic):                â”‚   â”‚
â”‚  â”‚  raw (1s) â†’ 1min â†’ 1hour â†’ 1day                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                             â”‚
â”‚  Cluster Configuration:                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚ Shard 1  â”‚  â”‚ Shard 2  â”‚  â”‚ Shard N  â”‚                   â”‚
â”‚  â”‚ Master   â”‚  â”‚ Master   â”‚  â”‚ Master   â”‚                   â”‚
â”‚  â”‚ Replica  â”‚  â”‚ Replica  â”‚  â”‚ Replica  â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                                             â”‚
â”‚  Memory tiering (optional):                                 â”‚
â”‚  - Hot data (7 days): RAM                                   â”‚
â”‚  - Warm data (1 year): Redis on Flash                       â”‚
â”‚  - Cold data (5 years): S3 export                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Analytics & Visualization Layer                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  - Real-time dashboards (Grafana)                   â”‚    â”‚
â”‚  â”‚  - Alerting engine                                  â”‚    â”‚
â”‚  â”‚  - Anomaly detection (ML)                           â”‚    â”‚
â”‚  â”‚  - Predictive maintenance                           â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 Flux d'ingestion

#### **Write Path (1M points/sec)**

```
1. Sensor sends data point
   {
       "sensor_id": "temp_001",
       "timestamp": 1702300800000,
       "value": 25.5,
       "unit": "celsius"
   }

2. IoT Gateway batches data
   Batch size: 100 points
   Frequency: Every 100ms or when full
   â†’ 10,000 batches/sec

3. Ingestion API receives batch
   POST /api/ingest/batch
   Body: [100 data points]

4. Write to Redis (pipeline)
   pipe = redis.pipeline()
   for point in batch:
       pipe.ts().add(
           f"sensor:{point.sensor_id}:raw",
           point.timestamp,
           point.value
       )
   pipe.execute()

   Latency per batch: ~5ms
   â†’ 5ms / 100 = 0.05ms per point

5. Automatic compaction (background)
   RedisTimeSeries auto-compacts to:
   â”œâ”€ sensor:temp_001:1min (avg)
   â”œâ”€ sensor:temp_001:1min_max (max)
   â”œâ”€ sensor:temp_001:1min_min (min)
   â””â”€ sensor:temp_001:1hour â†’ 1day

   Lag: < 1 second (real-time)

Total latency: Sensor â†’ Queryable = ~100ms
```

#### **Query Path (Dashboard)**

```
User requests: "Temperature trend last 24 hours for sensor temp_001"

1. API receives request
   GET /api/sensors/temp_001/data?range=24h&aggregation=1min

2. Determine optimal granularity
   Range: 24h = 1,440 minutes
   â†’ Use 1min aggregates (1,440 points)

   Alternative: Use raw data (86,400 points) = 60Ã— more data

3. Query RedisTimeSeries
   TS.RANGE sensor:temp_001:1min
       (now - 24h) now
       AGGREGATION avg 60000

4. Response
   [
       [1702300800000, 25.5],
       [1702300860000, 25.6],
       ...
   ]

   Data size: 1,440 points Ã— 16 bytes = 23 KB
   Latency: ~10ms

5. Optional: Multi-sensor query
   TS.MRANGE (now - 24h) now
       AGGREGATION avg 60000
       FILTER zone=A type=temperature

   Returns: 100 sensors Ã— 1,440 points
   Latency: ~100ms
```

### 3.3 DÃ©cisions architecturales clÃ©s

#### **Choix 1 : Multi-level compaction strategy**

```
Strategy: 4-level hierarchy

Level 1 (raw): 1 second granularity
â”œâ”€ Retention: 7 days
â”œâ”€ Storage: In-memory (hot)
â””â”€ Use case: Real-time monitoring, debugging

Level 2 (1min aggregates): avg, max, min
â”œâ”€ Retention: 1 year
â”œâ”€ Storage: In-memory or Redis on Flash
â”œâ”€ Automatic from raw (TS.CREATERULE)
â””â”€ Use case: Dashboard trends, alerts

Level 3 (1hour aggregates): avg, max, min
â”œâ”€ Retention: 5 years
â”œâ”€ Storage: Redis on Flash or periodic export to S3
â”œâ”€ Automatic from 1min
â””â”€ Use case: Historical analysis, reporting

Level 4 (1day aggregates): avg, max, min, sum
â”œâ”€ Retention: Forever
â”œâ”€ Storage: PostgreSQL or S3
â”œâ”€ Manual export (daily cron)
â””â”€ Use case: Long-term trends, compliance

Compaction latency: < 1 second (real-time)
```

**Trade-off assumÃ©** :
- â• Query speed Ã— 60-86,400 (vs raw)
- â• Storage Ã· 60-86,400
- â– Approximation (mais avg/max/min prÃ©servÃ©s)

---

#### **Choix 2 : Labeling strategy pour multi-sensor queries**

```bash
# Hierarchical labels
TS.CREATE sensor:temp_001:raw
    LABELS
        sensor_id temp_001
        zone A
        building Factory_1
        type temperature
        unit celsius
        criticality high

# Permet queries flexibles:

# 1. All sensors in zone A
TS.MRANGE - + FILTER zone=A

# 2. All temperature sensors
TS.MRANGE - + FILTER type=temperature

# 3. High criticality in building Factory_1
TS.MRANGE - + FILTER building=Factory_1 criticality=high

# 4. Combination
TS.MRANGE - + FILTER zone=A type=temperature
```

**Trade-off assumÃ©** :
- â• FlexibilitÃ© maximale pour analytics
- â• No schema changes needed
- â– Label storage overhead (~50 bytes per series)

---

#### **Choix 3 : Batching strategy**

**NaÃ¯f (1 point Ã  la fois)** :
```python
# âŒ 1M round-trips/sec (impossible)
for point in data_points:
    redis.ts().add(key, timestamp, value)

Latency: 1M Ã— 1ms = 1000 seconds!
```

**Optimal (batching avec pipeline)** :
```python
# âœ… 10k round-trips/sec (feasible)
def ingest_batch(points, batch_size=100):
    pipe = redis.pipeline()

    for point in points:
        pipe.ts().add(
            f"sensor:{point.sensor_id}:raw",
            point.timestamp,
            point.value
        )

        if len(pipe) >= batch_size:
            pipe.execute()
            pipe = redis.pipeline()

    if len(pipe) > 0:
        pipe.execute()

# 1M points / 100 per batch = 10k batches
# 10k Ã— 5ms = 50 seconds (vs 1000s)
```

**Trade-off assumÃ©** :
- â• Throughput Ã— 100
- â• Latency acceptable (~100ms end-to-end)
- â– Slightly delayed ingestion (batching delay)

---

## 4. ImplÃ©mentation technique

### 4.1 Code Python (Production-Ready)

```python
"""
IoT Time-Series Platform avec RedisTimeSeries
ImplÃ©mentation production-ready
"""

import time
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, field
from datetime import datetime, timedelta
import logging
from enum import Enum

import redis
from redis.commands.timeseries.commands import TimeSeriesCommands

# Configuration
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

REDIS_CONFIG = {
    'host': 'localhost',
    'port': 6379,
    'db': 0,
    'decode_responses': False
}

# Constants
BATCH_SIZE = 100
RETENTION_POLICIES = {
    'raw': 7 * 24 * 3600 * 1000,      # 7 days in ms
    '1min': 365 * 24 * 3600 * 1000,   # 1 year in ms
    '1hour': 5 * 365 * 24 * 3600 * 1000,  # 5 years in ms
}


# ============================================================================
# Data Classes
# ============================================================================

class AggregationType(Enum):
    AVG = "avg"
    SUM = "sum"
    MIN = "min"
    MAX = "max"
    RANGE = "range"
    COUNT = "count"
    FIRST = "first"
    LAST = "last"
    STD_P = "std.p"  # Standard deviation (population)
    STD_S = "std.s"  # Standard deviation (sample)


@dataclass
class SensorDataPoint:
    """Point de donnÃ©es d'un capteur"""
    sensor_id: str
    timestamp: int  # Unix timestamp in milliseconds
    value: float
    unit: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class TimeSeriesConfig:
    """Configuration d'une time-series"""
    sensor_id: str
    retention_ms: int
    labels: Dict[str, str]
    duplicate_policy: str = "LAST"  # LAST, FIRST, MIN, MAX, SUM


@dataclass
class CompactionRule:
    """RÃ¨gle de compaction"""
    source_key: str
    dest_key: str
    aggregation: AggregationType
    bucket_size_ms: int


# ============================================================================
# IoT Time-Series Manager
# ============================================================================

class IoTTimeSeriesManager:
    """
    Gestionnaire de time-series IoT avec RedisTimeSeries

    Features:
    - High-frequency ingestion (1M points/sec)
    - Automatic downsampling avec compaction rules
    - Multi-level aggregations
    - Flexible querying
    - Alerting support
    """

    def __init__(self, redis_config: Dict = None):
        redis_conf = redis_config or REDIS_CONFIG
        self.redis = redis.Redis(**redis_conf)
        self.ts: TimeSeriesCommands = self.redis.ts()

        # Test connexion
        try:
            self.redis.ping()
            logger.info("IoTTimeSeriesManager initialized")
        except redis.RedisError as e:
            logger.error(f"Redis connection failed: {e}")
            raise

    # ========================================================================
    # Time-Series Creation
    # ========================================================================

    def create_sensor_timeseries(self, config: TimeSeriesConfig) -> bool:
        """
        CrÃ©er une time-series pour un capteur

        Args:
            config: Configuration de la time-series

        Returns:
            True si crÃ©Ã©e, False si existe dÃ©jÃ 
        """
        key = f"sensor:{config.sensor_id}:raw"

        try:
            # Check if exists
            try:
                self.ts.info(key)
                logger.debug(f"Time-series already exists: {key}")
                return False
            except:
                pass

            # Create time-series
            self.ts.create(
                key,
                retention_msecs=config.retention_ms,
                duplicate_policy=config.duplicate_policy,
                labels=config.labels
            )

            logger.info(f"Time-series created: {key}")

            # Create compaction rules
            self._create_compaction_rules(config.sensor_id)

            return True

        except redis.RedisError as e:
            logger.error(f"Failed to create time-series: {e}")
            return False

    def _create_compaction_rules(self, sensor_id: str):
        """
        CrÃ©er rÃ¨gles de compaction automatique

        Hierarchy:
        raw (1s) â†’ 1min (avg/max/min) â†’ 1hour (avg/max/min) â†’ 1day (avg/max/min)
        """
        raw_key = f"sensor:{sensor_id}:raw"

        # Level 1: raw â†’ 1min aggregates
        aggregations_1min = [
            (AggregationType.AVG, "1min"),
            (AggregationType.MAX, "1min_max"),
            (AggregationType.MIN, "1min_min"),
            (AggregationType.SUM, "1min_sum"),
        ]

        for agg_type, suffix in aggregations_1min:
            dest_key = f"sensor:{sensor_id}:{suffix}"

            # Create destination time-series
            try:
                self.ts.create(
                    dest_key,
                    retention_msecs=RETENTION_POLICIES['1min'],
                    labels={"sensor_id": sensor_id, "aggregation": agg_type.value, "interval": "1min"}
                )
            except:
                pass  # Already exists

            # Create compaction rule
            try:
                self.ts.createrule(
                    raw_key,
                    dest_key,
                    aggregation_type=agg_type.value,
                    bucket_size_msec=60000  # 1 minute
                )
                logger.debug(f"Compaction rule created: {raw_key} â†’ {dest_key} ({agg_type.value})")
            except Exception as e:
                logger.warning(f"Failed to create rule: {e}")

        # Level 2: 1min â†’ 1hour aggregates
        min_key = f"sensor:{sensor_id}:1min"
        aggregations_1hour = [
            (AggregationType.AVG, "1hour"),
            (AggregationType.MAX, "1hour_max"),
            (AggregationType.MIN, "1hour_min"),
        ]

        for agg_type, suffix in aggregations_1hour:
            dest_key = f"sensor:{sensor_id}:{suffix}"

            try:
                self.ts.create(
                    dest_key,
                    retention_msecs=RETENTION_POLICIES['1hour'],
                    labels={"sensor_id": sensor_id, "aggregation": agg_type.value, "interval": "1hour"}
                )
            except:
                pass

            try:
                self.ts.createrule(
                    min_key,
                    dest_key,
                    aggregation_type=agg_type.value,
                    bucket_size_msec=3600000  # 1 hour
                )
            except Exception as e:
                logger.warning(f"Failed to create rule: {e}")

        # Level 3: 1hour â†’ 1day aggregates
        hour_key = f"sensor:{sensor_id}:1hour"

        day_key = f"sensor:{sensor_id}:1day"
        try:
            self.ts.create(
                day_key,
                retention_msecs=0,  # Forever
                labels={"sensor_id": sensor_id, "aggregation": "avg", "interval": "1day"}
            )
        except:
            pass

        try:
            self.ts.createrule(
                hour_key,
                day_key,
                aggregation_type="avg",
                bucket_size_msec=86400000  # 1 day
            )
        except Exception as e:
            logger.warning(f"Failed to create rule: {e}")

    # ========================================================================
    # Data Ingestion
    # ========================================================================

    def add_datapoint(
        self,
        sensor_id: str,
        value: float,
        timestamp: Optional[int] = None
    ) -> int:
        """
        Ajouter un point de donnÃ©es (usage simple)

        Args:
            sensor_id: ID du capteur
            value: Valeur mesurÃ©e
            timestamp: Timestamp en ms (None = now)

        Returns:
            Timestamp du point ajoutÃ©
        """
        key = f"sensor:{sensor_id}:raw"

        try:
            ts = self.ts.add(key, timestamp or "*", value)
            return ts
        except redis.RedisError as e:
            logger.error(f"Failed to add datapoint: {e}")
            raise

    def ingest_batch(
        self,
        datapoints: List[SensorDataPoint],
        batch_size: int = BATCH_SIZE
    ) -> Dict[str, int]:
        """
        Ingestion batch optimisÃ©e (usage production)

        Args:
            datapoints: Liste de points Ã  ingÃ©rer
            batch_size: Taille des batches pour pipeline

        Returns:
            Stats: {"success": N, "failed": M, "duration_ms": X}
        """
        start_time = time.time()
        success = 0
        failed = 0

        pipe = self.redis.pipeline()
        batch_count = 0

        for dp in datapoints:
            key = f"sensor:{dp.sensor_id}:raw"

            try:
                pipe.ts().add(key, dp.timestamp, dp.value)
                batch_count += 1

                # Execute batch when full
                if batch_count >= batch_size:
                    results = pipe.execute()
                    success += len([r for r in results if r])
                    failed += len([r for r in results if not r])

                    pipe = self.redis.pipeline()
                    batch_count = 0

            except Exception as e:
                logger.error(f"Failed to add to pipeline: {e}")
                failed += 1

        # Execute remaining
        if batch_count > 0:
            results = pipe.execute()
            success += len([r for r in results if r])
            failed += len([r for r in results if not r])

        duration_ms = (time.time() - start_time) * 1000

        logger.info(
            f"Batch ingested: {success} success, {failed} failed, "
            f"{duration_ms:.2f}ms ({success/duration_ms*1000:.0f} points/sec)"
        )

        return {
            "success": success,
            "failed": failed,
            "duration_ms": duration_ms
        }

    # ========================================================================
    # Querying
    # ========================================================================

    def get_range(
        self,
        sensor_id: str,
        from_time: int,
        to_time: int,
        aggregation: Optional[AggregationType] = None,
        bucket_size_ms: Optional[int] = None
    ) -> List[Tuple[int, float]]:
        """
        RÃ©cupÃ©rer donnÃ©es pour une plage temporelle

        Args:
            sensor_id: ID du capteur
            from_time: Timestamp dÃ©but (ms)
            to_time: Timestamp fin (ms)
            aggregation: Type d'agrÃ©gation (None = raw)
            bucket_size_ms: Taille bucket pour aggregation

        Returns:
            Liste de (timestamp, value)
        """
        # Auto-select optimal granularity
        key = self._select_optimal_key(sensor_id, from_time, to_time)

        try:
            if aggregation and bucket_size_ms:
                result = self.ts.range(
                    key,
                    from_time,
                    to_time,
                    aggregation_type=aggregation.value,
                    bucket_size_msec=bucket_size_ms
                )
            else:
                result = self.ts.range(key, from_time, to_time)

            return [(int(ts), float(val)) for ts, val in result]

        except redis.RedisError as e:
            logger.error(f"Query failed: {e}")
            return []

    def _select_optimal_key(
        self,
        sensor_id: str,
        from_time: int,
        to_time: int
    ) -> str:
        """
        SÃ©lectionner clÃ© optimale selon la plage temporelle

        Strategy:
        - < 1 hour: Use raw (1s)
        - 1 hour - 7 days: Use 1min aggregates
        - 7 days - 1 year: Use 1hour aggregates
        - > 1 year: Use 1day aggregates
        """
        duration_ms = to_time - from_time
        duration_hours = duration_ms / (3600 * 1000)

        if duration_hours < 1:
            return f"sensor:{sensor_id}:raw"
        elif duration_hours <= 168:  # 7 days
            return f"sensor:{sensor_id}:1min"
        elif duration_hours <= 8760:  # 1 year
            return f"sensor:{sensor_id}:1hour"
        else:
            return f"sensor:{sensor_id}:1day"

    def get_multi_range(
        self,
        filters: Dict[str, str],
        from_time: int,
        to_time: int,
        aggregation: Optional[AggregationType] = None,
        bucket_size_ms: Optional[int] = None
    ) -> Dict[str, List[Tuple[int, float]]]:
        """
        Query multiple sensors avec filtres

        Args:
            filters: Labels filters {"zone": "A", "type": "temperature"}
            from_time: Start timestamp
            to_time: End timestamp
            aggregation: Aggregation type
            bucket_size_ms: Bucket size

        Returns:
            Dict {sensor_id: [(timestamp, value), ...]}
        """
        try:
            if aggregation and bucket_size_ms:
                results = self.ts.mrange(
                    from_time,
                    to_time,
                    filters=filters,
                    aggregation_type=aggregation.value,
                    bucket_size_msec=bucket_size_ms
                )
            else:
                results = self.ts.mrange(from_time, to_time, filters=filters)

            # Parse results
            parsed = {}
            for key, labels, data in results:
                sensor_id = key.decode().split(":")[1] if isinstance(key, bytes) else key.split(":")[1]
                parsed[sensor_id] = [(int(ts), float(val)) for ts, val in data]

            return parsed

        except redis.RedisError as e:
            logger.error(f"Multi-range query failed: {e}")
            return {}

    def get_latest(self, sensor_id: str) -> Optional[Tuple[int, float]]:
        """RÃ©cupÃ©rer derniÃ¨re valeur"""
        key = f"sensor:{sensor_id}:raw"

        try:
            result = self.ts.get(key)
            if result:
                return (int(result[0]), float(result[1]))
            return None
        except redis.RedisError as e:
            logger.error(f"Failed to get latest: {e}")
            return None

    # ========================================================================
    # Analytics
    # ========================================================================

    def calculate_statistics(
        self,
        sensor_id: str,
        from_time: int,
        to_time: int
    ) -> Dict[str, float]:
        """
        Calculer statistiques sur une pÃ©riode

        Returns:
            {"avg": X, "min": Y, "max": Z, "count": N, "stddev": S}
        """
        # Use pre-aggregated data if available
        key_min = f"sensor:{sensor_id}:1min"

        try:
            # Get all aggregations
            avg_data = self.ts.range(key_min, from_time, to_time)

            if not avg_data:
                return {}

            values = [float(val) for _, val in avg_data]

            # Calculate statistics
            import statistics

            return {
                "avg": statistics.mean(values),
                "min": min(values),
                "max": max(values),
                "count": len(values),
                "stddev": statistics.stdev(values) if len(values) > 1 else 0.0
            }

        except Exception as e:
            logger.error(f"Statistics calculation failed: {e}")
            return {}

    # ========================================================================
    # Alerting
    # ========================================================================

    def check_threshold_alert(
        self,
        sensor_id: str,
        threshold: float,
        operator: str = ">"
    ) -> bool:
        """
        VÃ©rifier seuil d'alerte

        Args:
            sensor_id: ID du capteur
            threshold: Seuil
            operator: OpÃ©rateur (>, <, >=, <=, ==)

        Returns:
            True si alerte dÃ©clenchÃ©e
        """
        latest = self.get_latest(sensor_id)

        if not latest:
            return False

        _, value = latest

        operators = {
            ">": value > threshold,
            "<": value < threshold,
            ">=": value >= threshold,
            "<=": value <= threshold,
            "==": value == threshold
        }

        alert = operators.get(operator, False)

        if alert:
            logger.warning(
                f"ğŸš¨ Alert triggered: {sensor_id} = {value:.2f} "
                f"(threshold: {operator} {threshold})"
            )

        return alert

    def check_anomaly_detection(
        self,
        sensor_id: str,
        window_minutes: int = 60,
        std_threshold: float = 3.0
    ) -> bool:
        """
        DÃ©tection d'anomalie simple (Z-score)

        Alert si valeur actuelle > mean + (std_threshold Ã— stddev)
        """
        now = int(time.time() * 1000)
        from_time = now - (window_minutes * 60 * 1000)

        # Get statistics
        stats = self.calculate_statistics(sensor_id, from_time, now)

        if not stats:
            return False

        # Get latest value
        latest = self.get_latest(sensor_id)
        if not latest:
            return False

        _, current_value = latest

        # Z-score
        mean = stats['avg']
        stddev = stats['stddev']

        if stddev == 0:
            return False

        z_score = abs((current_value - mean) / stddev)

        anomaly = z_score > std_threshold

        if anomaly:
            logger.warning(
                f"ğŸš¨ Anomaly detected: {sensor_id} = {current_value:.2f} "
                f"(z-score: {z_score:.2f}, threshold: {std_threshold})"
            )

        return anomaly

    # ========================================================================
    # Monitoring
    # ========================================================================

    def get_sensor_info(self, sensor_id: str) -> Dict:
        """Info complÃ¨te sur un capteur"""
        key = f"sensor:{sensor_id}:raw"

        try:
            info = self.ts.info(key)

            return {
                "total_samples": info.total_samples,
                "memory_bytes": info.memory_usage,
                "first_timestamp": info.first_timestamp,
                "last_timestamp": info.last_timestamp,
                "retention_msecs": info.retention_msecs,
                "chunk_count": info.chunk_count,
                "labels": info.labels
            }
        except redis.RedisError as e:
            logger.error(f"Failed to get sensor info: {e}")
            return {}


# ============================================================================
# Exemple d'utilisation
# ============================================================================

if __name__ == "__main__":
    # Initialize
    manager = IoTTimeSeriesManager()

    print("\nğŸ­ IoT Time-Series Platform Demo\n")

    # Create sensor
    config = TimeSeriesConfig(
        sensor_id="temp_001",
        retention_ms=RETENTION_POLICIES['raw'],
        labels={
            "sensor_id": "temp_001",
            "zone": "A",
            "type": "temperature",
            "unit": "celsius",
            "building": "Factory_1"
        }
    )

    print("ğŸ“¡ Creating sensor time-series...")
    manager.create_sensor_timeseries(config)

    # Simulate ingestion
    print("\nğŸ“Š Simulating data ingestion (1000 points)...")

    now = int(time.time() * 1000)
    datapoints = []

    for i in range(1000):
        datapoints.append(SensorDataPoint(
            sensor_id="temp_001",
            timestamp=now - (1000 - i) * 1000,  # 1 point per second
            value=25.0 + (i % 10) * 0.5  # Simulate temperature variation
        ))

    stats = manager.ingest_batch(datapoints)
    print(f"   Success: {stats['success']}, Duration: {stats['duration_ms']:.2f}ms")
    print(f"   Throughput: {stats['success']/stats['duration_ms']*1000:.0f} points/sec")

    # Wait for compaction
    print("\nâ³ Waiting for automatic compaction...")
    time.sleep(2)

    # Query data
    print("\nğŸ” Querying last 10 minutes (raw data)...")
    from_time = now - (10 * 60 * 1000)
    data = manager.get_range("temp_001", from_time, now)
    print(f"   Retrieved {len(data)} data points")
    print(f"   Sample: {data[:3] if len(data) >= 3 else data}")

    # Query aggregated
    print("\nğŸ“ˆ Querying with 1-minute aggregation...")
    agg_data = manager.get_range(
        "temp_001",
        from_time,
        now,
        aggregation=AggregationType.AVG,
        bucket_size_ms=60000
    )
    print(f"   Retrieved {len(agg_data)} aggregated points")

    # Statistics
    print("\nğŸ“Š Statistics (last 10 minutes):")
    stats_data = manager.calculate_statistics("temp_001", from_time, now)
    for key, value in stats_data.items():
        print(f"   {key}: {value:.2f}")

    # Latest value
    print("\nğŸ“Œ Latest value:")
    latest = manager.get_latest("temp_001")
    if latest:
        ts, val = latest
        print(f"   Timestamp: {ts}, Value: {val:.2f}")

    # Threshold alert
    print("\nğŸš¨ Checking threshold alert (> 30Â°C):")
    alert = manager.check_threshold_alert("temp_001", 30.0, ">")
    print(f"   Alert triggered: {alert}")

    # Anomaly detection
    print("\nğŸ” Anomaly detection (Z-score):")
    anomaly = manager.check_anomaly_detection("temp_001", window_minutes=10, std_threshold=3.0)
    print(f"   Anomaly detected: {anomaly}")

    # Sensor info
    print("\nğŸ’¾ Sensor Info:")
    info = manager.get_sensor_info("temp_001")
    print(f"   Total samples: {info.get('total_samples', 0)}")
    print(f"   Memory: {info.get('memory_bytes', 0)} bytes")
    print(f"   Chunks: {info.get('chunk_count', 0)}")
```

---

## 5. Monitoring et mÃ©triques

### 5.1 KPIs critiques

```yaml
# 1. Ingestion performance
ingestion_throughput:
  target: > 1M points/sec
  current: 800k-2M points/sec

ingestion_latency_ms:
  p50: < 50ms
  p95: < 100ms
  p99: < 200ms

# 2. Query performance
query_latency_ms:
  raw_1h: < 10ms
  aggregated_24h: < 50ms
  multi_sensor_100: < 200ms

# 3. Storage efficiency
memory_per_datapoint:
  raw: ~50 bytes
  1min_agg: ~30 bytes
  compression_ratio: 10:1

# 4. Data quality
data_loss_rate:
  target: < 0.01%

compaction_lag:
  target: < 1 second
```

---

## 6. Conclusion

### Points clÃ©s

- âœ… **RedisTimeSeries** : Ingestion 1M-5M points/sec
- âœ… **Auto-compaction** : Downsampling temps rÃ©el (< 1s lag)
- âœ… **Multi-level** : raw â†’ 1min â†’ 1hour â†’ 1day automatique
- âœ… **Query < 10ms** : In-memory ultra-rapide
- âœ… **CoÃ»t Ã· 10** : vs InfluxDB Cloud

### Prochaines lectures

- [RedisTimeSeries Docs](https://redis.io/docs/stack/timeseries/)
- [Cas #4 : Analytics temps rÃ©el](./04-cas-analytics-temps-reel.md)

---

**ğŸ“š Ressources** :
- [Time-Series Best Practices](https://redis.io/docs/stack/timeseries/design/)
- [IoT Architecture Patterns](https://aws.amazon.com/iot/solutions/)

â­ï¸ [Design patterns recommandÃ©s](/16-etudes-cas-patterns-reels/09-design-patterns-recommandes.md)
