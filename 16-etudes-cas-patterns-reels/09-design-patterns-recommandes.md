ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# Design Patterns RecommandÃ©s

## Vue d'ensemble

Cette section synthÃ©tise les **design patterns Ã©prouvÃ©s** observÃ©s dans les 8 cas d'Ã©tude prÃ©cÃ©dents, avec des guidelines pour leur application.

**Objectif** : Fournir un catalogue de patterns rÃ©utilisables pour architecturer des systÃ¨mes Redis production-grade.

---

## 1. Cache Patterns

### 1.1 Cache-Aside (Lazy Loading)

**Cas observÃ©** : Cas #6 (Cache SQL)

**Principe** : L'application gÃ¨re le cache explicitement.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Application Flow                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Check cache
   â”œâ”€ HIT  â†’ Return cached data
   â””â”€ MISS â†’ Query database
              â”œâ”€ Store in cache
              â””â”€ Return data

Characteristics:
- Application controls caching logic
- Data only loaded when needed (lazy)
- Cache can be inconsistent with DB
```

**ImplÃ©mentation type** :

```python
def get_data(key):
    # 1. Try cache
    cached = redis.get(key)
    if cached:
        return deserialize(cached)  # Cache hit

    # 2. Cache miss â†’ Query DB
    data = db.query(...)

    # 3. Store in cache
    redis.setex(key, TTL, serialize(data))

    return data
```

**Quand utiliser** :

âœ… **Utiliser si** :
- Read-heavy workload (lecture >> Ã©criture)
- DonnÃ©es pas toujours nÃ©cessaires (lazy loading OK)
- TolÃ©rance Ã  staleness (donnÃ©es lÃ©gÃ¨rement obsolÃ¨tes OK)
- ContrÃ´le explicite de l'invalidation souhaitÃ©

âŒ **Ã‰viter si** :
- Write-heavy workload
- Consistance stricte requise
- Warm-up critique au dÃ©marrage

**Trade-offs** :

| Aspect | Avantage | InconvÃ©nient |
|--------|----------|--------------|
| Performance | âœ… Cache hit ultra-rapide | âš ï¸ Cache miss = 2Ã— latency (cache + DB) |
| Consistance | âš ï¸ Eventual consistency | âŒ Peut servir donnÃ©es stales |
| ComplexitÃ© | âœ… Simple Ã  implÃ©menter | âš ï¸ Application gÃ¨re invalidation |
| Resilience | âœ… Fail-safe (fallback DB) | - |

**Variante : Cache-Aside avec Stampede Protection** :

```python
def get_data_with_lock(key):
    # Try cache
    cached = redis.get(key)
    if cached:
        return cached

    # Acquire lock to prevent stampede
    lock_key = f"{key}:lock"
    lock = redis.set(lock_key, "1", nx=True, ex=30)

    if lock:
        # This thread queries DB
        try:
            data = db.query(...)
            redis.setex(key, TTL, data)
            return data
        finally:
            redis.delete(lock_key)
    else:
        # Wait for other thread
        time.sleep(0.1)
        return get_data_with_lock(key)  # Retry
```

---

### 1.2 Write-Through Cache

**Principe** : Ã‰criture synchrone dans cache ET database.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Write Flow                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Application
   â”œâ”€> Write to Cache
   â””â”€> Write to Database (sync)

Read Flow
   â””â”€> Always read from Cache (guaranteed fresh)

Characteristics:
- Cache always consistent with DB
- Write latency = cache + DB
- No cache misses on reads
```

**ImplÃ©mentation type** :

```python
def save_data(key, data):
    # 1. Write to cache (fast)
    redis.setex(key, TTL, serialize(data))

    # 2. Write to DB (slower)
    db.save(data)

    # Cache is always fresh

def get_data(key):
    # Simple read from cache (no fallback needed)
    return redis.get(key)
```

**Quand utiliser** :

âœ… **Utiliser si** :
- Consistance cache-DB critique
- Read-heavy avec writes modÃ©rÃ©s
- Cache misses inacceptables

âŒ **Ã‰viter si** :
- Write-heavy (latency d'Ã©criture Ã— 2)
- Writes non critiques

---

### 1.3 Write-Behind (Write-Back)

**Cas observÃ©** : Cas #1 (Session Store)

**Principe** : Ã‰criture asynchrone vers database.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Write Flow                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Application
   â””â”€> Write to Cache (fast)
        â””â”€> [Async] Write to Database (background)

Benefits:
- Write latency = cache only
- Batch writes to DB (throughput Ã—10-100)
- DB can lag behind cache
```

**ImplÃ©mentation type** :

```python
import queue
import threading

write_queue = queue.Queue()

def save_data(key, data):
    # 1. Write to cache (fast, synchronous)
    redis.setex(key, TTL, serialize(data))

    # 2. Queue for async DB write
    write_queue.put((key, data))

    # Return immediately (low latency)

# Background worker
def db_writer_worker():
    while True:
        batch = []

        # Collect batch
        for _ in range(100):
            try:
                batch.append(write_queue.get(timeout=1))
            except queue.Empty:
                break

        if batch:
            # Batch write to DB
            db.bulk_insert(batch)

# Start worker thread
threading.Thread(target=db_writer_worker, daemon=True).start()
```

**Quand utiliser** :

âœ… **Utiliser si** :
- Write-heavy workload
- Latence d'Ã©criture critique
- Batch writes possibles
- Eventual consistency acceptable

âŒ **Ã‰viter si** :
- DurabilitÃ© immÃ©diate requise (risque perte donnÃ©es si crash)
- ACID transactions nÃ©cessaires

**âš ï¸ Attention** : Risque de data loss si Redis crash avant flush vers DB.

---

### 1.4 Refresh-Ahead

**Principe** : RafraÃ®chir cache avant expiration.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Refresh Strategy                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Timeline:
0s â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 60s â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 120s
    Data cached   Pre-fetch      Expire
                  at 50s         (would be)

Benefit: No cache miss ever
```

**ImplÃ©mentation type** :

```python
def get_data_with_refresh(key, ttl=120):
    cached = redis.get(key)

    if cached:
        # Check TTL
        remaining_ttl = redis.ttl(key)

        # If close to expiration, refresh in background
        if remaining_ttl < ttl * 0.2:  # 20% threshold
            threading.Thread(
                target=_refresh_cache,
                args=(key,),
                daemon=True
            ).start()

        return cached

    # Cache miss (should be rare)
    return _load_and_cache(key, ttl)

def _refresh_cache(key):
    data = db.query(...)
    redis.setex(key, TTL, serialize(data))
```

**Quand utiliser** :

âœ… **Utiliser si** :
- Cache misses trÃ¨s coÃ»teuses
- Latence prÃ©dictible requise
- DonnÃ©es rafraÃ®chissables de maniÃ¨re asynchrone

---

## 2. Data Modeling Patterns

### 2.1 Key Naming Convention

**ObservÃ© dans tous les cas**

**Pattern recommandÃ©** :

```
Format: {namespace}:{entity}:{id}:{attribute}

Examples:
- user:profile:123                    (Hash)
- session:usr_abc:data                (JSON)
- product:prod_456:embedding          (Vector)
- sensor:temp_001:raw                 (TimeSeries)
- rate_limit:user:usr_123:1702300800  (String)

Benefits:
âœ… Hierarchical organization
âœ… Easy pattern matching (SCAN)
âœ… Collision avoidance
âœ… Self-documenting
```

**Guidelines** :

```yaml
Structure:
  - Use colons (:) as separators
  - Start with namespace (app/service name)
  - Include entity type
  - Include unique identifier
  - Optional: Add timestamp or version

Bad examples:
  âŒ "user123"              # No structure
  âŒ "user-123-profile"     # Inconsistent separator
  âŒ "123"                  # Not self-documenting
  âŒ "userprofile123data"   # No separation

Good examples:
  âœ… "myapp:user:123:profile"
  âœ… "cache:query:abc123"
  âœ… "session:usr_456:2024-12-11"
```

---

### 2.2 Hash vs JSON

**Cas observÃ©** : Cas #1 (RedisJSON vs Hash)

**Decision Tree** :

```
Data structure needed?
â”œâ”€ Simple key-value pairs
â”‚  â””â”€> Use HASH
â”‚      - Pros: Native, HGETALL, HINCRBY
â”‚      - Cons: Flat structure only
â”‚
â””â”€ Nested objects/arrays
   â””â”€> Use JSON
       - Pros: JSONPath queries, nested updates
       - Cons: Requires Redis Stack
```

**Exemples comparatifs** :

```python
# HASH (flat data)
redis.hset("user:123", mapping={
    "name": "Alice",
    "email": "alice@example.com",
    "age": 30
})

# Retrieve field
name = redis.hget("user:123", "name")

# Increment
redis.hincrby("user:123", "age", 1)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# JSON (nested data)
redis.json().set("user:123", "$", {
    "name": "Alice",
    "email": "alice@example.com",
    "age": 30,
    "address": {
        "city": "Paris",
        "country": "France"
    },
    "preferences": ["email", "sms"]
})

# Retrieve nested field
city = redis.json().get("user:123", "$.address.city")

# Update nested field
redis.json().set("user:123", "$.address.city", "Lyon")

# Append to array
redis.json().arrappend("user:123", "$.preferences", "push")
```

**Quand utiliser quoi** :

| CritÃ¨re | HASH | JSON |
|---------|------|------|
| Structure | âœ… Flat (1 niveau) | âœ… Nested (N niveaux) |
| Performance | âœ… LÃ©gÃ¨rement plus rapide | âš ï¸ Overhead parsing |
| Queries | âš ï¸ LimitÃ© (get/set fields) | âœ… JSONPath puissant |
| Memory | âœ… LÃ©gÃ¨rement plus compact | âš ï¸ JSON overhead |
| Compatibility | âœ… Redis core | âš ï¸ Requires Redis Stack |
| Atomic ops | âœ… HINCRBY, HINCRBYFLOAT | âœ… JSON.NUMINCRBY |

---

### 2.3 Secondary Indexing Pattern

**Cas observÃ©** : Cas #2 (RediSearch), Cas #7 (Vector Search)

**Sans RediSearch (manual)** :

```python
# Primary data
redis.hset("user:123", mapping={"name": "Alice", "city": "Paris"})

# Secondary index (Set)
redis.sadd("idx:city:Paris", "user:123")
redis.sadd("idx:city:Paris", "user:456")

# Query by city
user_ids = redis.smembers("idx:city:Paris")
users = [redis.hgetall(f"user:{uid}") for uid in user_ids]

# Problem: Manual maintenance
# - Must update indexes on every write
# - Risk of inconsistency
```

**Avec RediSearch (automatic)** :

```python
# Create index (once)
redis.ft("users_idx").create_index([
    TextField("name"),
    TagField("city"),
    NumericField("age")
])

# Add data (indexes automatically)
redis.hset("user:123", mapping={
    "name": "Alice",
    "city": "Paris",
    "age": 30
})

# Query (automatic index usage)
results = redis.ft("users_idx").search(
    "@city:{Paris} @age:[25 35]"
)

# Benefits:
# âœ… Automatic index maintenance
# âœ… Complex queries (AND, OR, NOT)
# âœ… Full-text search
# âœ… Aggregations
```

**Recommendation** : Utiliser RediSearch dÃ¨s que indexing requis (Ã©viter manual indexing).

---

### 2.4 Time-Series Data Modeling

**Cas observÃ©** : Cas #4 (Analytics), Cas #8 (IoT)

**Pattern : Multi-level aggregations**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Hierarchical Time-Series                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

sensor:temp_001:raw          (1s granularity, 7 days)
   â”œâ”€> sensor:temp_001:1min  (1min, 1 year)
   â”‚    â”œâ”€> sensor:temp_001:1hour (1h, 5 years)
   â”‚    â”‚    â””â”€> sensor:temp_001:1day (1d, forever)
   â”‚    â”‚
   â”‚    â”œâ”€> sensor:temp_001:1min_max (max values)
   â”‚    â””â”€> sensor:temp_001:1min_min (min values)
   â”‚
   â””â”€> Automatic compaction rules

Benefits:
âœ… Query speed (less data points)
âœ… Storage efficiency
âœ… Automatic downsampling
```

**ImplÃ©mentation** :

```python
# Create hierarchy
redis.ts().create("sensor:temp_001:raw", retention_msecs=7*24*3600*1000)

# Auto-compaction rules
redis.ts().createrule(
    "sensor:temp_001:raw",
    "sensor:temp_001:1min",
    aggregation_type="avg",
    bucket_size_msec=60000  # 1 minute
)

redis.ts().createrule(
    "sensor:temp_001:1min",
    "sensor:temp_001:1hour",
    aggregation_type="avg",
    bucket_size_msec=3600000  # 1 hour
)

# Query selection strategy
def get_optimal_key(time_range_hours):
    if time_range_hours < 1:
        return "sensor:temp_001:raw"
    elif time_range_hours <= 168:  # 1 week
        return "sensor:temp_001:1min"
    else:
        return "sensor:temp_001:1hour"
```

---

## 3. Performance Patterns

### 3.1 Pipeline Pattern

**ObservÃ© dans tous les cas**

**Principe** : Batch multiple commands en 1 round-trip.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Without Pipeline (N round-trips)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

for i in range(1000):
    redis.set(f"key:{i}", value)  # 1 RTT each

Total time: 1000 Ã— 1ms = 1000ms

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ With Pipeline (1 round-trip)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

pipe = redis.pipeline()
for i in range(1000):
    pipe.set(f"key:{i}", value)
pipe.execute()  # 1 RTT for all

Total time: 1ms + processing
Speedup: Ã—1000
```

**ImplÃ©mentation type** :

```python
def bulk_insert(data_list, batch_size=100):
    pipe = redis.pipeline()

    for i, data in enumerate(data_list):
        pipe.set(f"key:{data.id}", data.value)

        # Execute every batch_size commands
        if (i + 1) % batch_size == 0:
            pipe.execute()
            pipe = redis.pipeline()

    # Execute remaining
    if len(pipe) > 0:
        pipe.execute()
```

**Quand utiliser** :

âœ… **Toujours utiliser** pour operations multiples :
- Bulk inserts
- Batch updates
- Multiple independent reads

âŒ **Ne pas utiliser** si :
- Operations interdÃ©pendantes (rÃ©sultat de cmd1 nÃ©cessaire pour cmd2)
- Transactions ACID requises (utiliser MULTI/EXEC Ã  la place)

**Guidelines** :

```python
# âŒ Bad: No pipeline
for user_id in user_ids:
    redis.get(f"user:{user_id}")

# âœ… Good: Pipeline
pipe = redis.pipeline()
for user_id in user_ids:
    pipe.get(f"user:{user_id}")
results = pipe.execute()

# âœ… Better: Optimal batch size
def batch_get(keys, batch_size=100):
    results = []

    for i in range(0, len(keys), batch_size):
        batch = keys[i:i+batch_size]
        pipe = redis.pipeline()
        for key in batch:
            pipe.get(key)
        results.extend(pipe.execute())

    return results
```

**Batch size recommendation** : 50-500 commands per pipeline.

---

### 3.2 Lua Scripting for Atomicity

**Cas observÃ©** : Cas #3 (Leaderboard), Cas #5 (Rate Limiting)

**Principe** : Operations complexes atomiques en 1 RTT.

**Exemple : Rate Limiting atomique**

```lua
-- Without Lua (3 commands = race condition)
local count = redis.call('GET', key)
if count < limit then
    redis.call('INCR', key)
    return 1
end
return 0

-- Problem: Another thread can increment between GET and INCR
```

```lua
-- With Lua (atomic)
local key = KEYS[1]
local limit = tonumber(ARGV[1])

local current = redis.call('GET', key)

if current == false then
    redis.call('SETEX', key, 3600, 1)
    return 1
end

current = tonumber(current)

if current >= limit then
    return 0  -- Rate limited
end

redis.call('INCR', key)
return 1  -- Allowed
```

**Quand utiliser Lua** :

âœ… **Utiliser si** :
- AtomicitÃ© requise (check-and-set)
- RÃ©duire RTT (multiple commands â†’ 1)
- Logic complexe cÃ´tÃ© serveur

âŒ **Ã‰viter si** :
- Logic simple (un seul command suffit)
- Debugging critique (Lua = black box)
- Frequent changes (dÃ©ploiement script complexe)

**Best practices Lua** :

```python
# Pre-load script (once)
RATE_LIMIT_SCRIPT_SHA = redis.script_load(RATE_LIMIT_SCRIPT)

# Execute with EVALSHA (faster)
def check_rate_limit(user_id, limit):
    return redis.evalsha(
        RATE_LIMIT_SCRIPT_SHA,
        1,  # Number of keys
        f"rate_limit:{user_id}",  # KEYS[1]
        limit  # ARGV[1]
    )

# Fallback to EVAL if script not loaded
try:
    result = redis.evalsha(sha, ...)
except redis.exceptions.NoScriptError:
    result = redis.eval(script, ...)
```

---

### 3.3 Connection Pooling

**Principe** : RÃ©utiliser connexions TCP pour Ã©viter overhead.

```python
# âŒ Bad: New connection per operation
def get_user(user_id):
    client = redis.Redis(host='localhost', port=6379)
    user = client.get(f"user:{user_id}")
    client.close()  # Expensive!
    return user

# Cost: TCP handshake + auth = 5-10ms per call

# âœ… Good: Connection pool (shared)
pool = redis.ConnectionPool(
    host='localhost',
    port=6379,
    max_connections=50,
    decode_responses=True
)

client = redis.Redis(connection_pool=pool)

def get_user(user_id):
    return client.get(f"user:{user_id}")

# Cost: ~0.5ms per call (Ã—10-20 faster)
```

**Configuration optimale** :

```python
pool = redis.ConnectionPool(
    host='localhost',
    port=6379,
    max_connections=50,  # Threads Ã— 2-3
    socket_timeout=5,    # Prevent hanging
    socket_connect_timeout=3,
    socket_keepalive=True,
    health_check_interval=30  # Check stale connections
)
```

---

## 4. Scalability Patterns

### 4.1 Sharding Pattern

**Cas observÃ©** : Cas #3 (Leaderboard sharding)

**Principe** : Distribuer donnÃ©es sur multiple instances.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Hash-based Sharding                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

user_id â†’ hash(user_id) % num_shards â†’ Shard N

Example:
- user:123 â†’ hash(123) % 4 = 1 â†’ Shard 1
- user:456 â†’ hash(456) % 4 = 3 â†’ Shard 3

Benefits:
âœ… Distribute load
âœ… Horizontal scaling
âœ… Avoid hot keys
```

**ImplÃ©mentation** :

```python
class ShardedRedis:
    def __init__(self, shard_configs):
        self.shards = [
            redis.Redis(**config)
            for config in shard_configs
        ]
        self.num_shards = len(self.shards)

    def _get_shard(self, key):
        # Consistent hashing
        shard_idx = hash(key) % self.num_shards
        return self.shards[shard_idx]

    def set(self, key, value):
        shard = self._get_shard(key)
        return shard.set(key, value)

    def get(self, key):
        shard = self._get_shard(key)
        return shard.get(key)

    def get_multi(self, keys):
        # Group by shard
        shard_keys = {}
        for key in keys:
            shard = self._get_shard(key)
            shard_keys.setdefault(shard, []).append(key)

        # Parallel fetch
        results = {}
        for shard, shard_key_list in shard_keys.items():
            pipe = shard.pipeline()
            for key in shard_key_list:
                pipe.get(key)
            shard_results = pipe.execute()

            for key, value in zip(shard_key_list, shard_results):
                results[key] = value

        return results
```

**Trade-offs** :

| Aspect | Avantage | InconvÃ©nient |
|--------|----------|--------------|
| Scalability | âœ… Horizontal scaling | âš ï¸ Rebalancing complexe |
| Load distribution | âœ… No hot keys | - |
| Operations | âš ï¸ Single-key OK | âŒ Multi-key difficile (cross-shard) |
| Maintenance | âš ï¸ More instances | âš ï¸ More operational complexity |

**Recommendation** : Utiliser Redis Cluster plutÃ´t que sharding manuel.

---

### 4.2 Master-Replica Pattern

**ObservÃ© dans tous les cas**

**Principe** : RÃ©plication pour high availability et read scaling.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Master-Replica Setup                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Master  â”‚ (Writes)
        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
             â”‚ Replication
        â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚         â”‚        â”‚
    â”Œâ”€â”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”
    â”‚Replicaâ”‚ â”‚Replicaâ”‚ â”‚Replicaâ”‚ (Reads)
    â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜

Benefits:
âœ… Read scaling (N replicas)
âœ… High availability (failover)
âœ… Geographic distribution
```

**Configuration** :

```python
# Master (writes)
master = redis.Redis(host='redis-master', port=6379)

# Replicas (reads)
replicas = [
    redis.Redis(host='redis-replica-1', port=6379),
    redis.Redis(host='redis-replica-2', port=6379),
    redis.Redis(host='redis-replica-3', port=6379),
]

# Write to master
def write_data(key, value):
    return master.set(key, value)

# Read from replica (round-robin)
replica_idx = 0

def read_data(key):
    global replica_idx
    replica = replicas[replica_idx % len(replicas)]
    replica_idx += 1
    return replica.get(key)
```

**Avec Sentinel (automatic failover)** :

```python
from redis.sentinel import Sentinel

sentinel = Sentinel([
    ('sentinel-1', 26379),
    ('sentinel-2', 26379),
    ('sentinel-3', 26379)
], socket_timeout=0.1)

# Get master (automatic failover)
master = sentinel.master_for('mymaster', socket_timeout=0.1)

# Get replica (load balancing)
replica = sentinel.slave_for('mymaster', socket_timeout=0.1)

# Use
master.set('key', 'value')  # Write
value = replica.get('key')  # Read
```

---

## 5. Reliability Patterns

### 5.1 Circuit Breaker Pattern

**Principe** : Ã‰viter d'overload un service dÃ©faillant.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Circuit Breaker States                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

CLOSED (normal)
   â”‚ Failures > threshold
   â–¼
OPEN (reject all)
   â”‚ After timeout
   â–¼
HALF_OPEN (test)
   â”‚ Success â†’ CLOSED
   â”‚ Failure â†’ OPEN
```

**ImplÃ©mentation** :

```python
from enum import Enum
import time

class CircuitState(Enum):
    CLOSED = "closed"
    OPEN = "open"
    HALF_OPEN = "half_open"

class CircuitBreaker:
    def __init__(self, failure_threshold=5, timeout=60):
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.failures = 0
        self.last_failure_time = None
        self.state = CircuitState.CLOSED

    def call(self, func, *args, **kwargs):
        if self.state == CircuitState.OPEN:
            if time.time() - self.last_failure_time > self.timeout:
                self.state = CircuitState.HALF_OPEN
            else:
                raise Exception("Circuit breaker is OPEN")

        try:
            result = func(*args, **kwargs)
            self._on_success()
            return result
        except Exception as e:
            self._on_failure()
            raise

    def _on_success(self):
        self.failures = 0
        self.state = CircuitState.CLOSED

    def _on_failure(self):
        self.failures += 1
        self.last_failure_time = time.time()

        if self.failures >= self.failure_threshold:
            self.state = CircuitState.OPEN

# Usage
breaker = CircuitBreaker(failure_threshold=3, timeout=30)

def get_data(key):
    return breaker.call(redis.get, key)
```

---

### 5.2 Retry with Exponential Backoff

**Principe** : Retry avec dÃ©lai croissant.

```python
import time
import random

def retry_with_backoff(
    func,
    max_retries=3,
    base_delay=0.1,
    max_delay=10,
    exponential_base=2
):
    """
    Retry avec exponential backoff

    Delays: 0.1s, 0.2s, 0.4s, 0.8s, ...
    """
    for attempt in range(max_retries):
        try:
            return func()
        except redis.RedisError as e:
            if attempt == max_retries - 1:
                raise  # Last attempt

            # Calculate delay
            delay = min(
                base_delay * (exponential_base ** attempt),
                max_delay
            )

            # Add jitter to avoid thundering herd
            jitter = random.uniform(0, delay * 0.1)

            time.sleep(delay + jitter)

    raise Exception("Max retries exceeded")

# Usage
def get_user(user_id):
    return retry_with_backoff(
        lambda: redis.get(f"user:{user_id}"),
        max_retries=3
    )
```

---

## 6. Monitoring Patterns

### 6.1 Metrics Pattern

**ObservÃ© dans tous les cas**

**MÃ©triques essentielles** :

```python
from dataclasses import dataclass
from typing import Dict
import time

@dataclass
class RedisMetrics:
    """MÃ©triques Redis Ã  tracker"""

    # Performance
    operation_latency_ms: Dict[str, float]  # {operation: latency}
    throughput_ops_per_sec: float

    # Cache
    hit_ratio: float
    hits: int
    misses: int

    # Connections
    connected_clients: int
    blocked_clients: int

    # Memory
    used_memory_mb: float
    used_memory_peak_mb: float
    fragmentation_ratio: float

    # Persistence
    rdb_last_save_time: int
    rdb_changes_since_last_save: int

class RedisMonitor:
    def __init__(self, redis_client):
        self.redis = redis_client
        self.ops_count = {}
        self.ops_latency = {}

    def track_operation(self, operation: str):
        """Decorator to track operations"""
        def decorator(func):
            def wrapper(*args, **kwargs):
                start = time.time()

                try:
                    result = func(*args, **kwargs)

                    # Track success
                    latency = (time.time() - start) * 1000
                    self._record_latency(operation, latency)

                    return result
                except Exception as e:
                    # Track error
                    self._record_error(operation)
                    raise

            return wrapper
        return decorator

    def _record_latency(self, operation, latency):
        if operation not in self.ops_latency:
            self.ops_latency[operation] = []

        self.ops_latency[operation].append(latency)

        # Keep last 1000 operations
        if len(self.ops_latency[operation]) > 1000:
            self.ops_latency[operation].pop(0)

    def get_metrics(self) -> RedisMetrics:
        """Collect current metrics"""
        info = self.redis.info()
        stats = self.redis.info("stats")

        # Calculate hit ratio
        hits = stats.get("keyspace_hits", 0)
        misses = stats.get("keyspace_misses", 0)
        total = hits + misses
        hit_ratio = hits / total if total > 0 else 0

        return RedisMetrics(
            operation_latency_ms=self._calculate_avg_latency(),
            throughput_ops_per_sec=stats.get("instantaneous_ops_per_sec", 0),
            hit_ratio=hit_ratio,
            hits=hits,
            misses=misses,
            connected_clients=info.get("connected_clients", 0),
            blocked_clients=info.get("blocked_clients", 0),
            used_memory_mb=info.get("used_memory", 0) / (1024 * 1024),
            used_memory_peak_mb=info.get("used_memory_peak", 0) / (1024 * 1024),
            fragmentation_ratio=info.get("mem_fragmentation_ratio", 0),
            rdb_last_save_time=info.get("rdb_last_save_time", 0),
            rdb_changes_since_last_save=info.get("rdb_changes_since_last_save", 0)
        )

    def _calculate_avg_latency(self):
        return {
            op: sum(latencies) / len(latencies)
            for op, latencies in self.ops_latency.items()
            if latencies
        }

# Usage
monitor = RedisMonitor(redis)

@monitor.track_operation("get_user")
def get_user(user_id):
    return redis.get(f"user:{user_id}")

# Collect metrics
metrics = monitor.get_metrics()
print(f"Hit ratio: {metrics.hit_ratio:.2%}")
print(f"Avg latency: {metrics.operation_latency_ms}")
```

---

### 6.2 Health Check Pattern

```python
def redis_health_check(redis_client, timeout=2):
    """
    Health check complet

    Returns: {"healthy": bool, "details": dict}
    """
    health = {
        "healthy": True,
        "details": {}
    }

    try:
        # 1. Ping test
        start = time.time()
        redis_client.ping()
        ping_latency = (time.time() - start) * 1000

        health["details"]["ping_latency_ms"] = ping_latency

        if ping_latency > timeout * 1000:
            health["healthy"] = False
            health["details"]["error"] = "High latency"

        # 2. Memory check
        info = redis_client.info("memory")
        used_memory_pct = info.get("used_memory", 0) / info.get("maxmemory", 1)

        health["details"]["memory_usage_pct"] = used_memory_pct * 100

        if used_memory_pct > 0.9:
            health["healthy"] = False
            health["details"]["warning"] = "High memory usage"

        # 3. Replication check (if replica)
        replication = redis_client.info("replication")
        if replication.get("role") == "slave":
            master_link = replication.get("master_link_status")
            health["details"]["replication_status"] = master_link

            if master_link != "up":
                health["healthy"] = False
                health["details"]["error"] = "Replication down"

        return health

    except redis.RedisError as e:
        return {
            "healthy": False,
            "details": {"error": str(e)}
        }
```

---

## 7. Decision Matrix

### 7.1 Choisir le bon pattern de cache

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cache Pattern Selection                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Pattern      â”‚ Write Heavy  â”‚ Read Heavy â”‚ Consistency         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Cache-Aside  â”‚ âš ï¸           â”‚ âœ…         â”‚ Eventual            â”‚
â”‚ Write-Throughâ”‚ âš ï¸           â”‚ âœ…         â”‚ Strong              â”‚
â”‚ Write-Behind â”‚ âœ…           â”‚ âœ…         â”‚ Eventual (risk)     â”‚
â”‚ Refresh-Aheadâ”‚ âš ï¸           â”‚ âœ…         â”‚ Eventual            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 7.2 Choisir la structure de donnÃ©es

```
Use case â†’ Structure recommandÃ©e

Session data â†’ RedisJSON (nested) ou Hash (flat)
Full-text search â†’ RediSearch
Leaderboard â†’ Sorted Set
Rate limiting â†’ String + TTL + Lua
Time-series â†’ RedisTimeSeries
Recommendations â†’ RediSearch Vector
Analytics â†’ HyperLogLog + TimeSeries
Queue â†’ List ou Stream
Pub/Sub â†’ Pub/Sub ou Stream
Locks â†’ String (SET NX EX)
```

---

## 8. Anti-Patterns Ã  Ã©viter

### âŒ Anti-Pattern 1 : Unbounded Collections

```python
# âŒ BAD: List grows forever
redis.lpush("logs", log_entry)  # No size limit!

# âœ… GOOD: Trim to max size
redis.lpush("logs", log_entry)
redis.ltrim("logs", 0, 999)  # Keep last 1000

# âœ… BETTER: Use TTL
redis.lpush("logs:2024-12-11", log_entry)
redis.expire("logs:2024-12-11", 86400)  # 1 day
```

### âŒ Anti-Pattern 2 : Large Keys

```python
# âŒ BAD: Store 100 MB JSON in one key
redis.set("huge_data", json.dumps(huge_dict))

# âœ… GOOD: Split into smaller chunks
for i, chunk in enumerate(chunks(huge_dict, size=1000)):
    redis.set(f"data:chunk:{i}", json.dumps(chunk))
```

### âŒ Anti-Pattern 3 : SELECT Database

```python
# âŒ BAD: Use SELECT to switch DBs
redis.select(1)  # NOT thread-safe!
redis.set("key", "value")

# âœ… GOOD: Separate connection per DB
db0 = redis.Redis(host='localhost', port=6379, db=0)
db1 = redis.Redis(host='localhost', port=6379, db=1)

db1.set("key", "value")
```

### âŒ Anti-Pattern 4 : KEYS in Production

```python
# âŒ BAD: KEYS blocks Redis
all_keys = redis.keys("user:*")  # O(N) - blocks everything!

# âœ… GOOD: Use SCAN (iterative)
cursor = 0
keys = []
while True:
    cursor, batch = redis.scan(cursor, match="user:*", count=100)
    keys.extend(batch)
    if cursor == 0:
        break
```

---

## Conclusion

### Checklist : Design Production-Ready Redis

```yaml
Architecture:
  âœ… Cache pattern choisi (Cache-Aside, Write-Through, etc.)
  âœ… Master-Replica pour HA
  âœ… Connection pooling configurÃ©
  âœ… Sharding strategy si > 100 GB

Performance:
  âœ… Pipeline pour batch operations
  âœ… Lua pour atomicitÃ© si nÃ©cessaire
  âœ… Indexes (RediSearch) si queries complexes
  âœ… Multi-level aggregations (TimeSeries)

Reliability:
  âœ… Circuit breaker implÃ©mentÃ©
  âœ… Retry with exponential backoff
  âœ… Health checks automatiques
  âœ… Failover configurÃ© (Sentinel/Cluster)

Monitoring:
  âœ… Metrics collectÃ©es (latency, hit ratio, memory)
  âœ… Alerting configurÃ© (Prometheus/Grafana)
  âœ… Logging appropriÃ©
  âœ… Tracing distribuÃ© (si microservices)

Data Modeling:
  âœ… Key naming convention cohÃ©rente
  âœ… TTL configurÃ© (pas de memory leak)
  âœ… Structure de donnÃ©es optimale
  âœ… Pas d'anti-patterns
```

### Patterns par use case

| Use Case | Patterns recommandÃ©s |
|----------|---------------------|
| Web cache | Cache-Aside + Pipeline + Master-Replica |
| Session store | Write-Behind + RedisJSON + TTL |
| Real-time analytics | TimeSeries + HyperLogLog + Aggregations |
| Search engine | RediSearch + Secondary indexes |
| Leaderboard | Sorted Set + Composite scores |
| Rate limiting | Lua scripting + TTL |
| Recommendations | Vector search + Hybrid queries |
| IoT monitoring | TimeSeries + Auto-compaction |

---

**Prochaines lectures recommandÃ©es** :

- [Anti-patterns Ã  Ã©viter](./10-anti-patterns-eviter.md) â†’ Erreurs courantes
- Modules techniques spÃ©cifiques pour approfondissement
- Documentation Redis officielle pour features avancÃ©es

â­ï¸ [Anti-patterns Ã  Ã©viter absolument](/16-etudes-cas-patterns-reels/10-anti-patterns-a-eviter.md)
